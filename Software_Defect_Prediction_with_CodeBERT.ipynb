{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Software Defect Prediction with CodeBERT",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tranmanhcuong253/Software-Defect-Prediction-Using-CodeBERT/blob/main/Software_Defect_Prediction_with_CodeBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataset Folder"
      ],
      "metadata": {
        "id": "LDecDZLMW1xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir CodeXGLUE\n",
        "%cd CodeXGLUE\n",
        "!git init\n",
        "!git remote add -f origin https://github.com/microsoft/CodeXGLUE.git\n",
        "!git config core.sparseCheckout true\n",
        "!echo \"Code-Code/Defect-detection\" >> .git/info/sparse-checkout\n",
        "!git pull origin main"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T14:58:37.578229Z",
          "iopub.execute_input": "2024-07-05T14:58:37.57854Z",
          "iopub.status.idle": "2024-07-05T14:58:54.442561Z",
          "shell.execute_reply.started": "2024-07-05T14:58:37.578518Z",
          "shell.execute_reply": "2024-07-05T14:58:54.441537Z"
        },
        "trusted": true,
        "id": "xOLdYP8qW1xT",
        "outputId": "f3725625-a12c-495e-eaa3-82aa64ca7e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/working/CodeXGLUE\nInitialized empty Git repository in /kaggle/working/CodeXGLUE/.git/\nUpdating origin\nremote: Enumerating objects: 3373, done.\u001b[K\nremote: Counting objects: 100% (3372/3372), done.\u001b[K\nremote: Compressing objects: 100% (1534/1534), done.\u001b[K\nremote: Total 3373 (delta 1746), reused 3329 (delta 1733), pack-reused 1\u001b[K\nReceiving objects: 100% (3373/3373), 213.15 MiB | 23.16 MiB/s, done.\nResolving deltas: 100% (1746/1746), done.\nFrom https://github.com/microsoft/CodeXGLUE\n * [new branch]      dependabot/pip/Code-Code/TypePrediction-TypeScript/transformers-4.36.0 -> origin/dependabot/pip/Code-Code/TypePrediction-TypeScript/transformers-4.36.0\n * [new branch]      guody5-patch-1 -> origin/guody5-patch-1\n * [new branch]      main           -> origin/main\nFrom https://github.com/microsoft/CodeXGLUE\n * branch            main       -> FETCH_HEAD\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working/CodeXGLUE/Code-Code/Defect-detection"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T14:59:30.391337Z",
          "iopub.execute_input": "2024-07-05T14:59:30.391724Z",
          "iopub.status.idle": "2024-07-05T14:59:30.400238Z",
          "shell.execute_reply.started": "2024-07-05T14:59:30.391693Z",
          "shell.execute_reply": "2024-07-05T14:59:30.399126Z"
        },
        "trusted": true,
        "id": "jQMO-GqjW1xW",
        "outputId": "755cd2c8-041d-4b91-ca6a-2a4dbb677a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/working/CodeXGLUE/Code-Code/Defect-detection\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Dataset DEVIGN"
      ],
      "metadata": {
        "id": "PpQosLdCW1xW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd dataset\n",
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1x6hoF7G-tSYxg8AFybggypLZgMGDNHfF\n",
        "%cd .."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T14:59:31.17085Z",
          "iopub.execute_input": "2024-07-05T14:59:31.171194Z",
          "iopub.status.idle": "2024-07-05T14:59:48.61749Z",
          "shell.execute_reply.started": "2024-07-05T14:59:31.171169Z",
          "shell.execute_reply": "2024-07-05T14:59:48.616013Z"
        },
        "trusted": true,
        "id": "_B5J6LFSW1xW",
        "outputId": "fa38d62d-5eb6-4fe4-c77b-ac2fd15eb687"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset\nCollecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\nDownloading...\nFrom: https://drive.google.com/uc?id=1x6hoF7G-tSYxg8AFybggypLZgMGDNHfF\nTo: /kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/function.json\n100%|██████████████████████████████████████| 61.5M/61.5M [00:00<00:00, 65.5MB/s]\n/kaggle/working/CodeXGLUE/Code-Code/Defect-detection\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd dataset\n",
        "!python preprocess.py\n",
        "%cd .."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T04:59:58.873865Z",
          "iopub.execute_input": "2024-07-02T04:59:58.87417Z",
          "iopub.status.idle": "2024-07-02T05:00:00.887928Z",
          "shell.execute_reply.started": "2024-07-02T04:59:58.874138Z",
          "shell.execute_reply": "2024-07-02T05:00:00.886725Z"
        },
        "trusted": true,
        "id": "CGUbBcyZW1xX",
        "outputId": "6c93567d-ab0a-4902-a662-dfbf743aa60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset\n/kaggle/working/CodeXGLUE/Code-Code/Defect-detection\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Dataset PROMISE"
      ],
      "metadata": {
        "id": "NHfFTqH7W1xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working\n",
        "!pip install gdown\n",
        "!gdown 16FQPgQAFGn3v-j0ZDMkz_wuStkTVq77g"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T14:59:48.619041Z",
          "iopub.execute_input": "2024-07-05T14:59:48.619303Z",
          "iopub.status.idle": "2024-07-05T15:00:30.284632Z",
          "shell.execute_reply.started": "2024-07-05T14:59:48.61928Z",
          "shell.execute_reply": "2024-07-05T15:00:30.283583Z"
        },
        "trusted": true,
        "id": "8zqcZsSfW1xX",
        "outputId": "8ba5edcb-e733-430a-99c7-5030a4598a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/working\nRequirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading...\nFrom: https://drive.google.com/uc?id=16FQPgQAFGn3v-j0ZDMkz_wuStkTVq77g\nTo: /kaggle/working/go_empty.zip\n100%|██████████████████████████████████████| 19.0M/19.0M [00:00<00:00, 46.7MB/s]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working\n",
        "!unzip go_empty.zip"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T15:00:42.179661Z",
          "iopub.execute_input": "2024-07-05T15:00:42.180022Z",
          "iopub.status.idle": "2024-07-05T15:00:43.255006Z",
          "shell.execute_reply.started": "2024-07-05T15:00:42.179976Z",
          "shell.execute_reply": "2024-07-05T15:00:43.253433Z"
        },
        "trusted": true,
        "id": "_wvbM0aFW1xY",
        "outputId": "ee64545c-ab1a-4d12-b110-c0da1e562fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/working\nArchive:  go_empty.zip\n   creating: go_empty/\n  inflating: go_empty/camel-test-balanced.txt  \n  inflating: go_empty/camel-test.txt  \n  inflating: go_empty/camel-train-balanced.txt  \n  inflating: go_empty/camel-train.txt  \n  inflating: go_empty/jedit-test-balanced.txt  \n  inflating: go_empty/jedit-test.txt  \n  inflating: go_empty/jedit-train-balanced.txt  \n  inflating: go_empty/jedit-train.txt  \n  inflating: go_empty/lucene-test-balanced.txt  \n  inflating: go_empty/lucene-test.txt  \n  inflating: go_empty/lucene-train-balanced.txt  \n  inflating: go_empty/lucene-train.txt  \n  inflating: go_empty/poi-test-balanced.txt  \n  inflating: go_empty/poi-test.txt   \n  inflating: go_empty/poi-train-balanced.txt  \n  inflating: go_empty/poi-train.txt  \n  inflating: go_empty/synapse-test-balanced.txt  \n  inflating: go_empty/synapse-test.txt  \n  inflating: go_empty/synapse-train-balanced.txt  \n  inflating: go_empty/synapse-train.txt  \n  inflating: go_empty/w-camel.txt    \n  inflating: go_empty/w-jedit.txt    \n  inflating: go_empty/wo-camel.txt   \n  inflating: go_empty/wo-jedit.txt   \n  inflating: go_empty/xalan-test-balanced.txt  \n  inflating: go_empty/xalan-test.txt  \n  inflating: go_empty/xalan-train-balanced.txt  \n  inflating: go_empty/xalan-train.txt  \n  inflating: go_empty/xerces-test-balanced.txt  \n  inflating: go_empty/xerces-test.txt  \n  inflating: go_empty/xerces-train-balanced.txt  \n  inflating: go_empty/xerces-train.txt  \n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working/go_empty"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T15:00:47.624222Z",
          "iopub.execute_input": "2024-07-05T15:00:47.62458Z",
          "iopub.status.idle": "2024-07-05T15:00:47.63134Z",
          "shell.execute_reply.started": "2024-07-05T15:00:47.62455Z",
          "shell.execute_reply": "2024-07-05T15:00:47.630258Z"
        },
        "trusted": true,
        "id": "xpNn7J6QW1xY",
        "outputId": "1c23e6a9-b23b-4a1d-ad8a-39be13a34cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/working/go_empty\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess PROMISE Dataset"
      ],
      "metadata": {
        "id": "C7O_JxayW1xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "with open('synapse-train.txt', 'r') as file:\n",
        "  data = file.readlines()\n",
        "code = []\n",
        "for i in data:\n",
        "  part = i.split('<CODESPLIT>')\n",
        "  dic = {'target':part[0],'func':part[-1]}\n",
        "  code.append(dic)\n",
        "\n",
        "train = pd.DataFrame(code)\n",
        "train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T15:11:35.19438Z",
          "iopub.execute_input": "2024-07-05T15:11:35.194704Z",
          "iopub.status.idle": "2024-07-05T15:11:35.208426Z",
          "shell.execute_reply.started": "2024-07-05T15:11:35.194679Z",
          "shell.execute_reply": "2024-07-05T15:11:35.207342Z"
        },
        "trusted": true,
        "id": "CCx-3Y2jW1xZ",
        "outputId": "5c4c5704-325a-474b-e812-80525d83291e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  target                                               func\n0      0  package org.apache.synapse.registry; import or...\n1      1  package org.apache.synapse.core; import org.ap...\n2      0  package org.apache.synapse.config.xml; import ...\n3      0  package org.apache.synapse.metrics; import org...\n4      0  package org.apache.synapse.mediators.builtin; ...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>func</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>package org.apache.synapse.registry; import or...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>package org.apache.synapse.core; import org.ap...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>package org.apache.synapse.config.xml; import ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>package org.apache.synapse.metrics; import org...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>package org.apache.synapse.mediators.builtin; ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train),train['target'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T15:11:37.406244Z",
          "iopub.execute_input": "2024-07-05T15:11:37.406612Z",
          "iopub.status.idle": "2024-07-05T15:11:37.414542Z",
          "shell.execute_reply.started": "2024-07-05T15:11:37.406583Z",
          "shell.execute_reply": "2024-07-05T15:11:37.413079Z"
        },
        "trusted": true,
        "id": "OwWfIEcoW1xa",
        "outputId": "77f3ccbf-7bbe-4e77-b56e-8e8991969767"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 32,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(205,\n target\n 0    150\n 1     55\n Name: count, dtype: int64)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('synapse-test.txt', 'r') as file:\n",
        "  data = file.readlines()\n",
        "code = []\n",
        "for i in data:\n",
        "  part = i.split('<CODESPLIT>')\n",
        "  dic = {'target':part[0],'func':part[-1]}\n",
        "  code.append(dic)\n",
        "\n",
        "test = pd.DataFrame(code)\n",
        "test.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T15:11:39.005945Z",
          "iopub.execute_input": "2024-07-05T15:11:39.006378Z",
          "iopub.status.idle": "2024-07-05T15:11:39.021239Z",
          "shell.execute_reply.started": "2024-07-05T15:11:39.006348Z",
          "shell.execute_reply": "2024-07-05T15:11:39.020248Z"
        },
        "trusted": true,
        "id": "85MZyUgUW1xa",
        "outputId": "05de40be-7a12-4311-b294-1b14099ce032"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 33,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  target                                               func\n0      1  package org.apache.synapse.endpoints; import o...\n1      0  package org.apache.synapse.util.concurrent; im...\n2      1  package org.apache.synapse.mediators; import j...\n3      0  package org.apache.synapse.config.xml; import ...\n4      1  package org.apache.synapse.endpoints.algorithm...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>func</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>package org.apache.synapse.endpoints; import o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>package org.apache.synapse.util.concurrent; im...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>package org.apache.synapse.mediators; import j...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>package org.apache.synapse.config.xml; import ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>package org.apache.synapse.endpoints.algorithm...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test),test['target'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T15:11:40.63374Z",
          "iopub.execute_input": "2024-07-05T15:11:40.634109Z",
          "iopub.status.idle": "2024-07-05T15:11:40.641296Z",
          "shell.execute_reply.started": "2024-07-05T15:11:40.634085Z",
          "shell.execute_reply": "2024-07-05T15:11:40.640342Z"
        },
        "trusted": true,
        "id": "OJZKqNIYW1xb",
        "outputId": "b60858fb-d141-4488-c66f-a32070d993f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 34,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(256,\n target\n 0    170\n 1     86\n Name: count, dtype: int64)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create an array with values from 0 to 2895\n",
        "array = np.arange(0, len(train)+len(test))\n",
        "\n",
        "# Shuffle the array\n",
        "np.random.shuffle(array)\n",
        "\n",
        "print(array)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T12:16:40.593863Z",
          "iopub.execute_input": "2024-07-04T12:16:40.59466Z",
          "iopub.status.idle": "2024-07-04T12:16:40.602802Z",
          "shell.execute_reply.started": "2024-07-04T12:16:40.594628Z",
          "shell.execute_reply": "2024-07-04T12:16:40.601913Z"
        },
        "trusted": true,
        "id": "kKU63UtKW1xb",
        "outputId": "24ce0131-673e-402d-e567-0f4c375f5726"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[344 639 379 575 222 837 892 337  48 522 503  75 230 390 596 493 132 799\n   6 899 735 311 216 779 640 415 471 463 664 117 880 917 676 734 820 131\n 325 885 787  55 443 279 401 617 923 386  57 721 563 776 659  65 166 790\n 704 555 786  16 195 447 268 108 600 683 763 842 201 202 385 252 467 857\n 506 286 291 143 394 687 548 101 753 217 233 773 606 218 750 767 592 151\n 168 323 569 164 267 693 135 860  26 778 719 499 430 847 426 730 162 667\n 510 533   0 845 396 916 259 395 142 530  21 527 346 250 496 237 175 206\n 528 231 364   4 878 611 700 865  79 904 912  81 331 677  51 399 227 645\n 321 260 361 357 270 702  38 112 429 468 780 605 481 352 716 552 413 208\n 200 663 694 631 574 884 788  91  17 182 888 757 107 901 146 762 149 410\n 127 113 921 258 421  86 646 841 627 644 334 478 657 739  53 158 414  43\n 419  56 165 914 366 578 504 408 376 392 680 789 111 834 568 269   8 345\n 674 685  12 544 257 295 130 355 284 554 692   5 469 546 887  61 689 728\n 342 654 261  20 717 402 229 359 523  29 890 495 411 766 276 156 858 801\n 647 539 595 551 102 791 444 537 326 624 203 893 736 255 612 808 797 925\n  52  49 266 896 309 405 678 864   3 903 561 328 848 562 675 508 825 219\n 470  93 649 225  33 609 593 782 714 461 137 604 918 710 452 353 515 134\n 163 125 459 811 498 886 679 192 826  94 315 318 358 239 672 187  74 564\n 262 598 176 465 144 754  88 281 707 317 863 462 866 705 748 798 724 457\n 349 247 571 576 665 412 691 755 529 172 335 712 566 371 602 205 876 803\n 635 383 282 298 128 718   9  18 836 583 581 521 875  64 288 907 765 669\n 708 167 476 931  14 210 839 673 810 491 509 877 277 891 774 306 350 638\n  95 189  34 220 869 637 713 874 215 818  82 652 603 559 194 926 400 234\n 265 743 720 771 620 280 115 526 316 784 432 829  69 122 360 126 745 867\n 377 296 438 759  87 161 726 464  98 159 590 695 409 490 322 441 289 398\n  78 308 911 881 915 224 204 621 273 686 531 244 313 138 501 542 451 628\n 894 856  28 249 186 238 422 211 634 251 630 910 507 626 806 729 338 454\n 711 248 424  50 492 843 833 433 633 119 565 287 330 889 460 213 814 929\n 536 656 560   7 813 795 152 466 738 120 403 305 608 930 341 698 428  41\n 343 242 922 632 387 455 221  80 171  23 486  36 761 764 319 668 898 556\n 873 147 110  46 502 121  66 449 300 570 619 835  31 727 851  15 580 688\n 397 622 928 458 129 519 482 336 197 500 732 512 453 586 802 329 332 830\n 796 199 862 822 294 241 214 623 380 388 141  67  19 480 489 852 278 601\n 207 320 370 235 597  10 479 487 769 662 550 895 794 185 715 671 169 154\n 293 511 505 497 354 853 541 153 124 209 369 859 418 285 275 472 477 607\n 855 587 434 670 768 155 351 314 109 648 520 908 240 737 274 193 384 655\n 440   2  54 709 661 150 553 140 643 264 920 932 437 297 347 819 488 770\n  72  24 558 534 145 173 362 641 292 368 792 828 494 741 740 450 681 263\n 404 850 427 815 365 372 485 577 246 697 625 283 431 540 684 446  73 902\n  76 650 180  99 436 872  96 407 178 271 136 475 613  25 747 339 846  59\n 348 582 804 535 363 545 807 445 174  97 196 483 373 100 406 557 924 913\n 304 303 809 882 375 900 518  27 615 105  60 106 696 226 752 425  37 103\n 660 301 191 516  44 844 223 114 378 448 816 389 927 579  35  11 245 610\n 699 116 179 817  89 758 190  22 690 781 391 824 139 919  77 183 870 538\n 658 307 327 751 868  32 800 513 831 618 473 854 439 198 177 420 333 906\n 232 312 883 783 840 871 572 181  39  42 849  62 302 629 823 731 905 356\n 236 589 243 722  58 749 133 484  13  45 374 254 567 157 666  71  63  40\n 212 760 772 706 861 742 651 812 827 393 701 256  83 514 573 775 381 160\n  68 723 756 290  30 616 417 442 897 614 591 104 832 228 703 184 777 838\n 118 599 525  84 416 382 123 532 170 636 543 733 340 148 435 324 423 653\n 785 746 588 642 524  92 584  47 725 253 474 909 682 821 517 594 879 547\n 188 805  90 272 744   1  70 793 367 549 585 310 933  85 456 299]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx=array[:len(train)]\n",
        "test_idx=array[len(train):]\n",
        "len(train_idx),len(test_idx)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T12:16:43.890174Z",
          "iopub.execute_input": "2024-07-04T12:16:43.890893Z",
          "iopub.status.idle": "2024-07-04T12:16:43.897247Z",
          "shell.execute_reply.started": "2024-07-04T12:16:43.890858Z",
          "shell.execute_reply": "2024-07-04T12:16:43.896228Z"
        },
        "trusted": true,
        "id": "IMbt5I9mW1xb",
        "outputId": "ef6ec1a6-4507-439b-bb50-0c0122368eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 87,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(496, 438)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['idx']=train_idx\n",
        "test['idx']=test_idx\n",
        "train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T12:16:45.217345Z",
          "iopub.execute_input": "2024-07-04T12:16:45.2177Z",
          "iopub.status.idle": "2024-07-04T12:16:45.228819Z",
          "shell.execute_reply.started": "2024-07-04T12:16:45.217671Z",
          "shell.execute_reply": "2024-07-04T12:16:45.227905Z"
        },
        "trusted": true,
        "id": "ru3XAQQMW1xc",
        "outputId": "f4277957-7783-47c9-ee23-f5e95ff29fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 88,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  target                                               func  idx\n0      1  package org.apache.poi.hssf.record; import org...  344\n1      0  package org.apache.poi.ddf; public interface E...  639\n2      1  package org.apache.poi.hssf.usermodel; import ...  379\n3      1  package org.apache.poi.hssf.record; import org...  575\n4      1  package org.apache.poi.hssf.record.formula; im...  222",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>func</th>\n      <th>idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>package org.apache.poi.hssf.record; import org...</td>\n      <td>344</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>package org.apache.poi.ddf; public interface E...</td>\n      <td>639</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>package org.apache.poi.hssf.usermodel; import ...</td>\n      <td>379</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>package org.apache.poi.hssf.record; import org...</td>\n      <td>575</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>package org.apache.poi.hssf.record.formula; im...</td>\n      <td>222</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /kaggle/working/dataset/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T08:41:22.203616Z",
          "iopub.execute_input": "2024-07-04T08:41:22.203938Z",
          "iopub.status.idle": "2024-07-04T08:41:23.187898Z",
          "shell.execute_reply.started": "2024-07-04T08:41:22.203914Z",
          "shell.execute_reply": "2024-07-04T08:41:23.186663Z"
        },
        "trusted": true,
        "id": "7TrwFjZ0W1xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_json('/kaggle/working/dataset/train.jsonl', orient='records', lines=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T12:16:49.158276Z",
          "iopub.execute_input": "2024-07-04T12:16:49.158633Z",
          "iopub.status.idle": "2024-07-04T12:16:49.177715Z",
          "shell.execute_reply.started": "2024-07-04T12:16:49.158603Z",
          "shell.execute_reply": "2024-07-04T12:16:49.176953Z"
        },
        "trusted": true,
        "id": "9zHW6XQnW1xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.to_json('/kaggle/working/dataset/test.jsonl', orient='records', lines=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T12:16:51.64461Z",
          "iopub.execute_input": "2024-07-04T12:16:51.645284Z",
          "iopub.status.idle": "2024-07-04T12:16:51.659268Z",
          "shell.execute_reply.started": "2024-07-04T12:16:51.645252Z",
          "shell.execute_reply": "2024-07-04T12:16:51.658389Z"
        },
        "trusted": true,
        "id": "1c0v35IDW1xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Using CodeBERT model"
      ],
      "metadata": {
        "id": "jtyy3kKpW1xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%python\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import math\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "import json\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import multiprocessing\n",
        "cpu_cont = multiprocessing.cpu_count()\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "MODEL_CLASSES = {\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, encoder,config,tokenizer,args):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config=config\n",
        "        self.tokenizer=tokenizer\n",
        "        self.args=args\n",
        "\n",
        "        # Define dropout layer, dropout_probability is taken from args.\n",
        "        self.dropout = nn.Dropout(args.dropout_probability)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None,labels=None):\n",
        "        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\n",
        "\n",
        "        # Apply dropout\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        logits=outputs\n",
        "        prob=torch.sigmoid(logits)\n",
        "        if labels is not None:\n",
        "            labels=labels.float()\n",
        "            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\n",
        "            loss=-loss.mean()\n",
        "            return loss,prob\n",
        "        else:\n",
        "            return prob\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 input_tokens,\n",
        "                 input_ids,\n",
        "                 idx,\n",
        "                 label,\n",
        "\n",
        "    ):\n",
        "        self.input_tokens = input_tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.idx=str(idx)\n",
        "        self.label=label\n",
        "def convert_examples_to_features(js,tokenizer,args):\n",
        "    #source\n",
        "    code=' '.join(js['func'].split())\n",
        "    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\n",
        "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
        "    padding_length = args.block_size - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    return InputFeatures(source_tokens,source_ids,js['idx'],js['target'])\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, file_path=None):\n",
        "        self.examples = []\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                js=json.loads(line.strip())\n",
        "                self.examples.append(convert_examples_to_features(js,tokenizer,args))\n",
        "        if 'train' in file_path:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                    logger.info(\"*** Example ***\")\n",
        "                    logger.info(\"idx: {}\".format(idx))\n",
        "                    logger.info(\"label: {}\".format(example.label))\n",
        "                    logger.info(\"input_tokens: {}\".format([x.replace('\\u0120','_') for x in example.input_tokens]))\n",
        "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
        "                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\n",
        "    args.max_steps=args.epoch*len( train_dataloader)\n",
        "    args.save_steps=len( train_dataloader)\n",
        "    args.warmup_steps=len( train_dataloader)\n",
        "    args.logging_steps=len( train_dataloader)\n",
        "    args.num_train_epochs=args.epoch\n",
        "    model.to(args.device)\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\n",
        "                                                num_training_steps=args.max_steps)\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
        "    optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
        "    if os.path.exists(scheduler_last):\n",
        "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
        "    if os.path.exists(optimizer_last):\n",
        "        optimizer.load_state_dict(torch.load(optimizer_last))\n",
        "    # Train!\n",
        "    print(\"***** Running training *****\")\n",
        "    print(\"  Num examples = %d\", len(train_dataset))\n",
        "    print(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
        "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    print(\"  Total optimization steps = %d\", args.max_steps)\n",
        "\n",
        "    global_step = args.start_step\n",
        "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
        "    best_mrr=0.0\n",
        "    best_acc=0.0\n",
        "    # model.resize_token_embeddings(len(tokenizer))\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Initialize early stopping parameters at the start of training\n",
        "    early_stopping_counter = 0\n",
        "    best_loss = None\n",
        "\n",
        "    for idx in range(args.start_epoch, int(args.num_train_epochs)):\n",
        "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "        tr_num=0\n",
        "        train_loss=0\n",
        "        acc = 0\n",
        "        for step, batch in enumerate(bar):\n",
        "            inputs = batch[0].to(args.device)\n",
        "            labels=batch[1].to(args.device)\n",
        "            model.train()\n",
        "            loss,logits = model(inputs,labels)\n",
        "\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            preds=logits[:,0]>0.5\n",
        "            acc+=np.mean(labels==preds)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            tr_num+=1\n",
        "            train_loss+=loss.item()\n",
        "            if avg_loss==0:\n",
        "                avg_loss=tr_loss\n",
        "            avg_loss=round(train_loss/tr_num,5)\n",
        "            avg_acc = round(acc/tr_num,5)\n",
        "            bar.set_description(\"epoch {} loss {} acc {}\".format(idx,avg_loss,avg_acc))\n",
        "\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "                output_flag=True\n",
        "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    logging_loss = tr_loss\n",
        "                    tr_nb=global_step\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer,eval_when_training=True)\n",
        "                        for key, value in results.items():\n",
        "                            print(\"  %s = %s\", key, round(value,4))\n",
        "                        # Save model checkpoint\n",
        "\n",
        "                    if results['eval_acc']>best_acc:\n",
        "                        best_acc=results['eval_acc']\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "                        print(\"  Best acc:%s\",round(best_acc,4))\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "\n",
        "                        checkpoint_prefix = 'checkpoint-best-acc'\n",
        "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "                        if not os.path.exists(output_dir):\n",
        "                            os.makedirs(output_dir)\n",
        "                        model_to_save = model.module if hasattr(model,'module') else model\n",
        "                        output_dir = os.path.join(output_dir, '{}'.format('model.bin'))\n",
        "                        torch.save(model_to_save.state_dict(), output_dir)\n",
        "                        print(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = train_loss / tr_num\n",
        "\n",
        "        # Check for early stopping condition\n",
        "        if args.early_stopping_patience is not None:\n",
        "            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\n",
        "                best_loss = avg_loss\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= args.early_stopping_patience:\n",
        "                    print(\"Early stopping\")\n",
        "                    break  # Exit the loop early\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer,eval_when_training=False):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1 and eval_when_training is False:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running evaluation *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in eval_dataloader:\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            lm_loss,logit = model(inputs,label)\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "        nb_eval_steps += 1\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    eval_acc=np.mean(labels==preds)\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.tensor(eval_loss)\n",
        "\n",
        "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
        "    cm = confusion_matrix(y_true=labels, y_pred=preds)\n",
        "    tp = cm[1][1]\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    precision = 0 if tp + fp == 0 else tp/(tp+fp)\n",
        "    recall = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    fprate = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    pdVal = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    pfVal = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    G = 0 if pdVal+1-pfVal == 0 else 2*pdVal*(1-pfVal)/(pdVal+1-pfVal)\n",
        "    F1 = 0 if precision+recall == 0 else 2 * precision * recall / (precision+recall)\n",
        "    MCC = 0 if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) == 0 else (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
        "    result = {\n",
        "        \"eval_loss\": float(perplexity),\n",
        "        \"eval_acc\":round(eval_acc,4),\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"G\": G,\n",
        "        \"MCC\": MCC\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def test(args, model, tokenizer):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\n",
        "\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running Test *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            logit = model(inputs)\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    with open(os.path.join(args.output_dir,\"predictions.txt\"),'w') as f:\n",
        "        for example,pred in zip(eval_dataset.examples,preds):\n",
        "            if pred:\n",
        "                f.write(example.idx+'\\t1\\n')\n",
        "            else:\n",
        "                f.write(example.idx+'\\t0\\n')\n",
        "\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu\n",
        "    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu\n",
        "    # Setup logging\n",
        "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    args.start_epoch = 0\n",
        "    args.start_step = 0\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
        "        args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
        "        args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
        "        idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
        "        with open(idx_file, encoding='utf-8') as idxf:\n",
        "            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
        "\n",
        "        step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
        "        if os.path.exists(step_file):\n",
        "            with open(step_file, encoding='utf-8') as stepf:\n",
        "                args.start_step = int(stepf.readlines()[0].strip())\n",
        "\n",
        "        print(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))\n",
        "\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
        "                                          cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    config.num_labels=1\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\n",
        "                                                do_lower_case=args.do_lower_case,\n",
        "                                                cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    if args.block_size <= 0:\n",
        "        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "    if args.model_name_or_path:\n",
        "        model = model_class.from_pretrained(args.model_name_or_path,\n",
        "                                            from_tf=bool('.ckpt' in args.model_name_or_path),\n",
        "                                            config=config,\n",
        "                                            cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    else:\n",
        "        model = model_class(config)\n",
        "\n",
        "    model=Model(model,config,tokenizer,args)\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    print(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        if args.local_rank not in [-1, 0]:\n",
        "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\n",
        "        if args.local_rank == 0:\n",
        "            torch.distributed.barrier()\n",
        "\n",
        "        train(args, train_dataset, model, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            result=evaluate(args, model, tokenizer)\n",
        "            print(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                print(\"  %s = %s\", key, str(round(result[key],4)))\n",
        "\n",
        "    if args.do_test and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            test(args, model, tokenizer)\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
        "                        help=\"The input training data file (a text file).\")\n",
        "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "    parser.add_argument(\"--test_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "\n",
        "    parser.add_argument(\"--model_type\", default=\"bert\", type=str,\n",
        "                        help=\"The model architecture to be fine-tuned.\")\n",
        "    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\n",
        "                        help=\"The model checkpoint for weights initialization.\")\n",
        "\n",
        "    parser.add_argument(\"--mlm\", action='store_true',\n",
        "                        help=\"Train with masked-language modeling loss instead of language modeling.\")\n",
        "    parser.add_argument(\"--mlm_probability\", type=float, default=0.15,\n",
        "                        help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
        "\n",
        "    parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
        "                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\n",
        "    parser.add_argument(\"--block_size\", default=-1, type=int,\n",
        "                        help=\"Optional input sequence length after tokenization.\"\n",
        "                             \"The training dataset will be truncated in block of this size for training.\"\n",
        "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
        "    parser.add_argument(\"--do_train\", action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--do_test\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
        "                        help=\"Run evaluation during training at each logging step.\")\n",
        "    parser.add_argument(\"--do_lower_case\", action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "\n",
        "    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
        "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight deay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                        help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", default=1.0, type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
        "                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument('--logging_steps', type=int, default=50,\n",
        "                        help=\"Log every X updates steps.\")\n",
        "    parser.add_argument('--save_steps', type=int, default=50,\n",
        "                        help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument('--save_total_limit', type=int, default=None,\n",
        "                        help='Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default')\n",
        "    parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n",
        "                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\n",
        "    parser.add_argument(\"--no_cuda\", action='store_true',\n",
        "                        help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument('--overwrite_output_dir', action='store_true',\n",
        "                        help=\"Overwrite the content of the output directory\")\n",
        "    parser.add_argument('--overwrite_cache', action='store_true',\n",
        "                        help=\"Overwrite the cached training and evaluation sets\")\n",
        "    parser.add_argument('--seed', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--epoch', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--fp16', action='store_true',\n",
        "                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
        "    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
        "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                        help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument('--server_ip', type=str, default='', help=\"For distant debugging.\")\n",
        "    parser.add_argument('--server_port', type=str, default='', help=\"For distant debugging.\")\n",
        "\n",
        "    # Add early stopping parameters and dropout probability parameters\n",
        "    parser.add_argument(\"--early_stopping_patience\", type=int, default=None,\n",
        "                        help=\"Number of epochs with no improvement after which training will be stopped.\")\n",
        "    parser.add_argument(\"--min_loss_delta\", type=float, default=0.001,\n",
        "                        help=\"Minimum change in the loss required to qualify as an improvement.\")\n",
        "    parser.add_argument('--dropout_probability', type=float, default=0, help='dropout probability')\n",
        "\n",
        "    # Simulate command-line arguments\n",
        "    simulated_args = [\n",
        "        '--output_dir', './saved_models',\n",
        "        '--model_type', 'roberta',\n",
        "        '--tokenizer_name', 'microsoft/codebert-base',\n",
        "        '--model_name_or_path', 'microsoft/codebert-base',\n",
        "        '--do_train',\n",
        "        '--train_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl',\n",
        "        '--eval_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl',\n",
        "        '--test_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl',\n",
        "        '--epoch', '5',\n",
        "        '--block_size', '512',\n",
        "        '--train_batch_size', '16',\n",
        "        '--eval_batch_size', '16',\n",
        "        '--learning_rate', '2e-5',\n",
        "        '--max_grad_norm', '1.0',\n",
        "        '--evaluate_during_training',\n",
        "        '--seed', '123456'\n",
        "    ]\n",
        "\n",
        "    # Parse the simulated arguments\n",
        "    args = parser.parse_args(simulated_args)\n",
        "\n",
        "    main(args)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-03T15:10:02.333546Z",
          "iopub.execute_input": "2024-07-03T15:10:02.334282Z",
          "iopub.status.idle": "2024-07-03T15:10:34.071125Z",
          "shell.execute_reply.started": "2024-07-03T15:10:02.334249Z",
          "shell.execute_reply": "2024-07-03T15:10:34.067073Z"
        },
        "scrolled": true,
        "trusted": true,
        "id": "J5yG8jaDW1xg",
        "outputId": "e663735f-710c-49f0-d6a5-b595ce0d0636"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-07-03 15:10:08.449734: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-03 15:10:08.449839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-03 15:10:08.576795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training/evaluation parameters %s Namespace(train_data_file='/kaggle/working/dataset/train.jsonl', output_dir='./saved_models', eval_data_file='/kaggle/working/dataset/test.jsonl', test_data_file='/kaggle/working/dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='', block_size=510, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=16, eval_batch_size=16, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=10, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, start_epoch=0, start_step=0)\n***** Running training *****\n  Num examples = %d 1404\n  Num Epochs = %d 10\n  Instantaneous batch size per GPU = %d 16\n  Total train batch size (w. parallel, distributed & accumulation) = %d 16\n  Gradient Accumulation steps = %d 1\n  Total optimization steps = %d 880\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "  0%|          | 0/88 [00:00<?, ?it/s]\nTraceback (most recent call last):\n  File \"<stdin>\", line 623, in <module>\n  File \"<stdin>\", line 473, in main\n  File \"<stdin>\", line 188, in train\n  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1182, in __iter__\n    for obj in iterable:\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n    data = self._next_data()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1345, in _next_data\n    return self._process_data(data)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\n    data.reraise()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_utils.py\", line 694, in reraise\n    raise exception\nTypeError: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<stdin>\", line 106, in __getitem__\nTypeError: new(): invalid data type 'str'\n\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfrom __future__ import absolute_import, division, print_function\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport argparse\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport glob\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport logging\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport os\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport pickle\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport random\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport re\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport shutil\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport math\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom sklearn.metrics import f1_score, confusion_matrix\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport numpy as np\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport torch\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom torch.utils.data.distributed import DistributedSampler\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport torch.nn as nn\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom torch.autograd import Variable\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport copy\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom torch.nn import CrossEntropyLoss, MSELoss\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport json\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtry:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from torch.utils.tensorboard import SummaryWriter\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mexcept:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from tensorboardX import SummaryWriter\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom tqdm import tqdm, trange\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport multiprocessing\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mcpu_cont = multiprocessing.cpu_count()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mlogger = logging.getLogger(__name__)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMODEL_CLASSES = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mroberta\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mclass Model(nn.Module):   \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def __init__(self, encoder,config,tokenizer,args):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        super(Model, self).__init__()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.encoder = encoder\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.config=config\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.tokenizer=tokenizer\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.args=args\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # Define dropout layer, dropout_probability is taken from args.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.dropout = nn.Dropout(args.dropout_probability)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def forward(self, input_ids=None,labels=None): \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # Apply dropout\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        outputs = self.dropout(outputs)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        logits=outputs\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        prob=torch.sigmoid(logits)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        if labels is not None:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            labels=labels.float()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            loss=-loss.mean()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            return loss,prob\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        else:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            return prob\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mclass InputFeatures(object):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA single training/test features for a example.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def __init__(self,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 input_tokens,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 input_ids,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 idx,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 label,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.input_tokens = input_tokens\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.input_ids = input_ids\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.idx=str(idx)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.label=label\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef convert_examples_to_features(js,tokenizer,args):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    #source\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    code=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m.join(js[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mfunc\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].split())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    padding_length = args.block_size - len(source_ids)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    source_ids+=[tokenizer.pad_token_id]*padding_length\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return InputFeatures(source_tokens,source_ids,js[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43midx\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m],js[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mclass TextDataset(Dataset):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def __init__(self, tokenizer, args, file_path=None):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.examples = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        with open(file_path) as f:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            for line in f:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                js=json.loads(line.strip())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                self.examples.append(convert_examples_to_features(js,tokenizer,args))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        if \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m in file_path:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            for idx, example in enumerate(self.examples[:3]):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    logger.info(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*** Example ***\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    logger.info(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43midx: \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.format(idx))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    logger.info(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel: \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.format(example.label))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    logger.info(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_tokens: \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.format([x.replace(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mu0120\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m) for x in example.input_tokens]))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    logger.info(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids: \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.format(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m.join(map(str, example.input_ids))))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def __len__(self):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        return len(self.examples)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def __getitem__(self, i):       \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef set_seed(seed=42):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    random.seed(seed)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    os.environ[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mPYHTONHASHSEED\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = str(seed)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    np.random.seed(seed)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    torch.manual_seed(seed)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    torch.cuda.manual_seed(seed)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    torch.backends.cudnn.deterministic = True\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef train(args, train_dataset, model, tokenizer):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m Train the model \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.max_steps=args.epoch*len( train_dataloader)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.save_steps=len( train_dataloader)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.warmup_steps=len( train_dataloader)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.logging_steps=len( train_dataloader)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.num_train_epochs=args.epoch\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.to(args.device)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Prepare optimizer and schedule (linear warmup and decay)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    no_decay = [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mbias\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mLayerNorm.weight\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    optimizer_grouped_parameters = [\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m         \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: args.weight_decay},\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m: 0.0}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                                                num_training_steps=args.max_steps)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.fp16:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        try:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            from apex import amp\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        except ImportError:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            raise ImportError(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlease install apex from https://www.github.com/nvidia/apex to use fp16 training.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # multi-gpu training (should be after apex fp16 initialization)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.n_gpu > 1:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model = torch.nn.DataParallel(model)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Distributed training (should be after apex fp16 initialization)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.local_rank != -1:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                                                          output_device=args.local_rank,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                                                          find_unused_parameters=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    checkpoint_last = os.path.join(args.output_dir, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcheckpoint-last\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    scheduler_last = os.path.join(checkpoint_last, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mscheduler.pt\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    optimizer_last = os.path.join(checkpoint_last, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43moptimizer.pt\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if os.path.exists(scheduler_last):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        scheduler.load_state_dict(torch.load(scheduler_last))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if os.path.exists(optimizer_last):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        optimizer.load_state_dict(torch.load(optimizer_last))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Train!\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m***** Running training *****\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Num examples = \u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, len(train_dataset))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Num Epochs = \u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, args.num_train_epochs)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Instantaneous batch size per GPU = \u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, args.per_gpu_train_batch_size)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Total train batch size (w. parallel, distributed & accumulation) = \u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                args.train_batch_size * args.gradient_accumulation_steps * (\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Gradient Accumulation steps = \u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, args.gradient_accumulation_steps)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Total optimization steps = \u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, args.max_steps)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    global_step = args.start_step\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    best_mrr=0.0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    best_acc=0.0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # model.resize_token_embeddings(len(tokenizer))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.zero_grad()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Initialize early stopping parameters at the start of training\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    early_stopping_counter = 0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    best_loss = None\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    for idx in range(args.start_epoch, int(args.num_train_epochs)): \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        bar = tqdm(train_dataloader,total=len(train_dataloader))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        tr_num=0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        train_loss=0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        acc = 0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        for step, batch in enumerate(bar):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            inputs = batch[0].to(args.device)        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            labels=batch[1].to(args.device) \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            model.train()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            loss,logits = model(inputs,labels)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            if args.n_gpu > 1:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                loss = loss.mean()  # mean() to average on multi-gpu parallel training\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            if args.gradient_accumulation_steps > 1:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                loss = loss / args.gradient_accumulation_steps\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            if args.fp16:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                with amp.scale_loss(loss, optimizer) as scaled_loss:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    scaled_loss.backward()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            else:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                loss.backward()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            logits = logits.detach().cpu().numpy()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            labels = labels.detach().cpu().numpy()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            preds=logits[:,0]>0.5\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            acc+=np.mean(labels==preds)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            tr_loss += loss.item()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            tr_num+=1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            train_loss+=loss.item()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            if avg_loss==0:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                avg_loss=tr_loss\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            avg_loss=round(train_loss/tr_num,5)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            avg_acc = round(acc/tr_num,5)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            bar.set_description(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m loss \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m acc \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.format(idx,avg_loss,avg_acc))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            if (step + 1) \u001b[39;49m\u001b[38;5;132;43;01m% a\u001b[39;49;00m\u001b[38;5;124;43mrgs.gradient_accumulation_steps == 0:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                optimizer.step()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                optimizer.zero_grad()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                scheduler.step()  \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                global_step += 1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                output_flag=True\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step \u001b[39;49m\u001b[38;5;132;43;01m% a\u001b[39;49;00m\u001b[38;5;124;43mrgs.logging_steps == 0:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    logging_loss = tr_loss\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    tr_nb=global_step\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step \u001b[39;49m\u001b[38;5;132;43;01m% a\u001b[39;49;00m\u001b[38;5;124;43mrgs.save_steps == 0:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        results = evaluate(args, model, tokenizer,eval_when_training=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        for key, value in results.items():\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                            print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m = \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, key, round(value,4))                    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        # Save model checkpoint\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    if results[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43meval_acc\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]>best_acc:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        best_acc=results[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43meval_acc\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*20)  \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Best acc:\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,round(best_acc,4))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*20)                          \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        checkpoint_prefix = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcheckpoint-best-acc\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        output_dir = os.path.join(args.output_dir, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m.format(checkpoint_prefix))                        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        if not os.path.exists(output_dir):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                            os.makedirs(output_dir)                        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        model_to_save = model.module if hasattr(model,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mmodule\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m) else model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        output_dir = os.path.join(output_dir, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m.format(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mmodel.bin\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)) \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        torch.save(model_to_save.state_dict(), output_dir)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSaving model checkpoint to \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, output_dir)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # Calculate average loss for the epoch\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        avg_loss = train_loss / tr_num\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # Check for early stopping condition\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        if args.early_stopping_patience is not None:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                best_loss = avg_loss\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                early_stopping_counter = 0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            else:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                early_stopping_counter += 1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                if early_stopping_counter >= args.early_stopping_patience:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEarly stopping\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    break  # Exit the loop early\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef evaluate(args, model, tokenizer,eval_when_training=False):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Loop to handle MNLI double evaluation (matched, mis-matched)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    eval_output_dir = args.output_dir\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        os.makedirs(eval_output_dir)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Note that DistributedSampler samples randomly\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # multi-gpu evaluate\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.n_gpu > 1 and eval_when_training is False:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model = torch.nn.DataParallel(model)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Eval!\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m***** Running evaluation *****\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Num examples = \u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, len(eval_dataset))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Batch size = \u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, args.eval_batch_size)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    eval_loss = 0.0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    nb_eval_steps = 0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.eval()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logits=[] \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    labels=[]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    for batch in eval_dataloader:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        inputs = batch[0].to(args.device)        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        label=batch[1].to(args.device) \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        with torch.no_grad():\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            lm_loss,logit = model(inputs,label)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            eval_loss += lm_loss.mean().item()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            logits.append(logit.cpu().numpy())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            labels.append(label.cpu().numpy())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        nb_eval_steps += 1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logits=np.concatenate(logits,0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    labels=np.concatenate(labels,0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    preds=logits[:,0]>0.5\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    eval_acc=np.mean(labels==preds)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    eval_loss = eval_loss / nb_eval_steps\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    perplexity = torch.tensor(eval_loss)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    f1 = f1_score(y_true=labels, y_pred=preds)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    cm = confusion_matrix(y_true=labels, y_pred=preds)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    tp = cm[1][1]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    tn = cm[0][0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    fp = cm[0][1]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    fn = cm[1][0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    precision = 0 if tp + fp == 0 else tp/(tp+fp)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    recall = 0 if tp + fn == 0 else tp/(tp+fn)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    fprate = 0 if fp + tn == 0 else fp/(fp+tn)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    pdVal = 0 if tp + fn == 0 else tp/(tp+fn)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    pfVal = 0 if fp + tn == 0 else fp/(fp+tn)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    G = 0 if pdVal+1-pfVal == 0 else 2*pdVal*(1-pfVal)/(pdVal+1-pfVal)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    F1 = 0 if precision+recall == 0 else 2 * precision * recall / (precision+recall)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    MCC = 0 if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) == 0 else (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    result = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: float(perplexity),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_acc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:round(eval_acc,4),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: f1,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: precision,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: recall,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: G,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMCC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: MCC\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    }\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return result\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef test(args, model, tokenizer):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Loop to handle MNLI double evaluation (matched, mis-matched)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Note that DistributedSampler samples randomly\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # multi-gpu evaluate\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.n_gpu > 1:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model = torch.nn.DataParallel(model)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Eval!\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m***** Running Test *****\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Num examples = \u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, len(eval_dataset))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Batch size = \u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, args.eval_batch_size)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    eval_loss = 0.0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    nb_eval_steps = 0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.eval()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logits=[]   \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    labels=[]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        inputs = batch[0].to(args.device)        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        label=batch[1].to(args.device) \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        with torch.no_grad():\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            logit = model(inputs)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            logits.append(logit.cpu().numpy())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            labels.append(label.cpu().numpy())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logits=np.concatenate(logits,0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    labels=np.concatenate(labels,0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    preds=logits[:,0]>0.5\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    with open(os.path.join(args.output_dir,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredictions.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m) as f:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        for example,pred in zip(eval_dataset.examples,preds):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            if pred:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                f.write(example.idx+\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mt1\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            else:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                f.write(example.idx+\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mt0\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef main(args):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Setup distant debugging if needed\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.server_ip and args.server_port:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        import ptvsd\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWaiting for debugger attach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        ptvsd.wait_for_attach()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Setup CUDA, GPU & distributed training\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.local_rank == -1 or args.no_cuda:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        device = torch.device(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m if torch.cuda.is_available() and not args.no_cuda else \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        args.n_gpu = torch.cuda.device_count()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        torch.cuda.set_device(args.local_rank)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        device = torch.device(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, args.local_rank)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        torch.distributed.init_process_group(backend=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mnccl\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        args.n_gpu = 1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.device = device\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Setup logging\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logging.basicConfig(format=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;132;43;01m%(asctime)s\u001b[39;49;00m\u001b[38;5;124;43m - \u001b[39;49m\u001b[38;5;132;43;01m%(levelname)s\u001b[39;49;00m\u001b[38;5;124;43m - \u001b[39;49m\u001b[38;5;132;43;01m%(name)s\u001b[39;49;00m\u001b[38;5;124;43m -   \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        datefmt=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm/\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logger.warning(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcess rank: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m, device: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m, n_gpu: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m, distributed training: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m, 16-bits training: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Set seed\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    set_seed(args.seed)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Load pretrained model and tokenizer\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.local_rank not in [-1, 0]:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.start_epoch = 0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.start_step = 0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    checkpoint_last = os.path.join(args.output_dir, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcheckpoint-last\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        args.model_name_or_path = os.path.join(checkpoint_last, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mpytorch_model.bin\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        args.config_name = os.path.join(checkpoint_last, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mconfig.json\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        idx_file = os.path.join(checkpoint_last, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43midx_file.txt\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        with open(idx_file, encoding=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m) as idxf:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        step_file = os.path.join(checkpoint_last, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstep_file.txt\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        if os.path.exists(step_file):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            with open(step_file, encoding=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m) as stepf:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                args.start_step = int(stepf.readlines()[0].strip())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreload model from \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, resume from \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.format(checkpoint_last, args.start_epoch))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                                          cache_dir=args.cache_dir if args.cache_dir else None)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    config.num_labels=1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                                                do_lower_case=args.do_lower_case,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                                                cache_dir=args.cache_dir if args.cache_dir else None)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.block_size <= 0:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.model_name_or_path:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model = model_class.from_pretrained(args.model_name_or_path,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                                            from_tf=bool(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m.ckpt\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m in args.model_name_or_path),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                                            config=config,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                                            cache_dir=args.cache_dir if args.cache_dir else None)    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    else:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model = model_class(config)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model=Model(model,config,tokenizer,args)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.local_rank == 0:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining/evaluation parameters \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, args)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Training\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.do_train:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        if args.local_rank not in [-1, 0]:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        if args.local_rank == 0:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            torch.distributed.barrier()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        train(args, train_dataset, model, tokenizer)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Evaluation\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    results = \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.do_eval and args.local_rank in [-1, 0]:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            checkpoint_prefix = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcheckpoint-best-acc/model.bin\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            output_dir = os.path.join(args.output_dir, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m.format(checkpoint_prefix))  \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            model.load_state_dict(torch.load(output_dir))      \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            model.to(args.device)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            result=evaluate(args, model, tokenizer)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m***** Eval results *****\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            for key in sorted(result.keys()):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                print(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m = \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, key, str(round(result[key],4)))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if args.do_test and args.local_rank in [-1, 0]:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            checkpoint_prefix = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mcheckpoint-best-acc/model.bin\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            output_dir = os.path.join(args.output_dir, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m.format(checkpoint_prefix))  \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            model.load_state_dict(torch.load(output_dir))                  \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            model.to(args.device)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            test(args, model, tokenizer)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return results\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mif __name__ == \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__main__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser = argparse.ArgumentParser()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ## Required parameters\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--train_data_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=None, type=str, required=True,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe input training data file (a text file).\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--output_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=None, type=str, required=True,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe output directory where the model predictions and checkpoints will be written.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ## Other parameters\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--eval_data_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=None, type=str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAn optional input evaluation data file to evaluate the perplexity on (a text file).\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--test_data_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=None, type=str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAn optional input evaluation data file to evaluate the perplexity on (a text file).\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--model_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, type=str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe model architecture to be fine-tuned.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--model_name_or_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=None, type=str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe model checkpoint for weights initialization.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--mlm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, action=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrain with masked-language modeling loss instead of language modeling.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--mlm_probability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, type=float, default=0.15,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRatio of tokens to mask for masked language modeling loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--config_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, type=str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptional pretrained config name or path if not the same as model_name_or_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--tokenizer_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, type=str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptional pretrained tokenizer name or path if not the same as model_name_or_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--cache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, type=str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptional directory to store the pre-trained models downloaded from s3 (instread of the default one)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--block_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=-1, type=int,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptional input sequence length after tokenization.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                             \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe training dataset will be truncated in block of this size for training.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                             \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDefault to the model max input length for single sentence inputs (take into account special tokens).\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--do_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, action=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhether to run training.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--do_eval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, action=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhether to run eval on the dev set.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--do_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, action=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhether to run eval on the dev set.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--evaluate_during_training\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, action=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRun evaluation during training at each logging step.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--do_lower_case\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, action=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSet this flag if you are using an uncased model.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--train_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=4, type=int,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch size per GPU/CPU for training.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--eval_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=4, type=int,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch size per GPU/CPU for evaluation.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--gradient_accumulation_steps\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, type=int, default=1,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNumber of updates steps to accumulate before performing a backward/update pass.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--learning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=5e-5, type=float,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe initial learning rate for Adam.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=0.0, type=float,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWeight deay if we apply some.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--adam_epsilon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=1e-8, type=float,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpsilon for Adam optimizer.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--max_grad_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=1.0, type=float,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMax gradient norm.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--num_train_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=1.0, type=float,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTotal number of training epochs to perform.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--max_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=-1, type=int,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIf > 0: set total number of training steps to perform. Override num_train_epochs.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--warmup_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, default=0, type=int,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLinear warmup over warmup_steps.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--logging_steps\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, type=int, default=50,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLog every X updates steps.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--save_steps\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, type=int, default=50,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSave checkpoint every X updates steps.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--save_total_limit\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, type=int, default=None,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mLimit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--eval_all_checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, action=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--no_cuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, action=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAvoid using CUDA when available\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--overwrite_output_dir\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, action=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOverwrite the content of the output directory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--overwrite_cache\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, action=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOverwrite the cached training and evaluation sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--seed\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, type=int, default=42,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom seed for initialization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--epoch\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, type=int, default=42,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom seed for initialization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--fp16\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, action=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstore_true\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--fp16_opt_level\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, type=str, default=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mO1\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFor fp16: Apex AMP optimization level selected in [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mO0\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mO1\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mO2\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, and \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mO3\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                             \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSee details at https://nvidia.github.io/apex/amp.html\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--local_rank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, type=int, default=-1,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFor distributed training: local_rank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--server_ip\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, type=str, default=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFor distant debugging.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--server_port\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, type=str, default=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFor distant debugging.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Add early stopping parameters and dropout probability parameters\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--early_stopping_patience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, type=int, default=None,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNumber of epochs with no improvement after which training will be stopped.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--min_loss_delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, type=float, default=0.001,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        help=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMinimum change in the loss required to qualify as an improvement.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--dropout_probability\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, type=float, default=0, help=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mdropout probability\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Simulate command-line arguments\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    simulated_args = [\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--output_dir\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m./saved_models\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--model_type\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mroberta\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--tokenizer_name\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mmicrosoft/codebert-base\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--model_name_or_path\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mmicrosoft/codebert-base\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--do_train\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--train_data_file\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m/kaggle/working/dataset/train.jsonl\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--eval_data_file\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m/kaggle/working/dataset/test.jsonl\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--test_data_file\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m/kaggle/working/dataset/test.jsonl\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--epoch\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m10\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--block_size\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m512\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--train_batch_size\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m16\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--eval_batch_size\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m16\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--learning_rate\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m2e-5\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--max_grad_norm\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m1.0\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--evaluate_during_training\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m--seed\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m123456\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Parse the simulated arguments\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args = parser.parse_args(simulated_args)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    main(args)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'from __future__ import absolute_import, division, print_function\\n\\nimport argparse\\nimport glob\\nimport logging\\nimport os\\nimport pickle\\nimport random\\nimport re\\nimport shutil\\nimport math\\nfrom sklearn.metrics import f1_score, confusion_matrix\\n\\nimport numpy as np\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\\nfrom torch.utils.data.distributed import DistributedSampler\\nimport torch.nn as nn\\nfrom torch.autograd import Variable\\nimport copy\\nfrom torch.nn import CrossEntropyLoss, MSELoss\\nimport json\\ntry:\\n    from torch.utils.tensorboard import SummaryWriter\\nexcept:\\n    from tensorboardX import SummaryWriter\\n\\nfrom tqdm import tqdm, trange\\nimport multiprocessing\\ncpu_cont = multiprocessing.cpu_count()\\nfrom transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\\n                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\\n\\nlogger = logging.getLogger(__name__)\\nMODEL_CLASSES = {\\n    \\'roberta\\': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\\n}\\nclass Model(nn.Module):   \\n    def __init__(self, encoder,config,tokenizer,args):\\n        super(Model, self).__init__()\\n        self.encoder = encoder\\n        self.config=config\\n        self.tokenizer=tokenizer\\n        self.args=args\\n    \\n        # Define dropout layer, dropout_probability is taken from args.\\n        self.dropout = nn.Dropout(args.dropout_probability)\\n\\n        \\n    def forward(self, input_ids=None,labels=None): \\n        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\\n\\n        # Apply dropout\\n        outputs = self.dropout(outputs)\\n\\n        logits=outputs\\n        prob=torch.sigmoid(logits)\\n        if labels is not None:\\n            labels=labels.float()\\n            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\\n            loss=-loss.mean()\\n            return loss,prob\\n        else:\\n            return prob\\nclass InputFeatures(object):\\n    \"\"\"A single training/test features for a example.\"\"\"\\n    def __init__(self,\\n                 input_tokens,\\n                 input_ids,\\n                 idx,\\n                 label,\\n\\n    ):\\n        self.input_tokens = input_tokens\\n        self.input_ids = input_ids\\n        self.idx=str(idx)\\n        self.label=label\\ndef convert_examples_to_features(js,tokenizer,args):\\n    #source\\n    code=\\' \\'.join(js[\\'func\\'].split())\\n    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\\n    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\\n    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\\n    padding_length = args.block_size - len(source_ids)\\n    source_ids+=[tokenizer.pad_token_id]*padding_length\\n    return InputFeatures(source_tokens,source_ids,js[\\'idx\\'],js[\\'target\\'])\\nclass TextDataset(Dataset):\\n    def __init__(self, tokenizer, args, file_path=None):\\n        self.examples = []\\n        with open(file_path) as f:\\n            for line in f:\\n                js=json.loads(line.strip())\\n                self.examples.append(convert_examples_to_features(js,tokenizer,args))\\n        if \\'train\\' in file_path:\\n            for idx, example in enumerate(self.examples[:3]):\\n                    logger.info(\"*** Example ***\")\\n                    logger.info(\"idx: {}\".format(idx))\\n                    logger.info(\"label: {}\".format(example.label))\\n                    logger.info(\"input_tokens: {}\".format([x.replace(\\'\\\\u0120\\',\\'_\\') for x in example.input_tokens]))\\n                    logger.info(\"input_ids: {}\".format(\\' \\'.join(map(str, example.input_ids))))\\n\\n    def __len__(self):\\n        return len(self.examples)\\n\\n    def __getitem__(self, i):       \\n        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\\ndef set_seed(seed=42):\\n    random.seed(seed)\\n    os.environ[\\'PYHTONHASHSEED\\'] = str(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    torch.backends.cudnn.deterministic = True\\ndef train(args, train_dataset, model, tokenizer):\\n    \"\"\" Train the model \"\"\" \\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\\n    \\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, \\n                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\\n    args.max_steps=args.epoch*len( train_dataloader)\\n    args.save_steps=len( train_dataloader)\\n    args.warmup_steps=len( train_dataloader)\\n    args.logging_steps=len( train_dataloader)\\n    args.num_train_epochs=args.epoch\\n    model.to(args.device)\\n    # Prepare optimizer and schedule (linear warmup and decay)\\n    no_decay = [\\'bias\\', \\'LayerNorm.weight\\']\\n    optimizer_grouped_parameters = [\\n        {\\'params\\': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\\n         \\'weight_decay\\': args.weight_decay},\\n        {\\'params\\': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \\'weight_decay\\': 0.0}\\n    ]\\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\\n                                                num_training_steps=args.max_steps)\\n    if args.fp16:\\n        try:\\n            from apex import amp\\n        except ImportError:\\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\\n\\n    # multi-gpu training (should be after apex fp16 initialization)\\n    if args.n_gpu > 1:\\n        model = torch.nn.DataParallel(model)\\n\\n    # Distributed training (should be after apex fp16 initialization)\\n    if args.local_rank != -1:\\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\\n                                                          output_device=args.local_rank,\\n                                                          find_unused_parameters=True)\\n\\n    checkpoint_last = os.path.join(args.output_dir, \\'checkpoint-last\\')\\n    scheduler_last = os.path.join(checkpoint_last, \\'scheduler.pt\\')\\n    optimizer_last = os.path.join(checkpoint_last, \\'optimizer.pt\\')\\n    if os.path.exists(scheduler_last):\\n        scheduler.load_state_dict(torch.load(scheduler_last))\\n    if os.path.exists(optimizer_last):\\n        optimizer.load_state_dict(torch.load(optimizer_last))\\n    # Train!\\n    print(\"***** Running training *****\")\\n    print(\"  Num examples = %d\", len(train_dataset))\\n    print(\"  Num Epochs = %d\", args.num_train_epochs)\\n    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\\n    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\\n                args.train_batch_size * args.gradient_accumulation_steps * (\\n                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\\n    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\\n    print(\"  Total optimization steps = %d\", args.max_steps)\\n    \\n    global_step = args.start_step\\n    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\\n    best_mrr=0.0\\n    best_acc=0.0\\n    # model.resize_token_embeddings(len(tokenizer))\\n    model.zero_grad()\\n\\n    # Initialize early stopping parameters at the start of training\\n    early_stopping_counter = 0\\n    best_loss = None\\n \\n    for idx in range(args.start_epoch, int(args.num_train_epochs)): \\n        bar = tqdm(train_dataloader,total=len(train_dataloader))\\n        tr_num=0\\n        train_loss=0\\n        acc = 0\\n        for step, batch in enumerate(bar):\\n            inputs = batch[0].to(args.device)        \\n            labels=batch[1].to(args.device) \\n            model.train()\\n            loss,logits = model(inputs,labels)\\n\\n\\n            if args.n_gpu > 1:\\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\\n            if args.gradient_accumulation_steps > 1:\\n                loss = loss / args.gradient_accumulation_steps\\n\\n            if args.fp16:\\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\\n                    scaled_loss.backward()\\n                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\\n            else:\\n                loss.backward()\\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\\n\\n            logits = logits.detach().cpu().numpy()\\n            labels = labels.detach().cpu().numpy()\\n            preds=logits[:,0]>0.5\\n            acc+=np.mean(labels==preds)\\n            \\n            tr_loss += loss.item()\\n            tr_num+=1\\n            train_loss+=loss.item()\\n            if avg_loss==0:\\n                avg_loss=tr_loss\\n            avg_loss=round(train_loss/tr_num,5)\\n            avg_acc = round(acc/tr_num,5)\\n            bar.set_description(\"epoch {} loss {} acc {}\".format(idx,avg_loss,avg_acc))\\n\\n                \\n            if (step + 1) % args.gradient_accumulation_steps == 0:\\n                optimizer.step()\\n                optimizer.zero_grad()\\n                scheduler.step()  \\n                global_step += 1\\n                output_flag=True\\n                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\\n                    logging_loss = tr_loss\\n                    tr_nb=global_step\\n\\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\\n                    \\n                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\\n                        results = evaluate(args, model, tokenizer,eval_when_training=True)\\n                        for key, value in results.items():\\n                            print(\"  %s = %s\", key, round(value,4))                    \\n                        # Save model checkpoint\\n                        \\n                    if results[\\'eval_acc\\']>best_acc:\\n                        best_acc=results[\\'eval_acc\\']\\n                        print(\"  \"+\"*\"*20)  \\n                        print(\"  Best acc:%s\",round(best_acc,4))\\n                        print(\"  \"+\"*\"*20)                          \\n                        \\n                        checkpoint_prefix = \\'checkpoint-best-acc\\'\\n                        output_dir = os.path.join(args.output_dir, \\'{}\\'.format(checkpoint_prefix))                        \\n                        if not os.path.exists(output_dir):\\n                            os.makedirs(output_dir)                        \\n                        model_to_save = model.module if hasattr(model,\\'module\\') else model\\n                        output_dir = os.path.join(output_dir, \\'{}\\'.format(\\'model.bin\\')) \\n                        torch.save(model_to_save.state_dict(), output_dir)\\n                        print(\"Saving model checkpoint to %s\", output_dir)\\n\\n        # Calculate average loss for the epoch\\n        avg_loss = train_loss / tr_num\\n\\n        # Check for early stopping condition\\n        if args.early_stopping_patience is not None:\\n            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\\n                best_loss = avg_loss\\n                early_stopping_counter = 0\\n            else:\\n                early_stopping_counter += 1\\n                if early_stopping_counter >= args.early_stopping_patience:\\n                    print(\"Early stopping\")\\n                    break  # Exit the loop early\\n                        \\n\\n\\n\\ndef evaluate(args, model, tokenizer,eval_when_training=False):\\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\\n    eval_output_dir = args.output_dir\\n\\n    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\\n\\n    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\\n        os.makedirs(eval_output_dir)\\n\\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\\n    # Note that DistributedSampler samples randomly\\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\\n\\n    # multi-gpu evaluate\\n    if args.n_gpu > 1 and eval_when_training is False:\\n        model = torch.nn.DataParallel(model)\\n\\n    # Eval!\\n    print(\"***** Running evaluation *****\")\\n    print(\"  Num examples = %d\", len(eval_dataset))\\n    print(\"  Batch size = %d\", args.eval_batch_size)\\n    eval_loss = 0.0\\n    nb_eval_steps = 0\\n    model.eval()\\n    logits=[] \\n    labels=[]\\n    for batch in eval_dataloader:\\n        inputs = batch[0].to(args.device)        \\n        label=batch[1].to(args.device) \\n        with torch.no_grad():\\n            lm_loss,logit = model(inputs,label)\\n            eval_loss += lm_loss.mean().item()\\n            logits.append(logit.cpu().numpy())\\n            labels.append(label.cpu().numpy())\\n        nb_eval_steps += 1\\n    logits=np.concatenate(logits,0)\\n    labels=np.concatenate(labels,0)\\n    preds=logits[:,0]>0.5\\n    eval_acc=np.mean(labels==preds)\\n    eval_loss = eval_loss / nb_eval_steps\\n    perplexity = torch.tensor(eval_loss)\\n    \\n    f1 = f1_score(y_true=labels, y_pred=preds)\\n    cm = confusion_matrix(y_true=labels, y_pred=preds)\\n    tp = cm[1][1]\\n    tn = cm[0][0]\\n    fp = cm[0][1]\\n    fn = cm[1][0]\\n    precision = 0 if tp + fp == 0 else tp/(tp+fp)\\n    recall = 0 if tp + fn == 0 else tp/(tp+fn)\\n    fprate = 0 if fp + tn == 0 else fp/(fp+tn)\\n    pdVal = 0 if tp + fn == 0 else tp/(tp+fn)\\n    pfVal = 0 if fp + tn == 0 else fp/(fp+tn)\\n    G = 0 if pdVal+1-pfVal == 0 else 2*pdVal*(1-pfVal)/(pdVal+1-pfVal)\\n    F1 = 0 if precision+recall == 0 else 2 * precision * recall / (precision+recall)\\n    MCC = 0 if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) == 0 else (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\\n    result = {\\n        \"eval_loss\": float(perplexity),\\n        \"eval_acc\":round(eval_acc,4),\\n        \"f1\": f1,\\n        \"precision\": precision,\\n        \"recall\": recall,\\n        \"G\": G,\\n        \"MCC\": MCC\\n    }\\n    return result\\n\\ndef test(args, model, tokenizer):\\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\\n    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\\n\\n\\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\\n    # Note that DistributedSampler samples randomly\\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\\n\\n    # multi-gpu evaluate\\n    if args.n_gpu > 1:\\n        model = torch.nn.DataParallel(model)\\n\\n    # Eval!\\n    print(\"***** Running Test *****\")\\n    print(\"  Num examples = %d\", len(eval_dataset))\\n    print(\"  Batch size = %d\", args.eval_batch_size)\\n    eval_loss = 0.0\\n    nb_eval_steps = 0\\n    model.eval()\\n    logits=[]   \\n    labels=[]\\n    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\\n        inputs = batch[0].to(args.device)        \\n        label=batch[1].to(args.device) \\n        with torch.no_grad():\\n            logit = model(inputs)\\n            logits.append(logit.cpu().numpy())\\n            labels.append(label.cpu().numpy())\\n\\n    logits=np.concatenate(logits,0)\\n    labels=np.concatenate(labels,0)\\n    preds=logits[:,0]>0.5\\n    with open(os.path.join(args.output_dir,\"predictions.txt\"),\\'w\\') as f:\\n        for example,pred in zip(eval_dataset.examples,preds):\\n            if pred:\\n                f.write(example.idx+\\'\\\\t1\\\\n\\')\\n            else:\\n                f.write(example.idx+\\'\\\\t0\\\\n\\')    \\n    \\n                        \\n                        \\ndef main(args):\\n\\n    # Setup distant debugging if needed\\n    if args.server_ip and args.server_port:\\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\\n        import ptvsd\\n        print(\"Waiting for debugger attach\")\\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\\n        ptvsd.wait_for_attach()\\n\\n    # Setup CUDA, GPU & distributed training\\n    if args.local_rank == -1 or args.no_cuda:\\n        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\\n        args.n_gpu = torch.cuda.device_count()\\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\\n        torch.cuda.set_device(args.local_rank)\\n        device = torch.device(\"cuda\", args.local_rank)\\n        torch.distributed.init_process_group(backend=\\'nccl\\')\\n        args.n_gpu = 1\\n    args.device = device\\n    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu\\n    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu\\n    # Setup logging\\n    logging.basicConfig(format=\\'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\\',\\n                        datefmt=\\'%m/%d/%Y %H:%M:%S\\',\\n                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\\n    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\\n                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\\n\\n\\n\\n    # Set seed\\n    set_seed(args.seed)\\n\\n    # Load pretrained model and tokenizer\\n    if args.local_rank not in [-1, 0]:\\n        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\\n\\n    args.start_epoch = 0\\n    args.start_step = 0\\n    checkpoint_last = os.path.join(args.output_dir, \\'checkpoint-last\\')\\n    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\\n        args.model_name_or_path = os.path.join(checkpoint_last, \\'pytorch_model.bin\\')\\n        args.config_name = os.path.join(checkpoint_last, \\'config.json\\')\\n        idx_file = os.path.join(checkpoint_last, \\'idx_file.txt\\')\\n        with open(idx_file, encoding=\\'utf-8\\') as idxf:\\n            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\\n\\n        step_file = os.path.join(checkpoint_last, \\'step_file.txt\\')\\n        if os.path.exists(step_file):\\n            with open(step_file, encoding=\\'utf-8\\') as stepf:\\n                args.start_step = int(stepf.readlines()[0].strip())\\n\\n        print(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))\\n\\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\\n                                          cache_dir=args.cache_dir if args.cache_dir else None)\\n    config.num_labels=1\\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\\n                                                do_lower_case=args.do_lower_case,\\n                                                cache_dir=args.cache_dir if args.cache_dir else None)\\n    if args.block_size <= 0:\\n        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\\n    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\\n    if args.model_name_or_path:\\n        model = model_class.from_pretrained(args.model_name_or_path,\\n                                            from_tf=bool(\\'.ckpt\\' in args.model_name_or_path),\\n                                            config=config,\\n                                            cache_dir=args.cache_dir if args.cache_dir else None)    \\n    else:\\n        model = model_class(config)\\n\\n    model=Model(model,config,tokenizer,args)\\n    if args.local_rank == 0:\\n        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\\n\\n    print(\"Training/evaluation parameters %s\", args)\\n\\n    # Training\\n    if args.do_train:\\n        if args.local_rank not in [-1, 0]:\\n            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\\n\\n        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\\n        if args.local_rank == 0:\\n            torch.distributed.barrier()\\n\\n        train(args, train_dataset, model, tokenizer)\\n\\n\\n\\n    # Evaluation\\n    results = {}\\n    if args.do_eval and args.local_rank in [-1, 0]:\\n            checkpoint_prefix = \\'checkpoint-best-acc/model.bin\\'\\n            output_dir = os.path.join(args.output_dir, \\'{}\\'.format(checkpoint_prefix))  \\n            model.load_state_dict(torch.load(output_dir))      \\n            model.to(args.device)\\n            result=evaluate(args, model, tokenizer)\\n            print(\"***** Eval results *****\")\\n            for key in sorted(result.keys()):\\n                print(\"  %s = %s\", key, str(round(result[key],4)))\\n            \\n    if args.do_test and args.local_rank in [-1, 0]:\\n            checkpoint_prefix = \\'checkpoint-best-acc/model.bin\\'\\n            output_dir = os.path.join(args.output_dir, \\'{}\\'.format(checkpoint_prefix))  \\n            model.load_state_dict(torch.load(output_dir))                  \\n            model.to(args.device)\\n            test(args, model, tokenizer)\\n\\n    return results\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser()\\n\\n    ## Required parameters\\n    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\\n                        help=\"The input training data file (a text file).\")\\n    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\\n\\n    ## Other parameters\\n    parser.add_argument(\"--eval_data_file\", default=None, type=str,\\n                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\\n    parser.add_argument(\"--test_data_file\", default=None, type=str,\\n                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\\n                    \\n    parser.add_argument(\"--model_type\", default=\"bert\", type=str,\\n                        help=\"The model architecture to be fine-tuned.\")\\n    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\\n                        help=\"The model checkpoint for weights initialization.\")\\n\\n    parser.add_argument(\"--mlm\", action=\\'store_true\\',\\n                        help=\"Train with masked-language modeling loss instead of language modeling.\")\\n    parser.add_argument(\"--mlm_probability\", type=float, default=0.15,\\n                        help=\"Ratio of tokens to mask for masked language modeling loss\")\\n\\n    parser.add_argument(\"--config_name\", default=\"\", type=str,\\n                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\\n    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\\n                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\\n    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\\n                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\\n    parser.add_argument(\"--block_size\", default=-1, type=int,\\n                        help=\"Optional input sequence length after tokenization.\"\\n                             \"The training dataset will be truncated in block of this size for training.\"\\n                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\\n    parser.add_argument(\"--do_train\", action=\\'store_true\\',\\n                        help=\"Whether to run training.\")\\n    parser.add_argument(\"--do_eval\", action=\\'store_true\\',\\n                        help=\"Whether to run eval on the dev set.\")\\n    parser.add_argument(\"--do_test\", action=\\'store_true\\',\\n                        help=\"Whether to run eval on the dev set.\")    \\n    parser.add_argument(\"--evaluate_during_training\", action=\\'store_true\\',\\n                        help=\"Run evaluation during training at each logging step.\")\\n    parser.add_argument(\"--do_lower_case\", action=\\'store_true\\',\\n                        help=\"Set this flag if you are using an uncased model.\")\\n\\n    parser.add_argument(\"--train_batch_size\", default=4, type=int,\\n                        help=\"Batch size per GPU/CPU for training.\")\\n    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\\n                        help=\"Batch size per GPU/CPU for evaluation.\")\\n    parser.add_argument(\\'--gradient_accumulation_steps\\', type=int, default=1,\\n                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\\n    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\\n                        help=\"The initial learning rate for Adam.\")\\n    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\\n                        help=\"Weight deay if we apply some.\")\\n    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\\n                        help=\"Epsilon for Adam optimizer.\")\\n    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\\n                        help=\"Max gradient norm.\")\\n    parser.add_argument(\"--num_train_epochs\", default=1.0, type=float,\\n                        help=\"Total number of training epochs to perform.\")\\n    parser.add_argument(\"--max_steps\", default=-1, type=int,\\n                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\\n    parser.add_argument(\"--warmup_steps\", default=0, type=int,\\n                        help=\"Linear warmup over warmup_steps.\")\\n\\n    parser.add_argument(\\'--logging_steps\\', type=int, default=50,\\n                        help=\"Log every X updates steps.\")\\n    parser.add_argument(\\'--save_steps\\', type=int, default=50,\\n                        help=\"Save checkpoint every X updates steps.\")\\n    parser.add_argument(\\'--save_total_limit\\', type=int, default=None,\\n                        help=\\'Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default\\')\\n    parser.add_argument(\"--eval_all_checkpoints\", action=\\'store_true\\',\\n                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\\n    parser.add_argument(\"--no_cuda\", action=\\'store_true\\',\\n                        help=\"Avoid using CUDA when available\")\\n    parser.add_argument(\\'--overwrite_output_dir\\', action=\\'store_true\\',\\n                        help=\"Overwrite the content of the output directory\")\\n    parser.add_argument(\\'--overwrite_cache\\', action=\\'store_true\\',\\n                        help=\"Overwrite the cached training and evaluation sets\")\\n    parser.add_argument(\\'--seed\\', type=int, default=42,\\n                        help=\"random seed for initialization\")\\n    parser.add_argument(\\'--epoch\\', type=int, default=42,\\n                        help=\"random seed for initialization\")\\n    parser.add_argument(\\'--fp16\\', action=\\'store_true\\',\\n                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\\n    parser.add_argument(\\'--fp16_opt_level\\', type=str, default=\\'O1\\',\\n                        help=\"For fp16: Apex AMP optimization level selected in [\\'O0\\', \\'O1\\', \\'O2\\', and \\'O3\\'].\"\\n                             \"See details at https://nvidia.github.io/apex/amp.html\")\\n    parser.add_argument(\"--local_rank\", type=int, default=-1,\\n                        help=\"For distributed training: local_rank\")\\n    parser.add_argument(\\'--server_ip\\', type=str, default=\\'\\', help=\"For distant debugging.\")\\n    parser.add_argument(\\'--server_port\\', type=str, default=\\'\\', help=\"For distant debugging.\")\\n\\n    # Add early stopping parameters and dropout probability parameters\\n    parser.add_argument(\"--early_stopping_patience\", type=int, default=None,\\n                        help=\"Number of epochs with no improvement after which training will be stopped.\")\\n    parser.add_argument(\"--min_loss_delta\", type=float, default=0.001,\\n                        help=\"Minimum change in the loss required to qualify as an improvement.\")\\n    parser.add_argument(\\'--dropout_probability\\', type=float, default=0, help=\\'dropout probability\\')\\n\\n    # Simulate command-line arguments\\n    simulated_args = [\\n        \\'--output_dir\\', \\'./saved_models\\',\\n        \\'--model_type\\', \\'roberta\\',\\n        \\'--tokenizer_name\\', \\'microsoft/codebert-base\\',\\n        \\'--model_name_or_path\\', \\'microsoft/codebert-base\\',\\n        \\'--do_train\\',\\n        \\'--train_data_file\\', \\'/kaggle/working/dataset/train.jsonl\\',\\n        \\'--eval_data_file\\', \\'/kaggle/working/dataset/test.jsonl\\',\\n        \\'--test_data_file\\', \\'/kaggle/working/dataset/test.jsonl\\',\\n        \\'--epoch\\', \\'10\\',\\n        \\'--block_size\\', \\'512\\',\\n        \\'--train_batch_size\\', \\'16\\',\\n        \\'--eval_batch_size\\', \\'16\\',\\n        \\'--learning_rate\\', \\'2e-5\\',\\n        \\'--max_grad_norm\\', \\'1.0\\',\\n        \\'--evaluate_during_training\\',\\n        \\'--seed\\', \\'123456\\'\\n    ]\\n\\n    # Parse the simulated arguments\\n    args = parser.parse_args(simulated_args)\\n\\n    main(args)\\n'' returned non-zero exit status 1."
          ],
          "ename": "CalledProcessError",
          "evalue": "Command 'b'from __future__ import absolute_import, division, print_function\\n\\nimport argparse\\nimport glob\\nimport logging\\nimport os\\nimport pickle\\nimport random\\nimport re\\nimport shutil\\nimport math\\nfrom sklearn.metrics import f1_score, confusion_matrix\\n\\nimport numpy as np\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\\nfrom torch.utils.data.distributed import DistributedSampler\\nimport torch.nn as nn\\nfrom torch.autograd import Variable\\nimport copy\\nfrom torch.nn import CrossEntropyLoss, MSELoss\\nimport json\\ntry:\\n    from torch.utils.tensorboard import SummaryWriter\\nexcept:\\n    from tensorboardX import SummaryWriter\\n\\nfrom tqdm import tqdm, trange\\nimport multiprocessing\\ncpu_cont = multiprocessing.cpu_count()\\nfrom transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\\n                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\\n\\nlogger = logging.getLogger(__name__)\\nMODEL_CLASSES = {\\n    \\'roberta\\': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\\n}\\nclass Model(nn.Module):   \\n    def __init__(self, encoder,config,tokenizer,args):\\n        super(Model, self).__init__()\\n        self.encoder = encoder\\n        self.config=config\\n        self.tokenizer=tokenizer\\n        self.args=args\\n    \\n        # Define dropout layer, dropout_probability is taken from args.\\n        self.dropout = nn.Dropout(args.dropout_probability)\\n\\n        \\n    def forward(self, input_ids=None,labels=None): \\n        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\\n\\n        # Apply dropout\\n        outputs = self.dropout(outputs)\\n\\n        logits=outputs\\n        prob=torch.sigmoid(logits)\\n        if labels is not None:\\n            labels=labels.float()\\n            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\\n            loss=-loss.mean()\\n            return loss,prob\\n        else:\\n            return prob\\nclass InputFeatures(object):\\n    \"\"\"A single training/test features for a example.\"\"\"\\n    def __init__(self,\\n                 input_tokens,\\n                 input_ids,\\n                 idx,\\n                 label,\\n\\n    ):\\n        self.input_tokens = input_tokens\\n        self.input_ids = input_ids\\n        self.idx=str(idx)\\n        self.label=label\\ndef convert_examples_to_features(js,tokenizer,args):\\n    #source\\n    code=\\' \\'.join(js[\\'func\\'].split())\\n    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\\n    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\\n    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\\n    padding_length = args.block_size - len(source_ids)\\n    source_ids+=[tokenizer.pad_token_id]*padding_length\\n    return InputFeatures(source_tokens,source_ids,js[\\'idx\\'],js[\\'target\\'])\\nclass TextDataset(Dataset):\\n    def __init__(self, tokenizer, args, file_path=None):\\n        self.examples = []\\n        with open(file_path) as f:\\n            for line in f:\\n                js=json.loads(line.strip())\\n                self.examples.append(convert_examples_to_features(js,tokenizer,args))\\n        if \\'train\\' in file_path:\\n            for idx, example in enumerate(self.examples[:3]):\\n                    logger.info(\"*** Example ***\")\\n                    logger.info(\"idx: {}\".format(idx))\\n                    logger.info(\"label: {}\".format(example.label))\\n                    logger.info(\"input_tokens: {}\".format([x.replace(\\'\\\\u0120\\',\\'_\\') for x in example.input_tokens]))\\n                    logger.info(\"input_ids: {}\".format(\\' \\'.join(map(str, example.input_ids))))\\n\\n    def __len__(self):\\n        return len(self.examples)\\n\\n    def __getitem__(self, i):       \\n        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\\ndef set_seed(seed=42):\\n    random.seed(seed)\\n    os.environ[\\'PYHTONHASHSEED\\'] = str(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    torch.backends.cudnn.deterministic = True\\ndef train(args, train_dataset, model, tokenizer):\\n    \"\"\" Train the model \"\"\" \\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\\n    \\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, \\n                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\\n    args.max_steps=args.epoch*len( train_dataloader)\\n    args.save_steps=len( train_dataloader)\\n    args.warmup_steps=len( train_dataloader)\\n    args.logging_steps=len( train_dataloader)\\n    args.num_train_epochs=args.epoch\\n    model.to(args.device)\\n    # Prepare optimizer and schedule (linear warmup and decay)\\n    no_decay = [\\'bias\\', \\'LayerNorm.weight\\']\\n    optimizer_grouped_parameters = [\\n        {\\'params\\': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\\n         \\'weight_decay\\': args.weight_decay},\\n        {\\'params\\': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \\'weight_decay\\': 0.0}\\n    ]\\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\\n                                                num_training_steps=args.max_steps)\\n    if args.fp16:\\n        try:\\n            from apex import amp\\n        except ImportError:\\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\\n\\n    # multi-gpu training (should be after apex fp16 initialization)\\n    if args.n_gpu > 1:\\n        model = torch.nn.DataParallel(model)\\n\\n    # Distributed training (should be after apex fp16 initialization)\\n    if args.local_rank != -1:\\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\\n                                                          output_device=args.local_rank,\\n                                                          find_unused_parameters=True)\\n\\n    checkpoint_last = os.path.join(args.output_dir, \\'checkpoint-last\\')\\n    scheduler_last = os.path.join(checkpoint_last, \\'scheduler.pt\\')\\n    optimizer_last = os.path.join(checkpoint_last, \\'optimizer.pt\\')\\n    if os.path.exists(scheduler_last):\\n        scheduler.load_state_dict(torch.load(scheduler_last))\\n    if os.path.exists(optimizer_last):\\n        optimizer.load_state_dict(torch.load(optimizer_last))\\n    # Train!\\n    print(\"***** Running training *****\")\\n    print(\"  Num examples = %d\", len(train_dataset))\\n    print(\"  Num Epochs = %d\", args.num_train_epochs)\\n    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\\n    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\\n                args.train_batch_size * args.gradient_accumulation_steps * (\\n                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\\n    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\\n    print(\"  Total optimization steps = %d\", args.max_steps)\\n    \\n    global_step = args.start_step\\n    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\\n    best_mrr=0.0\\n    best_acc=0.0\\n    # model.resize_token_embeddings(len(tokenizer))\\n    model.zero_grad()\\n\\n    # Initialize early stopping parameters at the start of training\\n    early_stopping_counter = 0\\n    best_loss = None\\n \\n    for idx in range(args.start_epoch, int(args.num_train_epochs)): \\n        bar = tqdm(train_dataloader,total=len(train_dataloader))\\n        tr_num=0\\n        train_loss=0\\n        acc = 0\\n        for step, batch in enumerate(bar):\\n            inputs = batch[0].to(args.device)        \\n            labels=batch[1].to(args.device) \\n            model.train()\\n            loss,logits = model(inputs,labels)\\n\\n\\n            if args.n_gpu > 1:\\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\\n            if args.gradient_accumulation_steps > 1:\\n                loss = loss / args.gradient_accumulation_steps\\n\\n            if args.fp16:\\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\\n                    scaled_loss.backward()\\n                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\\n            else:\\n                loss.backward()\\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\\n\\n            logits = logits.detach().cpu().numpy()\\n            labels = labels.detach().cpu().numpy()\\n            preds=logits[:,0]>0.5\\n            acc+=np.mean(labels==preds)\\n            \\n            tr_loss += loss.item()\\n            tr_num+=1\\n            train_loss+=loss.item()\\n            if avg_loss==0:\\n                avg_loss=tr_loss\\n            avg_loss=round(train_loss/tr_num,5)\\n            avg_acc = round(acc/tr_num,5)\\n            bar.set_description(\"epoch {} loss {} acc {}\".format(idx,avg_loss,avg_acc))\\n\\n                \\n            if (step + 1) % args.gradient_accumulation_steps == 0:\\n                optimizer.step()\\n                optimizer.zero_grad()\\n                scheduler.step()  \\n                global_step += 1\\n                output_flag=True\\n                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\\n                    logging_loss = tr_loss\\n                    tr_nb=global_step\\n\\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\\n                    \\n                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\\n                        results = evaluate(args, model, tokenizer,eval_when_training=True)\\n                        for key, value in results.items():\\n                            print(\"  %s = %s\", key, round(value,4))                    \\n                        # Save model checkpoint\\n                        \\n                    if results[\\'eval_acc\\']>best_acc:\\n                        best_acc=results[\\'eval_acc\\']\\n                        print(\"  \"+\"*\"*20)  \\n                        print(\"  Best acc:%s\",round(best_acc,4))\\n                        print(\"  \"+\"*\"*20)                          \\n                        \\n                        checkpoint_prefix = \\'checkpoint-best-acc\\'\\n                        output_dir = os.path.join(args.output_dir, \\'{}\\'.format(checkpoint_prefix))                        \\n                        if not os.path.exists(output_dir):\\n                            os.makedirs(output_dir)                        \\n                        model_to_save = model.module if hasattr(model,\\'module\\') else model\\n                        output_dir = os.path.join(output_dir, \\'{}\\'.format(\\'model.bin\\')) \\n                        torch.save(model_to_save.state_dict(), output_dir)\\n                        print(\"Saving model checkpoint to %s\", output_dir)\\n\\n        # Calculate average loss for the epoch\\n        avg_loss = train_loss / tr_num\\n\\n        # Check for early stopping condition\\n        if args.early_stopping_patience is not None:\\n            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\\n                best_loss = avg_loss\\n                early_stopping_counter = 0\\n            else:\\n                early_stopping_counter += 1\\n                if early_stopping_counter >= args.early_stopping_patience:\\n                    print(\"Early stopping\")\\n                    break  # Exit the loop early\\n                        \\n\\n\\n\\ndef evaluate(args, model, tokenizer,eval_when_training=False):\\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\\n    eval_output_dir = args.output_dir\\n\\n    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\\n\\n    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\\n        os.makedirs(eval_output_dir)\\n\\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\\n    # Note that DistributedSampler samples randomly\\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\\n\\n    # multi-gpu evaluate\\n    if args.n_gpu > 1 and eval_when_training is False:\\n        model = torch.nn.DataParallel(model)\\n\\n    # Eval!\\n    print(\"***** Running evaluation *****\")\\n    print(\"  Num examples = %d\", len(eval_dataset))\\n    print(\"  Batch size = %d\", args.eval_batch_size)\\n    eval_loss = 0.0\\n    nb_eval_steps = 0\\n    model.eval()\\n    logits=[] \\n    labels=[]\\n    for batch in eval_dataloader:\\n        inputs = batch[0].to(args.device)        \\n        label=batch[1].to(args.device) \\n        with torch.no_grad():\\n            lm_loss,logit = model(inputs,label)\\n            eval_loss += lm_loss.mean().item()\\n            logits.append(logit.cpu().numpy())\\n            labels.append(label.cpu().numpy())\\n        nb_eval_steps += 1\\n    logits=np.concatenate(logits,0)\\n    labels=np.concatenate(labels,0)\\n    preds=logits[:,0]>0.5\\n    eval_acc=np.mean(labels==preds)\\n    eval_loss = eval_loss / nb_eval_steps\\n    perplexity = torch.tensor(eval_loss)\\n    \\n    f1 = f1_score(y_true=labels, y_pred=preds)\\n    cm = confusion_matrix(y_true=labels, y_pred=preds)\\n    tp = cm[1][1]\\n    tn = cm[0][0]\\n    fp = cm[0][1]\\n    fn = cm[1][0]\\n    precision = 0 if tp + fp == 0 else tp/(tp+fp)\\n    recall = 0 if tp + fn == 0 else tp/(tp+fn)\\n    fprate = 0 if fp + tn == 0 else fp/(fp+tn)\\n    pdVal = 0 if tp + fn == 0 else tp/(tp+fn)\\n    pfVal = 0 if fp + tn == 0 else fp/(fp+tn)\\n    G = 0 if pdVal+1-pfVal == 0 else 2*pdVal*(1-pfVal)/(pdVal+1-pfVal)\\n    F1 = 0 if precision+recall == 0 else 2 * precision * recall / (precision+recall)\\n    MCC = 0 if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) == 0 else (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\\n    result = {\\n        \"eval_loss\": float(perplexity),\\n        \"eval_acc\":round(eval_acc,4),\\n        \"f1\": f1,\\n        \"precision\": precision,\\n        \"recall\": recall,\\n        \"G\": G,\\n        \"MCC\": MCC\\n    }\\n    return result\\n\\ndef test(args, model, tokenizer):\\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\\n    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\\n\\n\\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\\n    # Note that DistributedSampler samples randomly\\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\\n\\n    # multi-gpu evaluate\\n    if args.n_gpu > 1:\\n        model = torch.nn.DataParallel(model)\\n\\n    # Eval!\\n    print(\"***** Running Test *****\")\\n    print(\"  Num examples = %d\", len(eval_dataset))\\n    print(\"  Batch size = %d\", args.eval_batch_size)\\n    eval_loss = 0.0\\n    nb_eval_steps = 0\\n    model.eval()\\n    logits=[]   \\n    labels=[]\\n    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\\n        inputs = batch[0].to(args.device)        \\n        label=batch[1].to(args.device) \\n        with torch.no_grad():\\n            logit = model(inputs)\\n            logits.append(logit.cpu().numpy())\\n            labels.append(label.cpu().numpy())\\n\\n    logits=np.concatenate(logits,0)\\n    labels=np.concatenate(labels,0)\\n    preds=logits[:,0]>0.5\\n    with open(os.path.join(args.output_dir,\"predictions.txt\"),\\'w\\') as f:\\n        for example,pred in zip(eval_dataset.examples,preds):\\n            if pred:\\n                f.write(example.idx+\\'\\\\t1\\\\n\\')\\n            else:\\n                f.write(example.idx+\\'\\\\t0\\\\n\\')    \\n    \\n                        \\n                        \\ndef main(args):\\n\\n    # Setup distant debugging if needed\\n    if args.server_ip and args.server_port:\\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\\n        import ptvsd\\n        print(\"Waiting for debugger attach\")\\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\\n        ptvsd.wait_for_attach()\\n\\n    # Setup CUDA, GPU & distributed training\\n    if args.local_rank == -1 or args.no_cuda:\\n        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\\n        args.n_gpu = torch.cuda.device_count()\\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\\n        torch.cuda.set_device(args.local_rank)\\n        device = torch.device(\"cuda\", args.local_rank)\\n        torch.distributed.init_process_group(backend=\\'nccl\\')\\n        args.n_gpu = 1\\n    args.device = device\\n    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu\\n    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu\\n    # Setup logging\\n    logging.basicConfig(format=\\'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\\',\\n                        datefmt=\\'%m/%d/%Y %H:%M:%S\\',\\n                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\\n    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\\n                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\\n\\n\\n\\n    # Set seed\\n    set_seed(args.seed)\\n\\n    # Load pretrained model and tokenizer\\n    if args.local_rank not in [-1, 0]:\\n        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\\n\\n    args.start_epoch = 0\\n    args.start_step = 0\\n    checkpoint_last = os.path.join(args.output_dir, \\'checkpoint-last\\')\\n    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\\n        args.model_name_or_path = os.path.join(checkpoint_last, \\'pytorch_model.bin\\')\\n        args.config_name = os.path.join(checkpoint_last, \\'config.json\\')\\n        idx_file = os.path.join(checkpoint_last, \\'idx_file.txt\\')\\n        with open(idx_file, encoding=\\'utf-8\\') as idxf:\\n            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\\n\\n        step_file = os.path.join(checkpoint_last, \\'step_file.txt\\')\\n        if os.path.exists(step_file):\\n            with open(step_file, encoding=\\'utf-8\\') as stepf:\\n                args.start_step = int(stepf.readlines()[0].strip())\\n\\n        print(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))\\n\\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\\n                                          cache_dir=args.cache_dir if args.cache_dir else None)\\n    config.num_labels=1\\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\\n                                                do_lower_case=args.do_lower_case,\\n                                                cache_dir=args.cache_dir if args.cache_dir else None)\\n    if args.block_size <= 0:\\n        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\\n    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\\n    if args.model_name_or_path:\\n        model = model_class.from_pretrained(args.model_name_or_path,\\n                                            from_tf=bool(\\'.ckpt\\' in args.model_name_or_path),\\n                                            config=config,\\n                                            cache_dir=args.cache_dir if args.cache_dir else None)    \\n    else:\\n        model = model_class(config)\\n\\n    model=Model(model,config,tokenizer,args)\\n    if args.local_rank == 0:\\n        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\\n\\n    print(\"Training/evaluation parameters %s\", args)\\n\\n    # Training\\n    if args.do_train:\\n        if args.local_rank not in [-1, 0]:\\n            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\\n\\n        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\\n        if args.local_rank == 0:\\n            torch.distributed.barrier()\\n\\n        train(args, train_dataset, model, tokenizer)\\n\\n\\n\\n    # Evaluation\\n    results = {}\\n    if args.do_eval and args.local_rank in [-1, 0]:\\n            checkpoint_prefix = \\'checkpoint-best-acc/model.bin\\'\\n            output_dir = os.path.join(args.output_dir, \\'{}\\'.format(checkpoint_prefix))  \\n            model.load_state_dict(torch.load(output_dir))      \\n            model.to(args.device)\\n            result=evaluate(args, model, tokenizer)\\n            print(\"***** Eval results *****\")\\n            for key in sorted(result.keys()):\\n                print(\"  %s = %s\", key, str(round(result[key],4)))\\n            \\n    if args.do_test and args.local_rank in [-1, 0]:\\n            checkpoint_prefix = \\'checkpoint-best-acc/model.bin\\'\\n            output_dir = os.path.join(args.output_dir, \\'{}\\'.format(checkpoint_prefix))  \\n            model.load_state_dict(torch.load(output_dir))                  \\n            model.to(args.device)\\n            test(args, model, tokenizer)\\n\\n    return results\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser()\\n\\n    ## Required parameters\\n    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\\n                        help=\"The input training data file (a text file).\")\\n    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\\n\\n    ## Other parameters\\n    parser.add_argument(\"--eval_data_file\", default=None, type=str,\\n                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\\n    parser.add_argument(\"--test_data_file\", default=None, type=str,\\n                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\\n                    \\n    parser.add_argument(\"--model_type\", default=\"bert\", type=str,\\n                        help=\"The model architecture to be fine-tuned.\")\\n    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\\n                        help=\"The model checkpoint for weights initialization.\")\\n\\n    parser.add_argument(\"--mlm\", action=\\'store_true\\',\\n                        help=\"Train with masked-language modeling loss instead of language modeling.\")\\n    parser.add_argument(\"--mlm_probability\", type=float, default=0.15,\\n                        help=\"Ratio of tokens to mask for masked language modeling loss\")\\n\\n    parser.add_argument(\"--config_name\", default=\"\", type=str,\\n                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\\n    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\\n                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\\n    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\\n                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\\n    parser.add_argument(\"--block_size\", default=-1, type=int,\\n                        help=\"Optional input sequence length after tokenization.\"\\n                             \"The training dataset will be truncated in block of this size for training.\"\\n                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\\n    parser.add_argument(\"--do_train\", action=\\'store_true\\',\\n                        help=\"Whether to run training.\")\\n    parser.add_argument(\"--do_eval\", action=\\'store_true\\',\\n                        help=\"Whether to run eval on the dev set.\")\\n    parser.add_argument(\"--do_test\", action=\\'store_true\\',\\n                        help=\"Whether to run eval on the dev set.\")    \\n    parser.add_argument(\"--evaluate_during_training\", action=\\'store_true\\',\\n                        help=\"Run evaluation during training at each logging step.\")\\n    parser.add_argument(\"--do_lower_case\", action=\\'store_true\\',\\n                        help=\"Set this flag if you are using an uncased model.\")\\n\\n    parser.add_argument(\"--train_batch_size\", default=4, type=int,\\n                        help=\"Batch size per GPU/CPU for training.\")\\n    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\\n                        help=\"Batch size per GPU/CPU for evaluation.\")\\n    parser.add_argument(\\'--gradient_accumulation_steps\\', type=int, default=1,\\n                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\\n    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\\n                        help=\"The initial learning rate for Adam.\")\\n    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\\n                        help=\"Weight deay if we apply some.\")\\n    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\\n                        help=\"Epsilon for Adam optimizer.\")\\n    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\\n                        help=\"Max gradient norm.\")\\n    parser.add_argument(\"--num_train_epochs\", default=1.0, type=float,\\n                        help=\"Total number of training epochs to perform.\")\\n    parser.add_argument(\"--max_steps\", default=-1, type=int,\\n                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\\n    parser.add_argument(\"--warmup_steps\", default=0, type=int,\\n                        help=\"Linear warmup over warmup_steps.\")\\n\\n    parser.add_argument(\\'--logging_steps\\', type=int, default=50,\\n                        help=\"Log every X updates steps.\")\\n    parser.add_argument(\\'--save_steps\\', type=int, default=50,\\n                        help=\"Save checkpoint every X updates steps.\")\\n    parser.add_argument(\\'--save_total_limit\\', type=int, default=None,\\n                        help=\\'Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default\\')\\n    parser.add_argument(\"--eval_all_checkpoints\", action=\\'store_true\\',\\n                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\\n    parser.add_argument(\"--no_cuda\", action=\\'store_true\\',\\n                        help=\"Avoid using CUDA when available\")\\n    parser.add_argument(\\'--overwrite_output_dir\\', action=\\'store_true\\',\\n                        help=\"Overwrite the content of the output directory\")\\n    parser.add_argument(\\'--overwrite_cache\\', action=\\'store_true\\',\\n                        help=\"Overwrite the cached training and evaluation sets\")\\n    parser.add_argument(\\'--seed\\', type=int, default=42,\\n                        help=\"random seed for initialization\")\\n    parser.add_argument(\\'--epoch\\', type=int, default=42,\\n                        help=\"random seed for initialization\")\\n    parser.add_argument(\\'--fp16\\', action=\\'store_true\\',\\n                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\\n    parser.add_argument(\\'--fp16_opt_level\\', type=str, default=\\'O1\\',\\n                        help=\"For fp16: Apex AMP optimization level selected in [\\'O0\\', \\'O1\\', \\'O2\\', and \\'O3\\'].\"\\n                             \"See details at https://nvidia.github.io/apex/amp.html\")\\n    parser.add_argument(\"--local_rank\", type=int, default=-1,\\n                        help=\"For distributed training: local_rank\")\\n    parser.add_argument(\\'--server_ip\\', type=str, default=\\'\\', help=\"For distant debugging.\")\\n    parser.add_argument(\\'--server_port\\', type=str, default=\\'\\', help=\"For distant debugging.\")\\n\\n    # Add early stopping parameters and dropout probability parameters\\n    parser.add_argument(\"--early_stopping_patience\", type=int, default=None,\\n                        help=\"Number of epochs with no improvement after which training will be stopped.\")\\n    parser.add_argument(\"--min_loss_delta\", type=float, default=0.001,\\n                        help=\"Minimum change in the loss required to qualify as an improvement.\")\\n    parser.add_argument(\\'--dropout_probability\\', type=float, default=0, help=\\'dropout probability\\')\\n\\n    # Simulate command-line arguments\\n    simulated_args = [\\n        \\'--output_dir\\', \\'./saved_models\\',\\n        \\'--model_type\\', \\'roberta\\',\\n        \\'--tokenizer_name\\', \\'microsoft/codebert-base\\',\\n        \\'--model_name_or_path\\', \\'microsoft/codebert-base\\',\\n        \\'--do_train\\',\\n        \\'--train_data_file\\', \\'/kaggle/working/dataset/train.jsonl\\',\\n        \\'--eval_data_file\\', \\'/kaggle/working/dataset/test.jsonl\\',\\n        \\'--test_data_file\\', \\'/kaggle/working/dataset/test.jsonl\\',\\n        \\'--epoch\\', \\'10\\',\\n        \\'--block_size\\', \\'512\\',\\n        \\'--train_batch_size\\', \\'16\\',\\n        \\'--eval_batch_size\\', \\'16\\',\\n        \\'--learning_rate\\', \\'2e-5\\',\\n        \\'--max_grad_norm\\', \\'1.0\\',\\n        \\'--evaluate_during_training\\',\\n        \\'--seed\\', \\'123456\\'\\n    ]\\n\\n    # Parse the simulated arguments\\n    args = parser.parse_args(simulated_args)\\n\\n    main(args)\\n'' returned non-zero exit status 1.",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Using CodeBERT freezed"
      ],
      "metadata": {
        "id": "SxItF6YNW1xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%python\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import math\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "import json\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import multiprocessing\n",
        "cpu_cont = multiprocessing.cpu_count()\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "MODEL_CLASSES = {\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, encoder,config,tokenizer,args):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config=config\n",
        "        self.tokenizer=tokenizer\n",
        "        self.args=args\n",
        "\n",
        "        # Define dropout layer, dropout_probability is taken from args.\n",
        "        self.dropout = nn.Dropout(args.dropout_probability)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None,labels=None):\n",
        "        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\n",
        "\n",
        "        # Apply dropout\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        logits=outputs\n",
        "        prob=torch.sigmoid(logits)\n",
        "        if labels is not None:\n",
        "            labels=labels.float()\n",
        "            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\n",
        "            loss=-loss.mean()\n",
        "            return loss,prob\n",
        "        else:\n",
        "            return prob\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 input_tokens,\n",
        "                 input_ids,\n",
        "                 idx,\n",
        "                 label,\n",
        "\n",
        "    ):\n",
        "        self.input_tokens = input_tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.idx=str(idx)\n",
        "        self.label=label\n",
        "def convert_examples_to_features(js,tokenizer,args):\n",
        "    #source\n",
        "    code=' '.join(js['func'].split())\n",
        "    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\n",
        "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
        "    padding_length = args.block_size - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    return InputFeatures(source_tokens,source_ids,js['idx'],js['target'])\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, file_path=None):\n",
        "        self.examples = []\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                js=json.loads(line.strip())\n",
        "                self.examples.append(convert_examples_to_features(js,tokenizer,args))\n",
        "        if 'train' in file_path:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                    logger.info(\"*** Example ***\")\n",
        "                    logger.info(\"idx: {}\".format(idx))\n",
        "                    logger.info(\"label: {}\".format(example.label))\n",
        "                    logger.info(\"input_tokens: {}\".format([x.replace('\\u0120','_') for x in example.input_tokens]))\n",
        "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
        "                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\n",
        "    args.max_steps=args.epoch*len( train_dataloader)\n",
        "    args.save_steps=len( train_dataloader)\n",
        "    args.warmup_steps=len( train_dataloader)\n",
        "    args.logging_steps=len( train_dataloader)\n",
        "    args.num_train_epochs=args.epoch\n",
        "    model.to(args.device)\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\n",
        "                                                num_training_steps=args.max_steps)\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
        "    optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
        "    if os.path.exists(scheduler_last):\n",
        "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
        "    if os.path.exists(optimizer_last):\n",
        "        optimizer.load_state_dict(torch.load(optimizer_last))\n",
        "    # Train!\n",
        "    print(\"***** Running training *****\")\n",
        "    print(\"  Num examples = %d\", len(train_dataset))\n",
        "    print(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
        "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    print(\"  Total optimization steps = %d\", args.max_steps)\n",
        "\n",
        "    global_step = args.start_step\n",
        "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
        "    best_mrr=0.0\n",
        "    best_acc=0.0\n",
        "    # model.resize_token_embeddings(len(tokenizer))\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Initialize early stopping parameters at the start of training\n",
        "    early_stopping_counter = 0\n",
        "    best_loss = None\n",
        "\n",
        "    for idx in range(args.start_epoch, int(args.num_train_epochs)):\n",
        "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "        tr_num=0\n",
        "        train_loss=0\n",
        "        acc = 0\n",
        "        for step, batch in enumerate(bar):\n",
        "            inputs = batch[0].to(args.device)\n",
        "            labels=batch[1].to(args.device)\n",
        "            model.train()\n",
        "            loss,logits = model(inputs,labels)\n",
        "\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            preds=logits[:,0]>0.5\n",
        "            acc+=np.mean(labels==preds)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            tr_num+=1\n",
        "            train_loss+=loss.item()\n",
        "            if avg_loss==0:\n",
        "                avg_loss=tr_loss\n",
        "            avg_loss=round(train_loss/tr_num,5)\n",
        "            avg_acc = round(acc/tr_num,5)\n",
        "            bar.set_description(\"epoch {} loss {} acc {}\".format(idx,avg_loss,avg_acc))\n",
        "\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "                output_flag=True\n",
        "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    logging_loss = tr_loss\n",
        "                    tr_nb=global_step\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer,eval_when_training=True)\n",
        "                        for key, value in results.items():\n",
        "                            print(\"  %s = %s\", key, round(value,4))\n",
        "                        # Save model checkpoint\n",
        "\n",
        "                    if results['eval_acc']>best_acc:\n",
        "                        best_acc=results['eval_acc']\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "                        print(\"  Best acc:%s\",round(best_acc,4))\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "\n",
        "                        checkpoint_prefix = 'checkpoint-best-acc'\n",
        "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "                        if not os.path.exists(output_dir):\n",
        "                            os.makedirs(output_dir)\n",
        "                        model_to_save = model.module if hasattr(model,'module') else model\n",
        "                        output_dir = os.path.join(output_dir, '{}'.format('model.bin'))\n",
        "                        torch.save(model_to_save.state_dict(), output_dir)\n",
        "                        print(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = train_loss / tr_num\n",
        "\n",
        "        # Check for early stopping condition\n",
        "        if args.early_stopping_patience is not None:\n",
        "            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\n",
        "                best_loss = avg_loss\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= args.early_stopping_patience:\n",
        "                    print(\"Early stopping\")\n",
        "                    break  # Exit the loop early\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer,eval_when_training=False):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1 and eval_when_training is False:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running evaluation *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in eval_dataloader:\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            lm_loss,logit = model(inputs,label)\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "        nb_eval_steps += 1\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    eval_acc=np.mean(labels==preds)\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.tensor(eval_loss)\n",
        "\n",
        "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
        "    cm = confusion_matrix(y_true=labels, y_pred=preds)\n",
        "    tp = cm[1][1]\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    precision = 0 if tp + fp == 0 else tp/(tp+fp)\n",
        "    recall = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    fprate = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    pdVal = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    pfVal = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    G = 0 if pdVal+1-pfVal == 0 else 2*pdVal*(1-pfVal)/(pdVal+1-pfVal)\n",
        "    F1 = 0 if precision+recall == 0 else 2 * precision * recall / (precision+recall)\n",
        "    MCC = 0 if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) == 0 else (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
        "    result = {\n",
        "        \"eval_loss\": float(perplexity),\n",
        "        \"eval_acc\":round(eval_acc,4),\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"G\": G,\n",
        "        \"MCC\": MCC\n",
        "    }\n",
        "    return result\n",
        "\n",
        "\n",
        "def test(args, model, tokenizer):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\n",
        "\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running Test *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            logit = model(inputs)\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    with open(os.path.join(args.output_dir,\"predictions.txt\"),'w') as f:\n",
        "        for example,pred in zip(eval_dataset.examples,preds):\n",
        "            if pred:\n",
        "                f.write(example.idx+'\\t1\\n')\n",
        "            else:\n",
        "                f.write(example.idx+'\\t0\\n')\n",
        "\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu\n",
        "    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu\n",
        "    # Setup logging\n",
        "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    args.start_epoch = 0\n",
        "    args.start_step = 0\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
        "        args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
        "        args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
        "        idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
        "        with open(idx_file, encoding='utf-8') as idxf:\n",
        "            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
        "\n",
        "        step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
        "        if os.path.exists(step_file):\n",
        "            with open(step_file, encoding='utf-8') as stepf:\n",
        "                args.start_step = int(stepf.readlines()[0].strip())\n",
        "\n",
        "        print(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))\n",
        "\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
        "                                          cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    config.num_labels=1\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\n",
        "                                                do_lower_case=args.do_lower_case,\n",
        "                                                cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    if args.block_size <= 0:\n",
        "        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "    if args.model_name_or_path:\n",
        "        model = model_class.from_pretrained(args.model_name_or_path,\n",
        "                                            from_tf=bool('.ckpt' in args.model_name_or_path),\n",
        "                                            config=config,\n",
        "                                            cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    else:\n",
        "        model = model_class(config)\n",
        "    for param in model.roberta.parameters():\n",
        "        param.requires_grad=False\n",
        "    model=Model(model,config,tokenizer,args)\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    print(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        if args.local_rank not in [-1, 0]:\n",
        "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\n",
        "        if args.local_rank == 0:\n",
        "            torch.distributed.barrier()\n",
        "\n",
        "        train(args, train_dataset, model, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            result=evaluate(args, model, tokenizer)\n",
        "            print(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                print(\"  %s = %s\", key, str(round(result[key],4)))\n",
        "\n",
        "    if args.do_test and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            test(args, model, tokenizer)\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
        "                        help=\"The input training data file (a text file).\")\n",
        "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "    parser.add_argument(\"--test_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "\n",
        "    parser.add_argument(\"--model_type\", default=\"bert\", type=str,\n",
        "                        help=\"The model architecture to be fine-tuned.\")\n",
        "    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\n",
        "                        help=\"The model checkpoint for weights initialization.\")\n",
        "\n",
        "    parser.add_argument(\"--mlm\", action='store_true',\n",
        "                        help=\"Train with masked-language modeling loss instead of language modeling.\")\n",
        "    parser.add_argument(\"--mlm_probability\", type=float, default=0.15,\n",
        "                        help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
        "\n",
        "    parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
        "                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\n",
        "    parser.add_argument(\"--block_size\", default=-1, type=int,\n",
        "                        help=\"Optional input sequence length after tokenization.\"\n",
        "                             \"The training dataset will be truncated in block of this size for training.\"\n",
        "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
        "    parser.add_argument(\"--do_train\", action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--do_test\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
        "                        help=\"Run evaluation during training at each logging step.\")\n",
        "    parser.add_argument(\"--do_lower_case\", action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "\n",
        "    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
        "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight deay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                        help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", default=1.0, type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
        "                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument('--logging_steps', type=int, default=50,\n",
        "                        help=\"Log every X updates steps.\")\n",
        "    parser.add_argument('--save_steps', type=int, default=50,\n",
        "                        help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument('--save_total_limit', type=int, default=None,\n",
        "                        help='Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default')\n",
        "    parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n",
        "                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\n",
        "    parser.add_argument(\"--no_cuda\", action='store_true',\n",
        "                        help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument('--overwrite_output_dir', action='store_true',\n",
        "                        help=\"Overwrite the content of the output directory\")\n",
        "    parser.add_argument('--overwrite_cache', action='store_true',\n",
        "                        help=\"Overwrite the cached training and evaluation sets\")\n",
        "    parser.add_argument('--seed', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--epoch', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--fp16', action='store_true',\n",
        "                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
        "    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
        "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                        help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument('--server_ip', type=str, default='', help=\"For distant debugging.\")\n",
        "    parser.add_argument('--server_port', type=str, default='', help=\"For distant debugging.\")\n",
        "\n",
        "    # Add early stopping parameters and dropout probability parameters\n",
        "    parser.add_argument(\"--early_stopping_patience\", type=int, default=None,\n",
        "                        help=\"Number of epochs with no improvement after which training will be stopped.\")\n",
        "    parser.add_argument(\"--min_loss_delta\", type=float, default=0.001,\n",
        "                        help=\"Minimum change in the loss required to qualify as an improvement.\")\n",
        "    parser.add_argument('--dropout_probability', type=float, default=0, help='dropout probability')\n",
        "\n",
        "    # Simulate command-line arguments\n",
        "#   '--train_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl',\n",
        "#   '--eval_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl',\n",
        "#   '--test_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl',\n",
        "    simulated_args = [\n",
        "        '--output_dir', './saved_models',\n",
        "        '--model_type', 'roberta',\n",
        "        '--tokenizer_name', 'microsoft/codebert-base',\n",
        "        '--model_name_or_path', 'microsoft/codebert-base',\n",
        "        '--do_train',\n",
        "        '--train_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl',\n",
        "        '--eval_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl',\n",
        "        '--test_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl',\n",
        "        '--epoch', '5',\n",
        "        '--block_size', '512',\n",
        "        '--train_batch_size', '16',\n",
        "        '--eval_batch_size', '16',\n",
        "        '--learning_rate', '2e-5',\n",
        "        '--max_grad_norm', '1.0',\n",
        "        '--evaluate_during_training',\n",
        "        '--seed', '123456'\n",
        "    ]\n",
        "\n",
        "    # Parse the simulated arguments\n",
        "    args = parser.parse_args(simulated_args)\n",
        "\n",
        "    main(args)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T17:36:38.794766Z",
          "iopub.execute_input": "2024-07-01T17:36:38.795165Z",
          "iopub.status.idle": "2024-07-01T18:16:45.890887Z",
          "shell.execute_reply.started": "2024-07-01T17:36:38.79513Z",
          "shell.execute_reply": "2024-07-01T18:16:45.88987Z"
        },
        "trusted": true,
        "id": "sWu48vEFW1xk",
        "outputId": "bd41056d-af25-4cac-f19a-00014a7ca575"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-07-01 17:36:41.918127: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-01 17:36:41.918190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-01 17:36:41.920395: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training/evaluation parameters %s Namespace(train_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl', output_dir='./saved_models', eval_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl', test_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='', block_size=510, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=16, eval_batch_size=16, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, start_epoch=0, start_step=0)\n***** Running training *****\n  Num examples = %d 21854\n  Num Epochs = %d 5\n  Instantaneous batch size per GPU = %d 16\n  Total train batch size (w. parallel, distributed & accumulation) = %d 16\n  Gradient Accumulation steps = %d 1\n  Total optimization steps = %d 6830\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.69151 acc 0.53015:  77%|███████▋  | 1054/1366 [05:13<01:32,  3.37it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.69068 acc 0.53282: 100%|██████████| 1366/1366 [07:44<00:00,  2.94it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6826\n  %s = %s eval_acc 0.5655\n  %s = %s f1 0.0\n  %s = %s precision 0\n  %s = %s recall 0.0\n  %s = %s G 0.0\n  %s = %s MCC 0\n  ********************\n  Best acc:%s 0.5655\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.68667 acc 0.54639:  77%|███████▋  | 1054/1366 [05:12<01:32,  3.37it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.68667 acc 0.54691: 100%|██████████| 1366/1366 [07:42<00:00,  2.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6816\n  %s = %s eval_acc 0.5549\n  %s = %s f1 0.4432\n  %s = %s precision 0.4855\n  %s = %s recall 0.4078\n  %s = %s G 0.5064\n  %s = %s MCC 0.078\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 2 loss 0.68645 acc 0.55261:  77%|███████▋  | 1054/1366 [05:12<01:32,  3.37it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 2 loss 0.68629 acc 0.55103: 100%|██████████| 1366/1366 [07:42<00:00,  2.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6787\n  %s = %s eval_acc 0.5564\n  %s = %s f1 0.3333\n  %s = %s precision 0.4802\n  %s = %s recall 0.2553\n  %s = %s G 0.3856\n  %s = %s MCC 0.0505\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 3 loss 0.68416 acc 0.558:  77%|███████▋  | 1054/1366 [05:12<01:32,  3.37it/s]  ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 3 loss 0.68418 acc 0.55573: 100%|██████████| 1366/1366 [07:43<00:00,  2.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6773\n  %s = %s eval_acc 0.5717\n  %s = %s f1 0.1384\n  %s = %s precision 0.5497\n  %s = %s recall 0.0792\n  %s = %s G 0.1462\n  %s = %s MCC 0.0601\n  ********************\n  Best acc:%s 0.5717\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 4 loss 0.68372 acc 0.55652:  77%|███████▋  | 1055/1366 [05:12<01:32,  3.38it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 4 loss 0.68332 acc 0.55735: 100%|██████████| 1366/1366 [07:42<00:00,  2.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.677\n  %s = %s eval_acc 0.5648\n  %s = %s f1 0.2373\n  %s = %s precision 0.4973\n  %s = %s recall 0.1559\n  %s = %s G 0.2648\n  %s = %s MCC 0.0503\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Using CodeBERT-mlm"
      ],
      "metadata": {
        "id": "lpWNpuXHW1xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%python\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import math\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "import json\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import multiprocessing\n",
        "cpu_cont = multiprocessing.cpu_count()\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "MODEL_CLASSES = {\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, encoder,config,tokenizer,args):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config=config\n",
        "        self.tokenizer=tokenizer\n",
        "        self.args=args\n",
        "\n",
        "        # Define dropout layer, dropout_probability is taken from args.\n",
        "        self.dropout = nn.Dropout(args.dropout_probability)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None,labels=None):\n",
        "        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\n",
        "\n",
        "        # Apply dropout\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        logits=outputs\n",
        "        prob=torch.sigmoid(logits)\n",
        "        if labels is not None:\n",
        "            labels=labels.float()\n",
        "            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\n",
        "            loss=-loss.mean()\n",
        "            return loss,prob\n",
        "        else:\n",
        "            return prob\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 input_tokens,\n",
        "                 input_ids,\n",
        "                 idx,\n",
        "                 label,\n",
        "\n",
        "    ):\n",
        "        self.input_tokens = input_tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.idx=str(idx)\n",
        "        self.label=label\n",
        "def convert_examples_to_features(js,tokenizer,args):\n",
        "    #source\n",
        "    code=' '.join(js['func'].split())\n",
        "    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\n",
        "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
        "    padding_length = args.block_size - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    return InputFeatures(source_tokens,source_ids,js['idx'],js['target'])\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, file_path=None):\n",
        "        self.examples = []\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                js=json.loads(line.strip())\n",
        "                self.examples.append(convert_examples_to_features(js,tokenizer,args))\n",
        "        if 'train' in file_path:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                    logger.info(\"*** Example ***\")\n",
        "                    logger.info(\"idx: {}\".format(idx))\n",
        "                    logger.info(\"label: {}\".format(example.label))\n",
        "                    logger.info(\"input_tokens: {}\".format([x.replace('\\u0120','_') for x in example.input_tokens]))\n",
        "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
        "                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\n",
        "    args.max_steps=args.epoch*len( train_dataloader)\n",
        "    args.save_steps=len( train_dataloader)\n",
        "    args.warmup_steps=len( train_dataloader)\n",
        "    args.logging_steps=len( train_dataloader)\n",
        "    args.num_train_epochs=args.epoch\n",
        "    model.to(args.device)\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\n",
        "                                                num_training_steps=args.max_steps)\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
        "    optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
        "    if os.path.exists(scheduler_last):\n",
        "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
        "    if os.path.exists(optimizer_last):\n",
        "        optimizer.load_state_dict(torch.load(optimizer_last))\n",
        "    # Train!\n",
        "    print(\"***** Running training *****\")\n",
        "    print(\"  Num examples = %d\", len(train_dataset))\n",
        "    print(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
        "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    print(\"  Total optimization steps = %d\", args.max_steps)\n",
        "\n",
        "    global_step = args.start_step\n",
        "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
        "    best_mrr=0.0\n",
        "    best_acc=0.0\n",
        "    # model.resize_token_embeddings(len(tokenizer))\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Initialize early stopping parameters at the start of training\n",
        "    early_stopping_counter = 0\n",
        "    best_loss = None\n",
        "\n",
        "    for idx in range(args.start_epoch, int(args.num_train_epochs)):\n",
        "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "        tr_num=0\n",
        "        train_loss=0\n",
        "        acc = 0\n",
        "        for step, batch in enumerate(bar):\n",
        "            inputs = batch[0].to(args.device)\n",
        "            labels=batch[1].to(args.device)\n",
        "            model.train()\n",
        "            loss,logits = model(inputs,labels)\n",
        "\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            preds=logits[:,0]>0.5\n",
        "            acc+=np.mean(labels==preds)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            tr_num+=1\n",
        "            train_loss+=loss.item()\n",
        "            if avg_loss==0:\n",
        "                avg_loss=tr_loss\n",
        "            avg_loss=round(train_loss/tr_num,5)\n",
        "            avg_acc = round(acc/tr_num,5)\n",
        "            bar.set_description(\"epoch {} loss {} acc {}\".format(idx,avg_loss,avg_acc))\n",
        "\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "                output_flag=True\n",
        "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    logging_loss = tr_loss\n",
        "                    tr_nb=global_step\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer,eval_when_training=True)\n",
        "                        for key, value in results.items():\n",
        "                            print(\"  %s = %s\", key, round(value,4))\n",
        "                        # Save model checkpoint\n",
        "\n",
        "                    if results['eval_acc']>best_acc:\n",
        "                        best_acc=results['eval_acc']\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "                        print(\"  Best acc:%s\",round(best_acc,4))\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "\n",
        "                        checkpoint_prefix = 'checkpoint-best-acc'\n",
        "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "                        if not os.path.exists(output_dir):\n",
        "                            os.makedirs(output_dir)\n",
        "                        model_to_save = model.module if hasattr(model,'module') else model\n",
        "                        output_dir = os.path.join(output_dir, '{}'.format('model.bin'))\n",
        "                        torch.save(model_to_save.state_dict(), output_dir)\n",
        "                        print(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = train_loss / tr_num\n",
        "\n",
        "        # Check for early stopping condition\n",
        "        if args.early_stopping_patience is not None:\n",
        "            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\n",
        "                best_loss = avg_loss\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= args.early_stopping_patience:\n",
        "                    print(\"Early stopping\")\n",
        "                    break  # Exit the loop early\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer,eval_when_training=False):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1 and eval_when_training is False:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running evaluation *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in eval_dataloader:\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            lm_loss,logit = model(inputs,label)\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "        nb_eval_steps += 1\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    eval_acc=np.mean(labels==preds)\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.tensor(eval_loss)\n",
        "\n",
        "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
        "    cm = confusion_matrix(y_true=labels, y_pred=preds)\n",
        "    tp = cm[1][1]\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    precision = 0 if tp + fp == 0 else tp/(tp+fp)\n",
        "    recall = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    fprate = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    pdVal = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    pfVal = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    G = 0 if pdVal+1-pfVal == 0 else 2*pdVal*(1-pfVal)/(pdVal+1-pfVal)\n",
        "    F1 = 0 if precision+recall == 0 else 2 * precision * recall / (precision+recall)\n",
        "    MCC = 0 if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) == 0 else (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
        "    result = {\n",
        "        \"eval_loss\": float(perplexity),\n",
        "        \"eval_acc\":round(eval_acc,4),\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"G\": G,\n",
        "        \"MCC\": MCC\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def test(args, model, tokenizer):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\n",
        "\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running Test *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            logit = model(inputs)\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    with open(os.path.join(args.output_dir,\"predictions.txt\"),'w') as f:\n",
        "        for example,pred in zip(eval_dataset.examples,preds):\n",
        "            if pred:\n",
        "                f.write(example.idx+'\\t1\\n')\n",
        "            else:\n",
        "                f.write(example.idx+'\\t0\\n')\n",
        "\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu\n",
        "    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu\n",
        "    # Setup logging\n",
        "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    args.start_epoch = 0\n",
        "    args.start_step = 0\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
        "        args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
        "        args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
        "        idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
        "        with open(idx_file, encoding='utf-8') as idxf:\n",
        "            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
        "\n",
        "        step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
        "        if os.path.exists(step_file):\n",
        "            with open(step_file, encoding='utf-8') as stepf:\n",
        "                args.start_step = int(stepf.readlines()[0].strip())\n",
        "\n",
        "        print(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))\n",
        "\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
        "                                          cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    config.num_labels=1\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\n",
        "                                                do_lower_case=args.do_lower_case,\n",
        "                                                cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    if args.block_size <= 0:\n",
        "        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "    if args.model_name_or_path:\n",
        "        model = model_class.from_pretrained(args.model_name_or_path,\n",
        "                                            from_tf=bool('.ckpt' in args.model_name_or_path),\n",
        "                                            config=config,\n",
        "                                            cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    else:\n",
        "        model = model_class(config)\n",
        "\n",
        "    model=Model(model,config,tokenizer,args)\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    print(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        if args.local_rank not in [-1, 0]:\n",
        "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\n",
        "        if args.local_rank == 0:\n",
        "            torch.distributed.barrier()\n",
        "\n",
        "        train(args, train_dataset, model, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            result=evaluate(args, model, tokenizer)\n",
        "            print(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                print(\"  %s = %s\", key, str(round(result[key],4)))\n",
        "\n",
        "    if args.do_test and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            test(args, model, tokenizer)\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
        "                        help=\"The input training data file (a text file).\")\n",
        "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "    parser.add_argument(\"--test_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "\n",
        "    parser.add_argument(\"--model_type\", default=\"bert\", type=str,\n",
        "                        help=\"The model architecture to be fine-tuned.\")\n",
        "    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\n",
        "                        help=\"The model checkpoint for weights initialization.\")\n",
        "\n",
        "    parser.add_argument(\"--mlm\", action='store_true',\n",
        "                        help=\"Train with masked-language modeling loss instead of language modeling.\")\n",
        "    parser.add_argument(\"--mlm_probability\", type=float, default=0.15,\n",
        "                        help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
        "\n",
        "    parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
        "                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\n",
        "    parser.add_argument(\"--block_size\", default=-1, type=int,\n",
        "                        help=\"Optional input sequence length after tokenization.\"\n",
        "                             \"The training dataset will be truncated in block of this size for training.\"\n",
        "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
        "    parser.add_argument(\"--do_train\", action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--do_test\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
        "                        help=\"Run evaluation during training at each logging step.\")\n",
        "    parser.add_argument(\"--do_lower_case\", action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "\n",
        "    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
        "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight deay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                        help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", default=1.0, type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
        "                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument('--logging_steps', type=int, default=50,\n",
        "                        help=\"Log every X updates steps.\")\n",
        "    parser.add_argument('--save_steps', type=int, default=50,\n",
        "                        help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument('--save_total_limit', type=int, default=None,\n",
        "                        help='Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default')\n",
        "    parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n",
        "                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\n",
        "    parser.add_argument(\"--no_cuda\", action='store_true',\n",
        "                        help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument('--overwrite_output_dir', action='store_true',\n",
        "                        help=\"Overwrite the content of the output directory\")\n",
        "    parser.add_argument('--overwrite_cache', action='store_true',\n",
        "                        help=\"Overwrite the cached training and evaluation sets\")\n",
        "    parser.add_argument('--seed', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--epoch', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--fp16', action='store_true',\n",
        "                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
        "    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
        "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                        help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument('--server_ip', type=str, default='', help=\"For distant debugging.\")\n",
        "    parser.add_argument('--server_port', type=str, default='', help=\"For distant debugging.\")\n",
        "\n",
        "    # Add early stopping parameters and dropout probability parameters\n",
        "    parser.add_argument(\"--early_stopping_patience\", type=int, default=None,\n",
        "                        help=\"Number of epochs with no improvement after which training will be stopped.\")\n",
        "    parser.add_argument(\"--min_loss_delta\", type=float, default=0.001,\n",
        "                        help=\"Minimum change in the loss required to qualify as an improvement.\")\n",
        "    parser.add_argument('--dropout_probability', type=float, default=0, help='dropout probability')\n",
        "\n",
        "    # Simulate command-line arguments\n",
        "    simulated_args = [\n",
        "        '--output_dir', './saved_models',\n",
        "        '--model_type', 'roberta',\n",
        "        '--tokenizer_name', 'microsoft/codebert-base-mlm',\n",
        "        '--model_name_or_path', 'microsoft/codebert-base-mlm',\n",
        "        '--do_train',\n",
        "        '--train_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl',\n",
        "        '--eval_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl',\n",
        "        '--test_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl',\n",
        "        '--epoch', '5',\n",
        "        '--block_size', '512',\n",
        "        '--train_batch_size', '16',\n",
        "        '--eval_batch_size', '16',\n",
        "        '--learning_rate', '2e-5',\n",
        "        '--max_grad_norm', '1.0',\n",
        "        '--evaluate_during_training',\n",
        "        '--seed', '123456'\n",
        "    ]\n",
        "\n",
        "    # Parse the simulated arguments\n",
        "    args = parser.parse_args(simulated_args)\n",
        "\n",
        "    main(args)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T14:35:43.140255Z",
          "iopub.execute_input": "2024-07-01T14:35:43.140669Z",
          "iopub.status.idle": "2024-07-01T16:18:01.055454Z",
          "shell.execute_reply.started": "2024-07-01T14:35:43.140638Z",
          "shell.execute_reply": "2024-07-01T16:18:01.054388Z"
        },
        "trusted": true,
        "id": "E4idRLQ3W1xm",
        "outputId": "576558c8-2929-401e-bbab-0574812f23ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-07-01 14:35:46.377958: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-01 14:35:46.378029: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-01 14:35:46.380187: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base-mlm and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training/evaluation parameters %s Namespace(train_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl', output_dir='./saved_models', eval_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl', test_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base-mlm', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base-mlm', cache_dir='', block_size=510, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=16, eval_batch_size=16, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, start_epoch=0, start_step=0)\n***** Running training *****\n  Num examples = %d 21854\n  Num Epochs = %d 5\n  Instantaneous batch size per GPU = %d 16\n  Total train batch size (w. parallel, distributed & accumulation) = %d 16\n  Gradient Accumulation steps = %d 1\n  Total optimization steps = %d 6830\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.6824 acc 0.55082:  77%|███████▋  | 1054/1366 [14:46<04:22,  1.19it/s] ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.67652 acc 0.56006: 100%|██████████| 1366/1366 [20:08<00:00,  1.13it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6314\n  %s = %s eval_acc 0.6204\n  %s = %s f1 0.3563\n  %s = %s precision 0.6769\n  %s = %s recall 0.2418\n  %s = %s G 0.3822\n  %s = %s MCC 0.2096\n  ********************\n  Best acc:%s 0.6204\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.61654 acc 0.6362:  77%|███████▋  | 1054/1366 [14:48<04:22,  1.19it/s] ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.61378 acc 0.63802: 100%|██████████| 1366/1366 [20:09<00:00,  1.13it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.5999\n  %s = %s eval_acc 0.6362\n  %s = %s f1 0.5727\n  %s = %s precision 0.5847\n  %s = %s recall 0.5611\n  %s = %s G 0.6204\n  %s = %s MCC 0.2563\n  ********************\n  Best acc:%s 0.6362\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 2 loss 0.54099 acc 0.70616:  77%|███████▋  | 1054/1366 [14:48<04:22,  1.19it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 2 loss 0.54032 acc 0.70615: 100%|██████████| 1366/1366 [20:08<00:00,  1.13it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6333\n  %s = %s eval_acc 0.6307\n  %s = %s f1 0.6191\n  %s = %s precision 0.5609\n  %s = %s recall 0.6908\n  %s = %s G 0.6332\n  %s = %s MCC 0.2736\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 3 loss 0.46734 acc 0.75556:  77%|███████▋  | 1055/1366 [14:49<04:22,  1.19it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 3 loss 0.46931 acc 0.75387: 100%|██████████| 1366/1366 [20:09<00:00,  1.13it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.7039\n  %s = %s eval_acc 0.6395\n  %s = %s f1 0.5911\n  %s = %s precision 0.5827\n  %s = %s recall 0.5998\n  %s = %s G 0.6329\n  %s = %s MCC 0.2689\n  ********************\n  Best acc:%s 0.6395\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 4 loss 0.41235 acc 0.79431:  77%|███████▋  | 1054/1366 [14:48<04:22,  1.19it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 4 loss 0.41322 acc 0.79436: 100%|██████████| 1366/1366 [20:09<00:00,  1.13it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.7671\n  %s = %s eval_acc 0.6468\n  %s = %s f1 0.5885\n  %s = %s precision 0.5959\n  %s = %s recall 0.5813\n  %s = %s G 0.6339\n  %s = %s MCC 0.2792\n  ********************\n  Best acc:%s 0.6468\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Using CodeBERTa (from HuggingFace)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-16T16:17:27.842368Z",
          "iopub.execute_input": "2024-07-16T16:17:27.842853Z",
          "iopub.status.idle": "2024-07-16T16:17:27.850399Z",
          "shell.execute_reply.started": "2024-07-16T16:17:27.842817Z",
          "shell.execute_reply": "2024-07-16T16:17:27.84901Z"
        },
        "id": "e3bhgIQKW1xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%python\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import math\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "import json\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import multiprocessing\n",
        "cpu_cont = multiprocessing.cpu_count()\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "MODEL_CLASSES = {\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, encoder,config,tokenizer,args):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config=config\n",
        "        self.tokenizer=tokenizer\n",
        "        self.args=args\n",
        "\n",
        "        # Define dropout layer, dropout_probability is taken from args.\n",
        "        self.dropout = nn.Dropout(args.dropout_probability)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None,labels=None):\n",
        "        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\n",
        "\n",
        "        # Apply dropout\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        logits=outputs\n",
        "        prob=torch.sigmoid(logits)\n",
        "        if labels is not None:\n",
        "            labels=labels.float()\n",
        "            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\n",
        "            loss=-loss.mean()\n",
        "            return loss,prob\n",
        "        else:\n",
        "            return prob\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 input_tokens,\n",
        "                 input_ids,\n",
        "                 idx,\n",
        "                 label,\n",
        "\n",
        "    ):\n",
        "        self.input_tokens = input_tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.idx=str(idx)\n",
        "        self.label=label\n",
        "def convert_examples_to_features(js,tokenizer,args):\n",
        "    #source\n",
        "    code=' '.join(js['func'].split())\n",
        "    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\n",
        "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
        "    padding_length = args.block_size - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    return InputFeatures(source_tokens,source_ids,js['idx'],js['target'])\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, file_path=None):\n",
        "        self.examples = []\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                js=json.loads(line.strip())\n",
        "                self.examples.append(convert_examples_to_features(js,tokenizer,args))\n",
        "        if 'train' in file_path:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                    logger.info(\"*** Example ***\")\n",
        "                    logger.info(\"idx: {}\".format(idx))\n",
        "                    logger.info(\"label: {}\".format(example.label))\n",
        "                    logger.info(\"input_tokens: {}\".format([x.replace('\\u0120','_') for x in example.input_tokens]))\n",
        "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
        "                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\n",
        "    args.max_steps=args.epoch*len( train_dataloader)\n",
        "    args.save_steps=len( train_dataloader)\n",
        "    args.warmup_steps=len( train_dataloader)\n",
        "    args.logging_steps=len( train_dataloader)\n",
        "    args.num_train_epochs=args.epoch\n",
        "    model.to(args.device)\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\n",
        "                                                num_training_steps=args.max_steps)\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
        "    optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
        "    if os.path.exists(scheduler_last):\n",
        "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
        "    if os.path.exists(optimizer_last):\n",
        "        optimizer.load_state_dict(torch.load(optimizer_last))\n",
        "    # Train!\n",
        "    print(\"***** Running training *****\")\n",
        "    print(\"  Num examples = %d\", len(train_dataset))\n",
        "    print(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
        "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    print(\"  Total optimization steps = %d\", args.max_steps)\n",
        "\n",
        "    global_step = args.start_step\n",
        "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
        "    best_mrr=0.0\n",
        "    best_acc=0.0\n",
        "    # model.resize_token_embeddings(len(tokenizer))\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Initialize early stopping parameters at the start of training\n",
        "    early_stopping_counter = 0\n",
        "    best_loss = None\n",
        "\n",
        "    for idx in range(args.start_epoch, int(args.num_train_epochs)):\n",
        "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "        tr_num=0\n",
        "        train_loss=0\n",
        "        acc = 0\n",
        "        for step, batch in enumerate(bar):\n",
        "            inputs = batch[0].to(args.device)\n",
        "            labels=batch[1].to(args.device)\n",
        "            model.train()\n",
        "            loss,logits = model(inputs,labels)\n",
        "\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            preds=logits[:,0]>0.5\n",
        "            acc+=np.mean(labels==preds)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            tr_num+=1\n",
        "            train_loss+=loss.item()\n",
        "            if avg_loss==0:\n",
        "                avg_loss=tr_loss\n",
        "            avg_loss=round(train_loss/tr_num,5)\n",
        "            avg_acc = round(acc/tr_num,5)\n",
        "            bar.set_description(\"epoch {} loss {} acc {}\".format(idx,avg_loss,avg_acc))\n",
        "\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "                output_flag=True\n",
        "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    logging_loss = tr_loss\n",
        "                    tr_nb=global_step\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer,eval_when_training=True)\n",
        "                        for key, value in results.items():\n",
        "                            print(\"  %s = %s\", key, round(value,4))\n",
        "                        # Save model checkpoint\n",
        "\n",
        "                    if results['eval_acc']>best_acc:\n",
        "                        best_acc=results['eval_acc']\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "                        print(\"  Best acc:%s\",round(best_acc,4))\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "\n",
        "                        checkpoint_prefix = 'checkpoint-best-acc'\n",
        "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "                        if not os.path.exists(output_dir):\n",
        "                            os.makedirs(output_dir)\n",
        "                        model_to_save = model.module if hasattr(model,'module') else model\n",
        "                        output_dir = os.path.join(output_dir, '{}'.format('model.bin'))\n",
        "                        torch.save(model_to_save.state_dict(), output_dir)\n",
        "                        print(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = train_loss / tr_num\n",
        "\n",
        "        # Check for early stopping condition\n",
        "        if args.early_stopping_patience is not None:\n",
        "            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\n",
        "                best_loss = avg_loss\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= args.early_stopping_patience:\n",
        "                    print(\"Early stopping\")\n",
        "                    break  # Exit the loop early\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer,eval_when_training=False):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1 and eval_when_training is False:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running evaluation *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in eval_dataloader:\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            lm_loss,logit = model(inputs,label)\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "        nb_eval_steps += 1\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    eval_acc=np.mean(labels==preds)\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.tensor(eval_loss)\n",
        "\n",
        "\n",
        "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
        "    cm = confusion_matrix(y_true=labels, y_pred=preds)\n",
        "    tp = cm[1][1]\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    precision = 0 if tp + fp == 0 else tp/(tp+fp)\n",
        "    recall = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    fprate = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    pdVal = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    pfVal = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    G = 0 if pdVal+1-pfVal == 0 else 2*pdVal*(1-pfVal)/(pdVal+1-pfVal)\n",
        "    F1 = 0 if precision+recall == 0 else 2 * precision * recall / (precision+recall)\n",
        "    MCC = 0 if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) == 0 else (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
        "    result = {\n",
        "        \"eval_loss\": float(perplexity),\n",
        "        \"eval_acc\":round(eval_acc,4),\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"G\": G,\n",
        "        \"MCC\": MCC\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def test(args, model, tokenizer):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\n",
        "\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running Test *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            logit = model(inputs)\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    with open(os.path.join(args.output_dir,\"predictions.txt\"),'w') as f:\n",
        "        for example,pred in zip(eval_dataset.examples,preds):\n",
        "            if pred:\n",
        "                f.write(example.idx+'\\t1\\n')\n",
        "            else:\n",
        "                f.write(example.idx+'\\t0\\n')\n",
        "\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu\n",
        "    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu\n",
        "    # Setup logging\n",
        "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    args.start_epoch = 0\n",
        "    args.start_step = 0\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
        "        args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
        "        args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
        "        idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
        "        with open(idx_file, encoding='utf-8') as idxf:\n",
        "            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
        "\n",
        "        step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
        "        if os.path.exists(step_file):\n",
        "            with open(step_file, encoding='utf-8') as stepf:\n",
        "                args.start_step = int(stepf.readlines()[0].strip())\n",
        "\n",
        "        print(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))\n",
        "\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
        "                                          cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    config.num_labels=1\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\n",
        "                                                do_lower_case=args.do_lower_case,\n",
        "                                                cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    if args.block_size <= 0:\n",
        "        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "    if args.model_name_or_path:\n",
        "        model = model_class.from_pretrained(args.model_name_or_path,\n",
        "                                            from_tf=bool('.ckpt' in args.model_name_or_path),\n",
        "                                            config=config,\n",
        "                                            cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    else:\n",
        "        model = model_class(config)\n",
        "\n",
        "    model=Model(model,config,tokenizer,args)\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    print(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        if args.local_rank not in [-1, 0]:\n",
        "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\n",
        "        if args.local_rank == 0:\n",
        "            torch.distributed.barrier()\n",
        "\n",
        "        train(args, train_dataset, model, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            result=evaluate(args, model, tokenizer)\n",
        "            print(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                print(\"  %s = %s\", key, str(round(result[key],4)))\n",
        "\n",
        "    if args.do_test and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            test(args, model, tokenizer)\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
        "                        help=\"The input training data file (a text file).\")\n",
        "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "    parser.add_argument(\"--test_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "\n",
        "    parser.add_argument(\"--model_type\", default=\"bert\", type=str,\n",
        "                        help=\"The model architecture to be fine-tuned.\")\n",
        "    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\n",
        "                        help=\"The model checkpoint for weights initialization.\")\n",
        "\n",
        "    parser.add_argument(\"--mlm\", action='store_true',\n",
        "                        help=\"Train with masked-language modeling loss instead of language modeling.\")\n",
        "    parser.add_argument(\"--mlm_probability\", type=float, default=0.15,\n",
        "                        help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
        "\n",
        "    parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
        "                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\n",
        "    parser.add_argument(\"--block_size\", default=-1, type=int,\n",
        "                        help=\"Optional input sequence length after tokenization.\"\n",
        "                             \"The training dataset will be truncated in block of this size for training.\"\n",
        "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
        "    parser.add_argument(\"--do_train\", action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--do_test\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
        "                        help=\"Run evaluation during training at each logging step.\")\n",
        "    parser.add_argument(\"--do_lower_case\", action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "\n",
        "    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
        "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight deay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                        help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", default=1.0, type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
        "                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument('--logging_steps', type=int, default=50,\n",
        "                        help=\"Log every X updates steps.\")\n",
        "    parser.add_argument('--save_steps', type=int, default=50,\n",
        "                        help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument('--save_total_limit', type=int, default=None,\n",
        "                        help='Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default')\n",
        "    parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n",
        "                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\n",
        "    parser.add_argument(\"--no_cuda\", action='store_true',\n",
        "                        help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument('--overwrite_output_dir', action='store_true',\n",
        "                        help=\"Overwrite the content of the output directory\")\n",
        "    parser.add_argument('--overwrite_cache', action='store_true',\n",
        "                        help=\"Overwrite the cached training and evaluation sets\")\n",
        "    parser.add_argument('--seed', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--epoch', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--fp16', action='store_true',\n",
        "                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
        "    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
        "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                        help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument('--server_ip', type=str, default='', help=\"For distant debugging.\")\n",
        "    parser.add_argument('--server_port', type=str, default='', help=\"For distant debugging.\")\n",
        "\n",
        "    # Add early stopping parameters and dropout probability parameters\n",
        "    parser.add_argument(\"--early_stopping_patience\", type=int, default=None,\n",
        "                        help=\"Number of epochs with no improvement after which training will be stopped.\")\n",
        "    parser.add_argument(\"--min_loss_delta\", type=float, default=0.001,\n",
        "                        help=\"Minimum change in the loss required to qualify as an improvement.\")\n",
        "    parser.add_argument('--dropout_probability', type=float, default=0, help='dropout probability')\n",
        "\n",
        "    # Simulate command-line arguments\n",
        "    simulated_args = [\n",
        "        '--output_dir', './saved_models',\n",
        "        '--model_type', 'roberta',\n",
        "        '--tokenizer_name', 'huggingface/CodeBERTa-small-v1',\n",
        "        '--model_name_or_path', 'huggingface/CodeBERTa-small-v1',\n",
        "        '--do_train',\n",
        "        '--train_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl',\n",
        "        '--eval_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl',\n",
        "        '--test_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl',\n",
        "        '--epoch', '5',\n",
        "        '--block_size', '512',\n",
        "        '--train_batch_size', '16',\n",
        "        '--eval_batch_size', '16',\n",
        "        '--learning_rate', '2e-5',\n",
        "        '--max_grad_norm', '1.0',\n",
        "        '--evaluate_during_training',\n",
        "        '--seed', '123456'\n",
        "    ]\n",
        "\n",
        "    # Parse the simulated arguments\n",
        "    args = parser.parse_args(simulated_args)\n",
        "\n",
        "    main(args)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T16:32:48.096626Z",
          "iopub.execute_input": "2024-07-01T16:32:48.097047Z",
          "iopub.status.idle": "2024-07-01T17:26:16.71764Z",
          "shell.execute_reply.started": "2024-07-01T16:32:48.097013Z",
          "shell.execute_reply": "2024-07-01T17:26:16.716594Z"
        },
        "trusted": true,
        "id": "ydcOLX4sW1xo",
        "outputId": "fc9ed24e-40a6-4979-dd50-370203276077"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-07-01 16:32:51.271098: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-01 16:32:51.271167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-01 16:32:51.273465: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training/evaluation parameters %s Namespace(train_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl', output_dir='./saved_models', eval_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl', test_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl', model_type='roberta', model_name_or_path='huggingface/CodeBERTa-small-v1', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='huggingface/CodeBERTa-small-v1', cache_dir='', block_size=510, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=16, eval_batch_size=16, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, start_epoch=0, start_step=0)\n***** Running training *****\n  Num examples = %d 21854\n  Num Epochs = %d 5\n  Instantaneous batch size per GPU = %d 16\n  Total train batch size (w. parallel, distributed & accumulation) = %d 16\n  Gradient Accumulation steps = %d 1\n  Total optimization steps = %d 6830\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.68565 acc 0.54686:  77%|███████▋  | 1055/1366 [07:33<02:14,  2.32it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.68139 acc 0.55287: 100%|██████████| 1366/1366 [10:21<00:00,  2.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6476\n  %s = %s eval_acc 0.5999\n  %s = %s f1 0.3985\n  %s = %s precision 0.5746\n  %s = %s recall 0.305\n  %s = %s G 0.4455\n  %s = %s MCC 0.1548\n  ********************\n  Best acc:%s 0.5999\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.61691 acc 0.64461:  77%|███████▋  | 1054/1366 [07:34<02:14,  2.33it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.61347 acc 0.64652: 100%|██████████| 1366/1366 [10:23<00:00,  2.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6156\n  %s = %s eval_acc 0.6402\n  %s = %s f1 0.6113\n  %s = %s precision 0.576\n  %s = %s recall 0.6512\n  %s = %s G 0.6413\n  %s = %s MCC 0.2805\n  ********************\n  Best acc:%s 0.6402\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 2 loss 0.5201 acc 0.72826:  77%|███████▋  | 1054/1366 [07:35<02:14,  2.32it/s] ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 2 loss 0.52017 acc 0.72746: 100%|██████████| 1366/1366 [10:23<00:00,  2.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6894\n  %s = %s eval_acc 0.6219\n  %s = %s f1 0.622\n  %s = %s precision 0.5498\n  %s = %s recall 0.7161\n  %s = %s G 0.6218\n  %s = %s MCC 0.2656\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 3 loss 0.4249 acc 0.79246:  77%|███████▋  | 1054/1366 [07:34<02:14,  2.33it/s] ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 3 loss 0.42627 acc 0.79052: 100%|██████████| 1366/1366 [10:23<00:00,  2.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.7437\n  %s = %s eval_acc 0.6369\n  %s = %s f1 0.5904\n  %s = %s precision 0.5789\n  %s = %s recall 0.6024\n  %s = %s G 0.6314\n  %s = %s MCC 0.2647\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 4 loss 0.34606 acc 0.84224:  77%|███████▋  | 1054/1366 [07:34<02:13,  2.33it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 4 loss 0.34736 acc 0.84106: 100%|██████████| 1366/1366 [10:22<00:00,  2.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.8704\n  %s = %s eval_acc 0.6303\n  %s = %s f1 0.5709\n  %s = %s precision 0.5758\n  %s = %s recall 0.5661\n  %s = %s G 0.6177\n  %s = %s MCC 0.2463\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Using RoBERTa"
      ],
      "metadata": {
        "id": "_W-OsC1_W1xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%python\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import math\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "import json\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import multiprocessing\n",
        "cpu_cont = multiprocessing.cpu_count()\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "MODEL_CLASSES = {\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, encoder,config,tokenizer,args):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config=config\n",
        "        self.tokenizer=tokenizer\n",
        "        self.args=args\n",
        "\n",
        "        # Define dropout layer, dropout_probability is taken from args.\n",
        "        self.dropout = nn.Dropout(args.dropout_probability)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None,labels=None):\n",
        "        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\n",
        "\n",
        "        # Apply dropout\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        logits=outputs\n",
        "        prob=torch.sigmoid(logits)\n",
        "        if labels is not None:\n",
        "            labels=labels.float()\n",
        "            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\n",
        "            loss=-loss.mean()\n",
        "            return loss,prob\n",
        "        else:\n",
        "            return prob\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 input_tokens,\n",
        "                 input_ids,\n",
        "                 idx,\n",
        "                 label,\n",
        "\n",
        "    ):\n",
        "        self.input_tokens = input_tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.idx=str(idx)\n",
        "        self.label=label\n",
        "def convert_examples_to_features(js,tokenizer,args):\n",
        "    #source\n",
        "    code=' '.join(js['func'].split())\n",
        "    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\n",
        "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
        "    padding_length = args.block_size - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    return InputFeatures(source_tokens,source_ids,js['idx'],js['target'])\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, file_path=None):\n",
        "        self.examples = []\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                js=json.loads(line.strip())\n",
        "                self.examples.append(convert_examples_to_features(js,tokenizer,args))\n",
        "        if 'train' in file_path:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                    logger.info(\"*** Example ***\")\n",
        "                    logger.info(\"idx: {}\".format(idx))\n",
        "                    logger.info(\"label: {}\".format(example.label))\n",
        "                    logger.info(\"input_tokens: {}\".format([x.replace('\\u0120','_') for x in example.input_tokens]))\n",
        "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
        "                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\n",
        "    args.max_steps=args.epoch*len( train_dataloader)\n",
        "    args.save_steps=len( train_dataloader)\n",
        "    args.warmup_steps=len( train_dataloader)\n",
        "    args.logging_steps=len( train_dataloader)\n",
        "    args.num_train_epochs=args.epoch\n",
        "    model.to(args.device)\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\n",
        "                                                num_training_steps=args.max_steps)\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
        "    optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
        "    if os.path.exists(scheduler_last):\n",
        "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
        "    if os.path.exists(optimizer_last):\n",
        "        optimizer.load_state_dict(torch.load(optimizer_last))\n",
        "    # Train!\n",
        "    print(\"***** Running training *****\")\n",
        "    print(\"  Num examples = %d\", len(train_dataset))\n",
        "    print(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
        "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    print(\"  Total optimization steps = %d\", args.max_steps)\n",
        "\n",
        "    global_step = args.start_step\n",
        "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
        "    best_mrr=0.0\n",
        "    best_acc=0.0\n",
        "    # model.resize_token_embeddings(len(tokenizer))\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Initialize early stopping parameters at the start of training\n",
        "    early_stopping_counter = 0\n",
        "    best_loss = None\n",
        "\n",
        "    for idx in range(args.start_epoch, int(args.num_train_epochs)):\n",
        "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "        tr_num=0\n",
        "        train_loss=0\n",
        "        acc = 0\n",
        "        for step, batch in enumerate(bar):\n",
        "            inputs = batch[0].to(args.device)\n",
        "            labels=batch[1].to(args.device)\n",
        "            model.train()\n",
        "            loss,logits = model(inputs,labels)\n",
        "\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            preds=logits[:,0]>0.5\n",
        "            acc+=np.mean(labels==preds)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            tr_num+=1\n",
        "            train_loss+=loss.item()\n",
        "            if avg_loss==0:\n",
        "                avg_loss=tr_loss\n",
        "            avg_loss=round(train_loss/tr_num,5)\n",
        "            avg_acc = round(acc/tr_num,5)\n",
        "            bar.set_description(\"epoch {} loss {} acc {}\".format(idx,avg_loss,avg_acc))\n",
        "\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "                output_flag=True\n",
        "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    logging_loss = tr_loss\n",
        "                    tr_nb=global_step\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer,eval_when_training=True)\n",
        "                        for key, value in results.items():\n",
        "                            print(\"  %s = %s\", key, round(value,4))\n",
        "                        # Save model checkpoint\n",
        "\n",
        "                    if results['eval_acc']>best_acc:\n",
        "                        best_acc=results['eval_acc']\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "                        print(\"  Best acc:%s\",round(best_acc,4))\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "\n",
        "                        checkpoint_prefix = 'checkpoint-best-acc'\n",
        "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "                        if not os.path.exists(output_dir):\n",
        "                            os.makedirs(output_dir)\n",
        "                        model_to_save = model.module if hasattr(model,'module') else model\n",
        "                        output_dir = os.path.join(output_dir, '{}'.format('model.bin'))\n",
        "                        torch.save(model_to_save.state_dict(), output_dir)\n",
        "                        print(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = train_loss / tr_num\n",
        "\n",
        "        # Check for early stopping condition\n",
        "        if args.early_stopping_patience is not None:\n",
        "            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\n",
        "                best_loss = avg_loss\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= args.early_stopping_patience:\n",
        "                    print(\"Early stopping\")\n",
        "                    break  # Exit the loop early\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer,eval_when_training=False):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1 and eval_when_training is False:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running evaluation *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in eval_dataloader:\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            lm_loss,logit = model(inputs,label)\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "        nb_eval_steps += 1\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    eval_acc=np.mean(labels==preds)\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.tensor(eval_loss)\n",
        "\n",
        "\n",
        "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
        "    cm = confusion_matrix(y_true=labels, y_pred=preds)\n",
        "    tp = cm[1][1]\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    precision = 0 if tp + fp == 0 else tp/(tp+fp)\n",
        "    recall = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    fprate = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    pdVal = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    pfVal = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    G = 0 if pdVal+1-pfVal == 0 else 2*pdVal*(1-pfVal)/(pdVal+1-pfVal)\n",
        "    F1 = 0 if precision+recall == 0 else 2 * precision * recall / (precision+recall)\n",
        "    MCC = 0 if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) == 0 else (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
        "    result = {\n",
        "        \"eval_loss\": float(perplexity),\n",
        "        \"eval_acc\":round(eval_acc,4),\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"G\": G,\n",
        "        \"MCC\": MCC\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def test(args, model, tokenizer):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\n",
        "\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running Test *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            logit = model(inputs)\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    with open(os.path.join(args.output_dir,\"predictions.txt\"),'w') as f:\n",
        "        for example,pred in zip(eval_dataset.examples,preds):\n",
        "            if pred:\n",
        "                f.write(example.idx+'\\t1\\n')\n",
        "            else:\n",
        "                f.write(example.idx+'\\t0\\n')\n",
        "\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu\n",
        "    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu\n",
        "    # Setup logging\n",
        "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    args.start_epoch = 0\n",
        "    args.start_step = 0\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
        "        args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
        "        args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
        "        idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
        "        with open(idx_file, encoding='utf-8') as idxf:\n",
        "            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
        "\n",
        "        step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
        "        if os.path.exists(step_file):\n",
        "            with open(step_file, encoding='utf-8') as stepf:\n",
        "                args.start_step = int(stepf.readlines()[0].strip())\n",
        "\n",
        "        print(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))\n",
        "\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
        "                                          cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    config.num_labels=1\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\n",
        "                                                do_lower_case=args.do_lower_case,\n",
        "                                                cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    if args.block_size <= 0:\n",
        "        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "    if args.model_name_or_path:\n",
        "        model = model_class.from_pretrained(args.model_name_or_path,\n",
        "                                            from_tf=bool('.ckpt' in args.model_name_or_path),\n",
        "                                            config=config,\n",
        "                                            cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    else:\n",
        "        model = model_class(config)\n",
        "\n",
        "    model=Model(model,config,tokenizer,args)\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    print(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        if args.local_rank not in [-1, 0]:\n",
        "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\n",
        "        if args.local_rank == 0:\n",
        "            torch.distributed.barrier()\n",
        "\n",
        "        train(args, train_dataset, model, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            result=evaluate(args, model, tokenizer)\n",
        "            print(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                print(\"  %s = %s\", key, str(round(result[key],4)))\n",
        "\n",
        "    if args.do_test and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            test(args, model, tokenizer)\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
        "                        help=\"The input training data file (a text file).\")\n",
        "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "    parser.add_argument(\"--test_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "\n",
        "    parser.add_argument(\"--model_type\", default=\"bert\", type=str,\n",
        "                        help=\"The model architecture to be fine-tuned.\")\n",
        "    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\n",
        "                        help=\"The model checkpoint for weights initialization.\")\n",
        "\n",
        "    parser.add_argument(\"--mlm\", action='store_true',\n",
        "                        help=\"Train with masked-language modeling loss instead of language modeling.\")\n",
        "    parser.add_argument(\"--mlm_probability\", type=float, default=0.15,\n",
        "                        help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
        "\n",
        "    parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
        "                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\n",
        "    parser.add_argument(\"--block_size\", default=-1, type=int,\n",
        "                        help=\"Optional input sequence length after tokenization.\"\n",
        "                             \"The training dataset will be truncated in block of this size for training.\"\n",
        "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
        "    parser.add_argument(\"--do_train\", action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--do_test\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
        "                        help=\"Run evaluation during training at each logging step.\")\n",
        "    parser.add_argument(\"--do_lower_case\", action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "\n",
        "    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
        "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight deay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                        help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", default=1.0, type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
        "                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument('--logging_steps', type=int, default=50,\n",
        "                        help=\"Log every X updates steps.\")\n",
        "    parser.add_argument('--save_steps', type=int, default=50,\n",
        "                        help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument('--save_total_limit', type=int, default=None,\n",
        "                        help='Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default')\n",
        "    parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n",
        "                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\n",
        "    parser.add_argument(\"--no_cuda\", action='store_true',\n",
        "                        help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument('--overwrite_output_dir', action='store_true',\n",
        "                        help=\"Overwrite the content of the output directory\")\n",
        "    parser.add_argument('--overwrite_cache', action='store_true',\n",
        "                        help=\"Overwrite the cached training and evaluation sets\")\n",
        "    parser.add_argument('--seed', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--epoch', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--fp16', action='store_true',\n",
        "                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
        "    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
        "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                        help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument('--server_ip', type=str, default='', help=\"For distant debugging.\")\n",
        "    parser.add_argument('--server_port', type=str, default='', help=\"For distant debugging.\")\n",
        "\n",
        "    # Add early stopping parameters and dropout probability parameters\n",
        "    parser.add_argument(\"--early_stopping_patience\", type=int, default=None,\n",
        "                        help=\"Number of epochs with no improvement after which training will be stopped.\")\n",
        "    parser.add_argument(\"--min_loss_delta\", type=float, default=0.001,\n",
        "                        help=\"Minimum change in the loss required to qualify as an improvement.\")\n",
        "    parser.add_argument('--dropout_probability', type=float, default=0, help='dropout probability')\n",
        "\n",
        "    # Simulate command-line arguments\n",
        "    simulated_args = [\n",
        "        '--output_dir', './saved_models',\n",
        "        '--model_type', 'roberta',\n",
        "        '--tokenizer_name', 'roberta-base',\n",
        "        '--model_name_or_path', 'roberta-base',\n",
        "        '--do_train',\n",
        "        '--train_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl',\n",
        "        '--eval_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl',\n",
        "        '--test_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl',\n",
        "        '--epoch', '5',\n",
        "        '--block_size', '512',\n",
        "        '--train_batch_size', '16',\n",
        "        '--eval_batch_size', '16',\n",
        "        '--learning_rate', '2e-5',\n",
        "        '--max_grad_norm', '1.0',\n",
        "        '--evaluate_during_training',\n",
        "        '--seed', '123456'\n",
        "    ]\n",
        "\n",
        "    # Parse the simulated arguments\n",
        "    args = parser.parse_args(simulated_args)\n",
        "\n",
        "    main(args)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T13:24:28.934755Z",
          "iopub.execute_input": "2024-07-02T13:24:28.935093Z",
          "iopub.status.idle": "2024-07-02T15:05:26.949876Z",
          "shell.execute_reply.started": "2024-07-02T13:24:28.935067Z",
          "shell.execute_reply": "2024-07-02T15:05:26.948939Z"
        },
        "trusted": true,
        "id": "zL3yLTlIW1xq",
        "outputId": "682f17d3-f804-40a8-c7ea-276e182207cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-07-02 13:24:32.084367: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-02 13:24:32.084431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-02 13:24:32.086655: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training/evaluation parameters %s Namespace(train_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl', output_dir='./saved_models', eval_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl', test_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl', model_type='roberta', model_name_or_path='roberta-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='roberta-base', cache_dir='', block_size=510, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=16, eval_batch_size=16, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, start_epoch=0, start_step=0)\n***** Running training *****\n  Num examples = %d 21854\n  Num Epochs = %d 5\n  Instantaneous batch size per GPU = %d 16\n  Total train batch size (w. parallel, distributed & accumulation) = %d 16\n  Gradient Accumulation steps = %d 1\n  Total optimization steps = %d 6830\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.69146 acc 0.53797:  77%|███████▋  | 1054/1366 [14:36<04:19,  1.20it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.69112 acc 0.53936: 100%|██████████| 1366/1366 [19:52<00:00,  1.15it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6845\n  %s = %s eval_acc 0.5655\n  %s = %s f1 0.0\n  %s = %s precision 0\n  %s = %s recall 0.0\n  %s = %s G 0.0\n  %s = %s MCC 0\n  ********************\n  Best acc:%s 0.5655\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.69043 acc 0.54443:  77%|███████▋  | 1054/1366 [14:37<04:19,  1.20it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.69072 acc 0.54203: 100%|██████████| 1366/1366 [19:52<00:00,  1.15it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6869\n  %s = %s eval_acc 0.5655\n  %s = %s f1 0.0\n  %s = %s precision 0\n  %s = %s recall 0.0\n  %s = %s G 0.0\n  %s = %s MCC 0\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 2 loss 0.69068 acc 0.53963:  77%|███████▋  | 1054/1366 [14:37<04:20,  1.20it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 2 loss 0.69077 acc 0.53913: 100%|██████████| 1366/1366 [19:53<00:00,  1.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6859\n  %s = %s eval_acc 0.5655\n  %s = %s f1 0.0\n  %s = %s precision 0\n  %s = %s recall 0.0\n  %s = %s G 0.0\n  %s = %s MCC 0\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 3 loss 0.69055 acc 0.54165:  77%|███████▋  | 1054/1366 [14:37<04:20,  1.20it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 3 loss 0.69036 acc 0.54076: 100%|██████████| 1366/1366 [19:53<00:00,  1.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6843\n  %s = %s eval_acc 0.5655\n  %s = %s f1 0.0\n  %s = %s precision 0\n  %s = %s recall 0.0\n  %s = %s G 0.0\n  %s = %s MCC 0\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 4 loss 0.68697 acc 0.5404:  77%|███████▋  | 1054/1366 [14:37<04:18,  1.20it/s] ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 4 loss 0.68595 acc 0.54283: 100%|██████████| 1366/1366 [19:53<00:00,  1.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6766\n  %s = %s eval_acc 0.563\n  %s = %s f1 0.2405\n  %s = %s precision 0.4909\n  %s = %s recall 0.1592\n  %s = %s G 0.2693\n  %s = %s MCC 0.0461\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model For PROMIST Dataset"
      ],
      "metadata": {
        "id": "d8R8kQQcW1xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%python\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import math\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "import json\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import multiprocessing\n",
        "cpu_cont = multiprocessing.cpu_count()\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "MODEL_CLASSES = {\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, encoder,config,tokenizer,args):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config=config\n",
        "        self.tokenizer=tokenizer\n",
        "        self.args=args\n",
        "\n",
        "        # Define dropout layer, dropout_probability is taken from args.\n",
        "        self.dropout = nn.Dropout(args.dropout_probability)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None,labels=None):\n",
        "        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\n",
        "\n",
        "        # Apply dropout\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        logits=outputs\n",
        "        prob=torch.sigmoid(logits)\n",
        "        if labels is not None:\n",
        "            labels=labels.float()\n",
        "            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\n",
        "            loss=-loss.mean()\n",
        "            return loss,prob\n",
        "        else:\n",
        "            return prob\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 input_tokens,\n",
        "                 input_ids,\n",
        "                 idx,\n",
        "                 label,\n",
        "\n",
        "    ):\n",
        "        self.input_tokens = input_tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.idx=str(idx)\n",
        "        self.label=label\n",
        "def convert_examples_to_features(js,tokenizer,args):\n",
        "    #source\n",
        "    code=' '.join(js['func'].split())\n",
        "    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\n",
        "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
        "    padding_length = args.block_size - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    return InputFeatures(source_tokens,source_ids,js['idx'],js['target'])\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, file_path=None):\n",
        "        self.examples = []\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                js=json.loads(line.strip())\n",
        "                self.examples.append(convert_examples_to_features(js,tokenizer,args))\n",
        "        if 'train' in file_path:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                    logger.info(\"*** Example ***\")\n",
        "                    logger.info(\"idx: {}\".format(idx))\n",
        "                    logger.info(\"label: {}\".format(example.label))\n",
        "                    logger.info(\"input_tokens: {}\".format([x.replace('\\u0120','_') for x in example.input_tokens]))\n",
        "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
        "                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\n",
        "    args.max_steps=args.epoch*len( train_dataloader)\n",
        "    args.save_steps=len( train_dataloader)\n",
        "    args.warmup_steps=len( train_dataloader)\n",
        "    args.logging_steps=len( train_dataloader)\n",
        "    args.num_train_epochs=args.epoch\n",
        "    model.to(args.device)\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\n",
        "                                                num_training_steps=args.max_steps)\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
        "    optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
        "    if os.path.exists(scheduler_last):\n",
        "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
        "    if os.path.exists(optimizer_last):\n",
        "        optimizer.load_state_dict(torch.load(optimizer_last))\n",
        "    # Train!\n",
        "    print(\"***** Running training *****\")\n",
        "    print(\"  Num examples = %d\", len(train_dataset))\n",
        "    print(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
        "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    print(\"  Total optimization steps = %d\", args.max_steps)\n",
        "\n",
        "    global_step = args.start_step\n",
        "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
        "    best_mrr=0.0\n",
        "    best_acc=0.0\n",
        "    # model.resize_token_embeddings(len(tokenizer))\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Initialize early stopping parameters at the start of training\n",
        "    early_stopping_counter = 0\n",
        "    best_loss = None\n",
        "\n",
        "    for idx in range(args.start_epoch, int(args.num_train_epochs)):\n",
        "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "        tr_num=0\n",
        "        train_loss=0\n",
        "        acc = 0\n",
        "        for step, batch in enumerate(bar):\n",
        "            inputs = batch[0].to(args.device)\n",
        "            labels=batch[1].to(args.device)\n",
        "            model.train()\n",
        "            loss,logits = model(inputs,labels)\n",
        "\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            preds=logits[:,0]>0.5\n",
        "            acc+=np.mean(labels==preds)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            tr_num+=1\n",
        "            train_loss+=loss.item()\n",
        "            if avg_loss==0:\n",
        "                avg_loss=tr_loss\n",
        "            avg_loss=round(train_loss/tr_num,5)\n",
        "            avg_acc = round(acc/tr_num,5)\n",
        "            bar.set_description(\"epoch {} loss {} acc {}\".format(idx,avg_loss,avg_acc))\n",
        "\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "                output_flag=True\n",
        "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    logging_loss = tr_loss\n",
        "                    tr_nb=global_step\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer,eval_when_training=True)\n",
        "                        for key, value in results.items():\n",
        "                            print(\"  %s = %s\", key, round(value,4))\n",
        "                        # Save model checkpoint\n",
        "\n",
        "                    if results['eval_acc']>best_acc:\n",
        "                        best_acc=results['eval_acc']\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "                        print(\"  Best acc:%s\",round(best_acc,4))\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "\n",
        "                        checkpoint_prefix = 'checkpoint-best-acc'\n",
        "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "                        if not os.path.exists(output_dir):\n",
        "                            os.makedirs(output_dir)\n",
        "                        model_to_save = model.module if hasattr(model,'module') else model\n",
        "                        output_dir = os.path.join(output_dir, '{}'.format('model.bin'))\n",
        "                        torch.save(model_to_save.state_dict(), output_dir)\n",
        "                        print(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = train_loss / tr_num\n",
        "\n",
        "        # Check for early stopping condition\n",
        "        if args.early_stopping_patience is not None:\n",
        "            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\n",
        "                best_loss = avg_loss\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= args.early_stopping_patience:\n",
        "                    print(\"Early stopping\")\n",
        "                    break  # Exit the loop early\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer,eval_when_training=False):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1 and eval_when_training is False:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running evaluation *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in eval_dataloader:\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            lm_loss,logit = model(inputs,label)\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "        nb_eval_steps += 1\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    eval_acc=np.mean(labels==preds)\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.tensor(eval_loss)\n",
        "\n",
        "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
        "    cm = confusion_matrix(y_true=labels, y_pred=preds)\n",
        "    tp = cm[1][1]\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    precision = 0 if tp + fp == 0 else tp/(tp+fp)\n",
        "    recall = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    fprate = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    pdVal = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    pfVal = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    G = 0 if pdVal+1-pfVal == 0 else 2*pdVal*(1-pfVal)/(pdVal+1-pfVal)\n",
        "    F1 = 0 if precision+recall == 0 else 2 * precision * recall / (precision+recall)\n",
        "    MCC = 0 if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) == 0 else (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
        "    result = {\n",
        "        \"eval_loss\": float(perplexity),\n",
        "        \"eval_acc\":round(eval_acc,4),\n",
        "        \"tp\": tp,\n",
        "        \"tn\": tn,\n",
        "        \"fp\": fp,\n",
        "        \"fn\": fn,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"G\": G,\n",
        "        \"MCC\": MCC\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def test(args, model, tokenizer):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\n",
        "\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running Test *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            logit = model(inputs)\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    with open(os.path.join(args.output_dir,\"predictions.txt\"),'w') as f:\n",
        "        for example,pred in zip(eval_dataset.examples,preds):\n",
        "            if pred:\n",
        "                f.write(example.idx+'\\t1\\n')\n",
        "            else:\n",
        "                f.write(example.idx+'\\t0\\n')\n",
        "\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu\n",
        "    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu\n",
        "    # Setup logging\n",
        "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    args.start_epoch = 0\n",
        "    args.start_step = 0\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
        "        args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
        "        args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
        "        idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
        "        with open(idx_file, encoding='utf-8') as idxf:\n",
        "            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
        "\n",
        "        step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
        "        if os.path.exists(step_file):\n",
        "            with open(step_file, encoding='utf-8') as stepf:\n",
        "                args.start_step = int(stepf.readlines()[0].strip())\n",
        "\n",
        "        print(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))\n",
        "\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
        "                                          cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    config.num_labels=1\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\n",
        "                                                do_lower_case=args.do_lower_case,\n",
        "                                                cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    if args.block_size <= 0:\n",
        "        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "    if args.model_name_or_path:\n",
        "        model = model_class.from_pretrained(args.model_name_or_path,\n",
        "                                            from_tf=bool('.ckpt' in args.model_name_or_path),\n",
        "                                            config=config,\n",
        "                                            cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    else:\n",
        "        model = model_class(config)\n",
        "\n",
        "    model=Model(model,config,tokenizer,args)\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    print(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        if args.local_rank not in [-1, 0]:\n",
        "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\n",
        "        if args.local_rank == 0:\n",
        "            torch.distributed.barrier()\n",
        "\n",
        "        train(args, train_dataset, model, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            result=evaluate(args, model, tokenizer)\n",
        "            print(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                print(\"  %s = %s\", key, str(round(result[key],4)))\n",
        "\n",
        "    if args.do_test and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            test(args, model, tokenizer)\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
        "                        help=\"The input training data file (a text file).\")\n",
        "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "    parser.add_argument(\"--test_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "\n",
        "    parser.add_argument(\"--model_type\", default=\"bert\", type=str,\n",
        "                        help=\"The model architecture to be fine-tuned.\")\n",
        "    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\n",
        "                        help=\"The model checkpoint for weights initialization.\")\n",
        "\n",
        "    parser.add_argument(\"--mlm\", action='store_true',\n",
        "                        help=\"Train with masked-language modeling loss instead of language modeling.\")\n",
        "    parser.add_argument(\"--mlm_probability\", type=float, default=0.15,\n",
        "                        help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
        "\n",
        "    parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
        "                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\n",
        "    parser.add_argument(\"--block_size\", default=-1, type=int,\n",
        "                        help=\"Optional input sequence length after tokenization.\"\n",
        "                             \"The training dataset will be truncated in block of this size for training.\"\n",
        "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
        "    parser.add_argument(\"--do_train\", action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--do_test\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
        "                        help=\"Run evaluation during training at each logging step.\")\n",
        "    parser.add_argument(\"--do_lower_case\", action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "\n",
        "    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
        "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight deay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                        help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", default=1.0, type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
        "                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument('--logging_steps', type=int, default=50,\n",
        "                        help=\"Log every X updates steps.\")\n",
        "    parser.add_argument('--save_steps', type=int, default=50,\n",
        "                        help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument('--save_total_limit', type=int, default=None,\n",
        "                        help='Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default')\n",
        "    parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n",
        "                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\n",
        "    parser.add_argument(\"--no_cuda\", action='store_true',\n",
        "                        help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument('--overwrite_output_dir', action='store_true',\n",
        "                        help=\"Overwrite the content of the output directory\")\n",
        "    parser.add_argument('--overwrite_cache', action='store_true',\n",
        "                        help=\"Overwrite the cached training and evaluation sets\")\n",
        "    parser.add_argument('--seed', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--epoch', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--fp16', action='store_true',\n",
        "                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
        "    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
        "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                        help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument('--server_ip', type=str, default='', help=\"For distant debugging.\")\n",
        "    parser.add_argument('--server_port', type=str, default='', help=\"For distant debugging.\")\n",
        "\n",
        "    # Add early stopping parameters and dropout probability parameters\n",
        "    parser.add_argument(\"--early_stopping_patience\", type=int, default=None,\n",
        "                        help=\"Number of epochs with no improvement after which training will be stopped.\")\n",
        "    parser.add_argument(\"--min_loss_delta\", type=float, default=0.001,\n",
        "                        help=\"Minimum change in the loss required to qualify as an improvement.\")\n",
        "    parser.add_argument('--dropout_probability', type=float, default=0, help='dropout probability')\n",
        "\n",
        "    # Simulate command-line arguments\n",
        "    simulated_args = [\n",
        "        '--output_dir', './saved_models',\n",
        "        '--model_type', 'roberta',\n",
        "        '--tokenizer_name', 'FacebookAI/roberta-base',\n",
        "        '--model_name_or_path', 'FacebookAI/roberta-base',\n",
        "        '--do_train',\n",
        "        '--train_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl',\n",
        "        '--eval_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl',\n",
        "        '--test_data_file', '/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl',\n",
        "        '--epoch', '5',\n",
        "        '--block_size', '500',\n",
        "        '--train_batch_size', '16',\n",
        "        '--eval_batch_size', '16',\n",
        "        '--learning_rate', '2e-5',\n",
        "        '--max_grad_norm', '1.0',\n",
        "        '--evaluate_during_training',\n",
        "        '--seed', '123456'\n",
        "    ]\n",
        "\n",
        "    # Parse the simulated arguments\n",
        "    args = parser.parse_args(simulated_args)\n",
        "\n",
        "    main(args)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T12:42:09.755229Z",
          "iopub.execute_input": "2024-07-02T12:42:09.755612Z",
          "iopub.status.idle": "2024-07-02T13:24:11.755251Z",
          "shell.execute_reply.started": "2024-07-02T12:42:09.755581Z",
          "shell.execute_reply": "2024-07-02T13:24:11.754135Z"
        },
        "trusted": true,
        "id": "p4q4Gq8RW1xs",
        "outputId": "492a9e03-5460-4bb6-ea4c-64fdf72950fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-07-02 12:42:12.979875: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-02 12:42:12.979942: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-02 12:42:12.981931: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training/evaluation parameters %s Namespace(train_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/train.jsonl', output_dir='./saved_models', eval_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/valid.jsonl', test_data_file='/kaggle/working/CodeXGLUE/Code-Code/Defect-detection/dataset/test.jsonl', model_type='roberta', model_name_or_path='FacebookAI/roberta-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='FacebookAI/roberta-base', cache_dir='', block_size=500, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=16, eval_batch_size=16, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, start_epoch=0, start_step=0)\n***** Running training *****\n  Num examples = %d 21854\n  Num Epochs = %d 5\n  Instantaneous batch size per GPU = %d 16\n  Total train batch size (w. parallel, distributed & accumulation) = %d 16\n  Gradient Accumulation steps = %d 1\n  Total optimization steps = %d 6830\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.6913 acc 0.53395:  77%|███████▋  | 1054/1366 [14:25<04:15,  1.22it/s] ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.69093 acc 0.53616: 100%|██████████| 1366/1366 [19:37<00:00,  1.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6845\n  %s = %s eval_acc 0.5655\n  %s = %s tp 0\n  %s = %s tn 1545\n  %s = %s fp 0\n  %s = %s fn 1187\n  %s = %s f1 0.0\n  %s = %s precision 0\n  %s = %s recall 0.0\n  %s = %s G 0.0\n  %s = %s MCC 0\n  ********************\n  Best acc:%s 0.5655\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.69063 acc 0.54115:  77%|███████▋  | 1054/1366 [14:24<04:15,  1.22it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "***** Running evaluation *****\n  Num examples = %d 2732\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.69091 acc 0.53819: 100%|██████████| 1366/1366 [19:36<00:00,  1.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6884\n  %s = %s eval_acc 0.5655\n  %s = %s tp 0\n  %s = %s tn 1545\n  %s = %s fp 0\n  %s = %s fn 1187\n  %s = %s f1 0.0\n  %s = %s precision 0\n  %s = %s recall 0.0\n  %s = %s G 0.0\n  %s = %s MCC 0\nError while terminating subprocess (pid=1341): \n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 2 loss 0.69113 acc 0.5332:   7%|▋         | 96/1366 [01:19<17:33,  1.21it/s] \nTraceback (most recent call last):\n  File \"<stdin>\", line 627, in <module>\n  File \"<stdin>\", line 477, in main\n  File \"<stdin>\", line 208, in train\nKeyboardInterrupt\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%python\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import math\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "import json\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import multiprocessing\n",
        "cpu_cont = multiprocessing.cpu_count()\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "MODEL_CLASSES = {\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, encoder,config,tokenizer,args):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config=config\n",
        "        self.tokenizer=tokenizer\n",
        "        self.args=args\n",
        "\n",
        "        # Define dropout layer, dropout_probability is taken from args.\n",
        "        self.dropout = nn.Dropout(args.dropout_probability)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None,labels=None):\n",
        "        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\n",
        "\n",
        "        # Apply dropout\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        logits=outputs\n",
        "        prob=torch.sigmoid(logits)\n",
        "        if labels is not None:\n",
        "            labels=labels.float()\n",
        "            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\n",
        "            loss=-loss.mean()\n",
        "            return loss,prob\n",
        "        else:\n",
        "            return prob\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 input_tokens,\n",
        "                 input_ids,\n",
        "                 idx,\n",
        "                 label,\n",
        "\n",
        "    ):\n",
        "        self.input_tokens = input_tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.idx=str(idx)\n",
        "        self.label=int(label)\n",
        "def convert_examples_to_features(js,tokenizer,args):\n",
        "    #source\n",
        "    code=' '.join(js['func'].split())\n",
        "    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\n",
        "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
        "    padding_length = args.block_size - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    return InputFeatures(source_tokens,source_ids,js['idx'],js['target'])\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, file_path=None):\n",
        "        self.examples = []\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                js=json.loads(line.strip())\n",
        "                self.examples.append(convert_examples_to_features(js,tokenizer,args))\n",
        "        if 'train' in file_path:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                    logger.info(\"*** Example ***\")\n",
        "                    logger.info(\"idx: {}\".format(idx))\n",
        "                    logger.info(\"label: {}\".format(example.label))\n",
        "                    logger.info(\"input_tokens: {}\".format([x.replace('\\u0120','_') for x in example.input_tokens]))\n",
        "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
        "                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\n",
        "    args.max_steps=args.epoch*len( train_dataloader)\n",
        "    args.save_steps=len( train_dataloader)\n",
        "    args.warmup_steps=len( train_dataloader)\n",
        "    args.logging_steps=len( train_dataloader)\n",
        "    args.num_train_epochs=args.epoch\n",
        "    model.to(args.device)\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\n",
        "                                                num_training_steps=args.max_steps)\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
        "    optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
        "    if os.path.exists(scheduler_last):\n",
        "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
        "    if os.path.exists(optimizer_last):\n",
        "        optimizer.load_state_dict(torch.load(optimizer_last))\n",
        "    # Train!\n",
        "    print(\"***** Running training *****\")\n",
        "    print(\"  Num examples = %d\", len(train_dataset))\n",
        "    print(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
        "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    print(\"  Total optimization steps = %d\", args.max_steps)\n",
        "\n",
        "    global_step = args.start_step\n",
        "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
        "    best_mrr=0.0\n",
        "    best_acc=0.0\n",
        "    # model.resize_token_embeddings(len(tokenizer))\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Initialize early stopping parameters at the start of training\n",
        "    early_stopping_counter = 0\n",
        "    best_loss = None\n",
        "\n",
        "    for idx in range(args.start_epoch, int(args.num_train_epochs)):\n",
        "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "        tr_num=0\n",
        "        train_loss=0\n",
        "        acc = 0\n",
        "        for step, batch in enumerate(bar):\n",
        "            inputs = batch[0].to(args.device)\n",
        "            labels=batch[1].to(args.device)\n",
        "            model.train()\n",
        "            loss,logits = model(inputs,labels)\n",
        "\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            preds=logits[:,0]>0.5\n",
        "            acc+=np.mean(labels==preds)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            tr_num+=1\n",
        "            train_loss+=loss.item()\n",
        "            if avg_loss==0:\n",
        "                avg_loss=tr_loss\n",
        "            avg_loss=round(train_loss/tr_num,5)\n",
        "            avg_acc = round(acc/tr_num,5)\n",
        "            bar.set_description(\"epoch {} loss {} acc {}\".format(idx,avg_loss,avg_acc))\n",
        "\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "                output_flag=True\n",
        "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    logging_loss = tr_loss\n",
        "                    tr_nb=global_step\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer,eval_when_training=True)\n",
        "                        for key, value in results.items():\n",
        "                            print(\"  %s = %s\", key, round(value,4))\n",
        "                        # Save model checkpoint\n",
        "\n",
        "                    if results['eval_acc']>best_acc:\n",
        "                        best_acc=results['eval_acc']\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "                        print(\"  Best acc:%s\",round(best_acc,4))\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "\n",
        "                        checkpoint_prefix = 'checkpoint-best-acc'\n",
        "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "                        if not os.path.exists(output_dir):\n",
        "                            os.makedirs(output_dir)\n",
        "                        model_to_save = model.module if hasattr(model,'module') else model\n",
        "                        output_dir = os.path.join(output_dir, '{}'.format('model.bin'))\n",
        "                        torch.save(model_to_save.state_dict(), output_dir)\n",
        "                        print(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = train_loss / tr_num\n",
        "\n",
        "        # Check for early stopping condition\n",
        "        if args.early_stopping_patience is not None:\n",
        "            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\n",
        "                best_loss = avg_loss\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= args.early_stopping_patience:\n",
        "                    print(\"Early stopping\")\n",
        "                    break  # Exit the loop early\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer,eval_when_training=False):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1 and eval_when_training is False:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running evaluation *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in eval_dataloader:\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            lm_loss,logit = model(inputs,label)\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "        nb_eval_steps += 1\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    eval_acc=np.mean(labels==preds)\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.tensor(eval_loss)\n",
        "\n",
        "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
        "    cm = confusion_matrix(y_true=labels, y_pred=preds)\n",
        "    tp = cm[1][1]\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    precision = 0 if tp + fp == 0 else tp/(tp+fp)\n",
        "    recall = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    fprate = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    pdVal = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    pfVal = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    G = 0 if pdVal+1-pfVal == 0 else 2*pdVal*(1-pfVal)/(pdVal+1-pfVal)\n",
        "    F1 = 0 if precision+recall == 0 else 2 * precision * recall / (precision+recall)\n",
        "    MCC = 0 if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) == 0 else (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
        "    result = {\n",
        "        \"eval_loss\": float(perplexity),\n",
        "        \"eval_acc\":round(eval_acc,4),\n",
        "        \"tp\": tp,\n",
        "        \"tn\": tn,\n",
        "        \"fp\": fp,\n",
        "        \"fn\": fn,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"G\": G,\n",
        "        \"MCC\": MCC\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def test(args, model, tokenizer):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\n",
        "\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running Test *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            logit = model(inputs)\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    with open(os.path.join(args.output_dir,\"predictions.txt\"),'w') as f:\n",
        "        for example,pred in zip(eval_dataset.examples,preds):\n",
        "            if pred:\n",
        "                f.write(example.idx+'\\t1\\n')\n",
        "            else:\n",
        "                f.write(example.idx+'\\t0\\n')\n",
        "\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu\n",
        "    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu\n",
        "    # Setup logging\n",
        "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    args.start_epoch = 0\n",
        "    args.start_step = 0\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
        "        args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
        "        args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
        "        idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
        "        with open(idx_file, encoding='utf-8') as idxf:\n",
        "            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
        "\n",
        "        step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
        "        if os.path.exists(step_file):\n",
        "            with open(step_file, encoding='utf-8') as stepf:\n",
        "                args.start_step = int(stepf.readlines()[0].strip())\n",
        "\n",
        "        print(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))\n",
        "\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
        "                                          cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    config.num_labels=1\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\n",
        "                                                do_lower_case=args.do_lower_case,\n",
        "                                                cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    if args.block_size <= 0:\n",
        "        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "    if args.model_name_or_path:\n",
        "        model = model_class.from_pretrained(args.model_name_or_path,\n",
        "                                            from_tf=bool('.ckpt' in args.model_name_or_path),\n",
        "                                            config=config,\n",
        "                                            cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    else:\n",
        "        model = model_class(config)\n",
        "\n",
        "    model=Model(model,config,tokenizer,args)\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    print(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        if args.local_rank not in [-1, 0]:\n",
        "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\n",
        "        if args.local_rank == 0:\n",
        "            torch.distributed.barrier()\n",
        "\n",
        "        train(args, train_dataset, model, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            result=evaluate(args, model, tokenizer)\n",
        "            print(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                print(\"  %s = %s\", key, str(round(result[key],4)))\n",
        "\n",
        "    if args.do_test and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            test(args, model, tokenizer)\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
        "                        help=\"The input training data file (a text file).\")\n",
        "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "    parser.add_argument(\"--test_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "\n",
        "    parser.add_argument(\"--model_type\", default=\"bert\", type=str,\n",
        "                        help=\"The model architecture to be fine-tuned.\")\n",
        "    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\n",
        "                        help=\"The model checkpoint for weights initialization.\")\n",
        "\n",
        "    parser.add_argument(\"--mlm\", action='store_true',\n",
        "                        help=\"Train with masked-language modeling loss instead of language modeling.\")\n",
        "    parser.add_argument(\"--mlm_probability\", type=float, default=0.15,\n",
        "                        help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
        "\n",
        "    parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
        "                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\n",
        "    parser.add_argument(\"--block_size\", default=-1, type=int,\n",
        "                        help=\"Optional input sequence length after tokenization.\"\n",
        "                             \"The training dataset will be truncated in block of this size for training.\"\n",
        "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
        "    parser.add_argument(\"--do_train\", action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--do_test\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
        "                        help=\"Run evaluation during training at each logging step.\")\n",
        "    parser.add_argument(\"--do_lower_case\", action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "\n",
        "    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
        "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight deay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                        help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", default=1.0, type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
        "                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument('--logging_steps', type=int, default=50,\n",
        "                        help=\"Log every X updates steps.\")\n",
        "    parser.add_argument('--save_steps', type=int, default=50,\n",
        "                        help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument('--save_total_limit', type=int, default=None,\n",
        "                        help='Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default')\n",
        "    parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n",
        "                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\n",
        "    parser.add_argument(\"--no_cuda\", action='store_true',\n",
        "                        help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument('--overwrite_output_dir', action='store_true',\n",
        "                        help=\"Overwrite the content of the output directory\")\n",
        "    parser.add_argument('--overwrite_cache', action='store_true',\n",
        "                        help=\"Overwrite the cached training and evaluation sets\")\n",
        "    parser.add_argument('--seed', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--epoch', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--fp16', action='store_true',\n",
        "                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
        "    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
        "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                        help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument('--server_ip', type=str, default='', help=\"For distant debugging.\")\n",
        "    parser.add_argument('--server_port', type=str, default='', help=\"For distant debugging.\")\n",
        "\n",
        "    # Add early stopping parameters and dropout probability parameters\n",
        "    parser.add_argument(\"--early_stopping_patience\", type=int, default=None,\n",
        "                        help=\"Number of epochs with no improvement after which training will be stopped.\")\n",
        "    parser.add_argument(\"--min_loss_delta\", type=float, default=0.001,\n",
        "                        help=\"Minimum change in the loss required to qualify as an improvement.\")\n",
        "    parser.add_argument('--dropout_probability', type=float, default=0, help='dropout probability')\n",
        "\n",
        "    # Simulate command-line arguments\n",
        "    simulated_args = [\n",
        "        '--output_dir', './saved_models',\n",
        "        '--model_type', 'roberta',\n",
        "        '--tokenizer_name', 'roberta-base',\n",
        "        '--model_name_or_path', 'roberta-base',\n",
        "        '--do_train',\n",
        "        '--train_data_file', '/kaggle/working/dataset/train.jsonl',\n",
        "        '--eval_data_file', '/kaggle/working/dataset/test.jsonl',\n",
        "        '--test_data_file', '/kaggle/working/dataset/test.jsonl',\n",
        "        '--epoch', '10',\n",
        "        '--block_size', '512',\n",
        "        '--train_batch_size', '16',\n",
        "        '--eval_batch_size', '16',\n",
        "        '--learning_rate', '2e-5',\n",
        "        '--max_grad_norm', '1.0',\n",
        "        '--evaluate_during_training',\n",
        "        '--seed', '123456'\n",
        "    ]\n",
        "\n",
        "    # Parse the simulated arguments\n",
        "    args = parser.parse_args(simulated_args)\n",
        "\n",
        "    main(args)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T12:46:03.753367Z",
          "iopub.execute_input": "2024-07-04T12:46:03.753949Z",
          "iopub.status.idle": "2024-07-04T12:52:21.428893Z",
          "shell.execute_reply.started": "2024-07-04T12:46:03.753917Z",
          "shell.execute_reply": "2024-07-04T12:52:21.428087Z"
        },
        "trusted": true,
        "id": "GXmm1HHGW1xu",
        "outputId": "026235cb-85ce-439d-c573-5262eb295bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-07-04 12:46:06.787614: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-04 12:46:06.787670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-04 12:46:06.789730: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training/evaluation parameters %s Namespace(train_data_file='/kaggle/working/dataset/train.jsonl', output_dir='./saved_models', eval_data_file='/kaggle/working/dataset/test.jsonl', test_data_file='/kaggle/working/dataset/test.jsonl', model_type='roberta', model_name_or_path='roberta-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='roberta-base', cache_dir='', block_size=510, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=16, eval_batch_size=16, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=10, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, start_epoch=0, start_step=0)\n***** Running training *****\n  Num examples = %d 496\n  Num Epochs = %d 10\n  Instantaneous batch size per GPU = %d 16\n  Total train batch size (w. parallel, distributed & accumulation) = %d 16\n  Gradient Accumulation steps = %d 1\n  Total optimization steps = %d 310\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.68197 acc 0.56048: 100%|██████████| 31/31 [00:37<00:00,  1.21s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.5699\n  %s = %s eval_acc 0.7123\n  %s = %s tp 256\n  %s = %s tn 56\n  %s = %s fp 101\n  %s = %s fn 25\n  %s = %s f1 0.8025\n  %s = %s precision 0.7171\n  %s = %s recall 0.911\n  %s = %s G 0.5127\n  %s = %s MCC 0.3307\n  ********************\n  Best acc:%s 0.7123\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.4912 acc 0.77419: 100%|██████████| 31/31 [00:36<00:00,  1.16s/it] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.7007\n  %s = %s eval_acc 0.6758\n  %s = %s tp 192\n  %s = %s tn 104\n  %s = %s fp 53\n  %s = %s fn 89\n  %s = %s f1 0.73\n  %s = %s precision 0.7837\n  %s = %s recall 0.6833\n  %s = %s G 0.6727\n  %s = %s MCC 0.3339\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 2 loss 0.27751 acc 0.89516: 100%|██████████| 31/31 [00:36<00:00,  1.17s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 1.077\n  %s = %s eval_acc 0.6324\n  %s = %s tp 201\n  %s = %s tn 76\n  %s = %s fp 81\n  %s = %s fn 80\n  %s = %s f1 0.714\n  %s = %s precision 0.7128\n  %s = %s recall 0.7153\n  %s = %s G 0.5774\n  %s = %s MCC 0.1997\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 3 loss 0.19243 acc 0.9375: 100%|██████████| 31/31 [00:36<00:00,  1.17s/it] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 1.4319\n  %s = %s eval_acc 0.6416\n  %s = %s tp 208\n  %s = %s tn 73\n  %s = %s fp 84\n  %s = %s fn 73\n  %s = %s f1 0.726\n  %s = %s precision 0.7123\n  %s = %s recall 0.7402\n  %s = %s G 0.5712\n  %s = %s MCC 0.2087\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 4 loss 0.16451 acc 0.95363: 100%|██████████| 31/31 [00:36<00:00,  1.17s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 1.797\n  %s = %s eval_acc 0.6301\n  %s = %s tp 203\n  %s = %s tn 73\n  %s = %s fp 84\n  %s = %s fn 78\n  %s = %s f1 0.7148\n  %s = %s precision 0.7073\n  %s = %s recall 0.7224\n  %s = %s G 0.5658\n  %s = %s MCC 0.1891\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 5 loss 0.1037 acc 0.96976: 100%|██████████| 31/31 [00:36<00:00,  1.17s/it] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 1.864\n  %s = %s eval_acc 0.6347\n  %s = %s tp 202\n  %s = %s tn 76\n  %s = %s fp 81\n  %s = %s fn 79\n  %s = %s f1 0.7163\n  %s = %s precision 0.7138\n  %s = %s recall 0.7189\n  %s = %s G 0.5786\n  %s = %s MCC 0.2035\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 6 loss 0.09246 acc 0.97379: 100%|██████████| 31/31 [00:36<00:00,  1.18s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 1.9442\n  %s = %s eval_acc 0.637\n  %s = %s tp 205\n  %s = %s tn 74\n  %s = %s fp 83\n  %s = %s fn 76\n  %s = %s f1 0.7206\n  %s = %s precision 0.7118\n  %s = %s recall 0.7295\n  %s = %s G 0.5727\n  %s = %s MCC 0.203\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 7 loss 0.07527 acc 0.97984: 100%|██████████| 31/31 [00:36<00:00,  1.17s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 2.0973\n  %s = %s eval_acc 0.6347\n  %s = %s tp 201\n  %s = %s tn 77\n  %s = %s fp 80\n  %s = %s fn 80\n  %s = %s f1 0.7153\n  %s = %s precision 0.7153\n  %s = %s recall 0.7153\n  %s = %s G 0.5819\n  %s = %s MCC 0.2057\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 8 loss 0.05207 acc 0.98589: 100%|██████████| 31/31 [00:36<00:00,  1.17s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 2.0458\n  %s = %s eval_acc 0.6347\n  %s = %s tp 201\n  %s = %s tn 77\n  %s = %s fp 80\n  %s = %s fn 80\n  %s = %s f1 0.7153\n  %s = %s precision 0.7153\n  %s = %s recall 0.7153\n  %s = %s G 0.5819\n  %s = %s MCC 0.2057\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 9 loss 0.04356 acc 0.98992: 100%|██████████| 31/31 [00:36<00:00,  1.17s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 2.1095\n  %s = %s eval_acc 0.6347\n  %s = %s tp 201\n  %s = %s tn 77\n  %s = %s fp 80\n  %s = %s fn 80\n  %s = %s f1 0.7153\n  %s = %s precision 0.7153\n  %s = %s recall 0.7153\n  %s = %s G 0.5819\n  %s = %s MCC 0.2057\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%python\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import math\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "import json\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import multiprocessing\n",
        "cpu_cont = multiprocessing.cpu_count()\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "MODEL_CLASSES = {\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, encoder,config,tokenizer,args):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.config=config\n",
        "        self.tokenizer=tokenizer\n",
        "        self.args=args\n",
        "\n",
        "        # Define dropout layer, dropout_probability is taken from args.\n",
        "        self.dropout = nn.Dropout(args.dropout_probability)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None,labels=None):\n",
        "        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\n",
        "\n",
        "        # Apply dropout\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        logits=outputs\n",
        "        prob=torch.sigmoid(logits)\n",
        "        if labels is not None:\n",
        "            labels=labels.float()\n",
        "            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\n",
        "            loss=-loss.mean()\n",
        "            return loss,prob\n",
        "        else:\n",
        "            return prob\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 input_tokens,\n",
        "                 input_ids,\n",
        "                 idx,\n",
        "                 label,\n",
        "\n",
        "    ):\n",
        "        self.input_tokens = input_tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.idx=str(idx)\n",
        "        self.label=int(label)\n",
        "def convert_examples_to_features(js,tokenizer,args):\n",
        "    #source\n",
        "    code=' '.join(js['func'].split())\n",
        "    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\n",
        "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
        "    padding_length = args.block_size - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    return InputFeatures(source_tokens,source_ids,js['idx'],js['target'])\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, file_path=None):\n",
        "        self.examples = []\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                js=json.loads(line.strip())\n",
        "                self.examples.append(convert_examples_to_features(js,tokenizer,args))\n",
        "        if 'train' in file_path:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                    logger.info(\"*** Example ***\")\n",
        "                    logger.info(\"idx: {}\".format(idx))\n",
        "                    logger.info(\"label: {}\".format(example.label))\n",
        "                    logger.info(\"input_tokens: {}\".format([x.replace('\\u0120','_') for x in example.input_tokens]))\n",
        "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
        "                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\n",
        "    args.max_steps=args.epoch*len( train_dataloader)\n",
        "    args.save_steps=len( train_dataloader)\n",
        "    args.warmup_steps=len( train_dataloader)\n",
        "    args.logging_steps=len( train_dataloader)\n",
        "    args.num_train_epochs=args.epoch\n",
        "    model.to(args.device)\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\n",
        "                                                num_training_steps=args.max_steps)\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
        "    optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
        "    if os.path.exists(scheduler_last):\n",
        "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
        "    if os.path.exists(optimizer_last):\n",
        "        optimizer.load_state_dict(torch.load(optimizer_last))\n",
        "    # Train!\n",
        "    print(\"***** Running training *****\")\n",
        "    print(\"  Num examples = %d\", len(train_dataset))\n",
        "    print(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
        "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    print(\"  Total optimization steps = %d\", args.max_steps)\n",
        "\n",
        "    global_step = args.start_step\n",
        "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
        "    best_mrr=0.0\n",
        "    best_acc=0.0\n",
        "    # model.resize_token_embeddings(len(tokenizer))\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Initialize early stopping parameters at the start of training\n",
        "    early_stopping_counter = 0\n",
        "    best_loss = None\n",
        "\n",
        "    for idx in range(args.start_epoch, int(args.num_train_epochs)):\n",
        "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "        tr_num=0\n",
        "        train_loss=0\n",
        "        acc = 0\n",
        "        for step, batch in enumerate(bar):\n",
        "            inputs = batch[0].to(args.device)\n",
        "            labels=batch[1].to(args.device)\n",
        "            model.train()\n",
        "            loss,logits = model(inputs,labels)\n",
        "\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            preds=logits[:,0]>0.5\n",
        "            acc+=np.mean(labels==preds)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            tr_num+=1\n",
        "            train_loss+=loss.item()\n",
        "            if avg_loss==0:\n",
        "                avg_loss=tr_loss\n",
        "            avg_loss=round(train_loss/tr_num,5)\n",
        "            avg_acc = round(acc/tr_num,5)\n",
        "            bar.set_description(\"epoch {} loss {} acc {}\".format(idx,avg_loss,avg_acc))\n",
        "\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "                output_flag=True\n",
        "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    logging_loss = tr_loss\n",
        "                    tr_nb=global_step\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer,eval_when_training=True)\n",
        "                        for key, value in results.items():\n",
        "                            print(\"  %s = %s\", key, round(value,4))\n",
        "                        # Save model checkpoint\n",
        "\n",
        "                    if results['eval_acc']>best_acc:\n",
        "                        best_acc=results['eval_acc']\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "                        print(\"  Best acc:%s\",round(best_acc,4))\n",
        "                        print(\"  \"+\"*\"*20)\n",
        "\n",
        "                        checkpoint_prefix = 'checkpoint-best-acc'\n",
        "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "                        if not os.path.exists(output_dir):\n",
        "                            os.makedirs(output_dir)\n",
        "                        model_to_save = model.module if hasattr(model,'module') else model\n",
        "                        output_dir = os.path.join(output_dir, '{}'.format('model.bin'))\n",
        "                        torch.save(model_to_save.state_dict(), output_dir)\n",
        "                        print(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = train_loss / tr_num\n",
        "\n",
        "        # Check for early stopping condition\n",
        "        if args.early_stopping_patience is not None:\n",
        "            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\n",
        "                best_loss = avg_loss\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= args.early_stopping_patience:\n",
        "                    print(\"Early stopping\")\n",
        "                    break  # Exit the loop early\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer,eval_when_training=False):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1 and eval_when_training is False:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running evaluation *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in eval_dataloader:\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            lm_loss,logit = model(inputs,label)\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "        nb_eval_steps += 1\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    eval_acc=np.mean(labels==preds)\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.tensor(eval_loss)\n",
        "\n",
        "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
        "    cm = confusion_matrix(y_true=labels, y_pred=preds)\n",
        "    tp = cm[1][1]\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    precision = 0 if tp + fp == 0 else tp/(tp+fp)\n",
        "    recall = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    fprate = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    pdVal = 0 if tp + fn == 0 else tp/(tp+fn)\n",
        "    pfVal = 0 if fp + tn == 0 else fp/(fp+tn)\n",
        "    G = 0 if pdVal+1-pfVal == 0 else 2*pdVal*(1-pfVal)/(pdVal+1-pfVal)\n",
        "    F1 = 0 if precision+recall == 0 else 2 * precision * recall / (precision+recall)\n",
        "    MCC = 0 if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) == 0 else (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
        "    result = {\n",
        "        \"eval_loss\": float(perplexity),\n",
        "        \"eval_acc\":round(eval_acc,4),\n",
        "        \"tp\": tp,\n",
        "        \"tn\": tn,\n",
        "        \"fp\": fp,\n",
        "        \"fn\": fn,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"G\": G,\n",
        "        \"MCC\": MCC\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def test(args, model, tokenizer):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\n",
        "\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running Test *****\")\n",
        "    print(\"  Num examples = %d\", len(eval_dataset))\n",
        "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "    logits=[]\n",
        "    labels=[]\n",
        "    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "        inputs = batch[0].to(args.device)\n",
        "        label=batch[1].to(args.device)\n",
        "        with torch.no_grad():\n",
        "            logit = model(inputs)\n",
        "            logits.append(logit.cpu().numpy())\n",
        "            labels.append(label.cpu().numpy())\n",
        "\n",
        "    logits=np.concatenate(logits,0)\n",
        "    labels=np.concatenate(labels,0)\n",
        "    preds=logits[:,0]>0.5\n",
        "    with open(os.path.join(args.output_dir,\"predictions.txt\"),'w') as f:\n",
        "        for example,pred in zip(eval_dataset.examples,preds):\n",
        "            if pred:\n",
        "                f.write(example.idx+'\\t1\\n')\n",
        "            else:\n",
        "                f.write(example.idx+'\\t0\\n')\n",
        "\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu\n",
        "    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu\n",
        "    # Setup logging\n",
        "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    args.start_epoch = 0\n",
        "    args.start_step = 0\n",
        "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
        "        args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
        "        args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
        "        idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
        "        with open(idx_file, encoding='utf-8') as idxf:\n",
        "            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
        "\n",
        "        step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
        "        if os.path.exists(step_file):\n",
        "            with open(step_file, encoding='utf-8') as stepf:\n",
        "                args.start_step = int(stepf.readlines()[0].strip())\n",
        "\n",
        "        print(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))\n",
        "\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
        "                                          cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    config.num_labels=1\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\n",
        "                                                do_lower_case=args.do_lower_case,\n",
        "                                                cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    if args.block_size <= 0:\n",
        "        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "    if args.model_name_or_path:\n",
        "        model = model_class.from_pretrained(args.model_name_or_path,\n",
        "                                            from_tf=bool('.ckpt' in args.model_name_or_path),\n",
        "                                            config=config,\n",
        "                                            cache_dir=args.cache_dir if args.cache_dir else None)\n",
        "    else:\n",
        "        model = model_class(config)\n",
        "    for param in model.roberta.parameters():\n",
        "        param.requires_grad=False\n",
        "    model=Model(model,config,tokenizer,args)\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    print(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        if args.local_rank not in [-1, 0]:\n",
        "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\n",
        "        if args.local_rank == 0:\n",
        "            torch.distributed.barrier()\n",
        "\n",
        "        train(args, train_dataset, model, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            result=evaluate(args, model, tokenizer)\n",
        "            print(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                print(\"  %s = %s\", key, str(round(result[key],4)))\n",
        "\n",
        "    if args.do_test and args.local_rank in [-1, 0]:\n",
        "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
        "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
        "            model.load_state_dict(torch.load(output_dir))\n",
        "            model.to(args.device)\n",
        "            test(args, model, tokenizer)\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
        "                        help=\"The input training data file (a text file).\")\n",
        "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "    parser.add_argument(\"--test_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "\n",
        "    parser.add_argument(\"--model_type\", default=\"bert\", type=str,\n",
        "                        help=\"The model architecture to be fine-tuned.\")\n",
        "    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\n",
        "                        help=\"The model checkpoint for weights initialization.\")\n",
        "\n",
        "    parser.add_argument(\"--mlm\", action='store_true',\n",
        "                        help=\"Train with masked-language modeling loss instead of language modeling.\")\n",
        "    parser.add_argument(\"--mlm_probability\", type=float, default=0.15,\n",
        "                        help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
        "\n",
        "    parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
        "    parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
        "                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\n",
        "    parser.add_argument(\"--block_size\", default=-1, type=int,\n",
        "                        help=\"Optional input sequence length after tokenization.\"\n",
        "                             \"The training dataset will be truncated in block of this size for training.\"\n",
        "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
        "    parser.add_argument(\"--do_train\", action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--do_test\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
        "                        help=\"Run evaluation during training at each logging step.\")\n",
        "    parser.add_argument(\"--do_lower_case\", action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "\n",
        "    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
        "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight deay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                        help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", default=1.0, type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
        "                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument('--logging_steps', type=int, default=50,\n",
        "                        help=\"Log every X updates steps.\")\n",
        "    parser.add_argument('--save_steps', type=int, default=50,\n",
        "                        help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument('--save_total_limit', type=int, default=None,\n",
        "                        help='Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default')\n",
        "    parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n",
        "                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\n",
        "    parser.add_argument(\"--no_cuda\", action='store_true',\n",
        "                        help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument('--overwrite_output_dir', action='store_true',\n",
        "                        help=\"Overwrite the content of the output directory\")\n",
        "    parser.add_argument('--overwrite_cache', action='store_true',\n",
        "                        help=\"Overwrite the cached training and evaluation sets\")\n",
        "    parser.add_argument('--seed', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--epoch', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--fp16', action='store_true',\n",
        "                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
        "    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
        "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                        help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument('--server_ip', type=str, default='', help=\"For distant debugging.\")\n",
        "    parser.add_argument('--server_port', type=str, default='', help=\"For distant debugging.\")\n",
        "\n",
        "    # Add early stopping parameters and dropout probability parameters\n",
        "    parser.add_argument(\"--early_stopping_patience\", type=int, default=None,\n",
        "                        help=\"Number of epochs with no improvement after which training will be stopped.\")\n",
        "    parser.add_argument(\"--min_loss_delta\", type=float, default=0.001,\n",
        "                        help=\"Minimum change in the loss required to qualify as an improvement.\")\n",
        "    parser.add_argument('--dropout_probability', type=float, default=0, help='dropout probability')\n",
        "\n",
        "    # Simulate command-line arguments\n",
        "    simulated_args = [\n",
        "        '--output_dir', './saved_models',\n",
        "        '--model_type', 'roberta',\n",
        "        '--tokenizer_name', 'microsoft/codebert-base',\n",
        "        '--model_name_or_path', 'microsoft/codebert-base',\n",
        "        '--do_train',\n",
        "        '--train_data_file', '/kaggle/working/dataset/train.jsonl',\n",
        "        '--eval_data_file', '/kaggle/working/dataset/test.jsonl',\n",
        "        '--test_data_file', '/kaggle/working/dataset/test.jsonl',\n",
        "        '--epoch', '10',\n",
        "        '--block_size', '512',\n",
        "        '--train_batch_size', '16',\n",
        "        '--eval_batch_size', '16',\n",
        "        '--learning_rate', '2e-5',\n",
        "        '--max_grad_norm', '1.0',\n",
        "        '--evaluate_during_training',\n",
        "        '--seed', '123456'\n",
        "    ]\n",
        "\n",
        "    # Parse the simulated arguments\n",
        "    args = parser.parse_args(simulated_args)\n",
        "\n",
        "    main(args)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-04T12:30:58.984893Z",
          "iopub.execute_input": "2024-07-04T12:30:58.985244Z",
          "iopub.status.idle": "2024-07-04T12:34:29.527747Z",
          "shell.execute_reply.started": "2024-07-04T12:30:58.985214Z",
          "shell.execute_reply": "2024-07-04T12:34:29.526876Z"
        },
        "trusted": true,
        "id": "2GdkP7fTW1xv",
        "outputId": "bd516e91-5863-4fdb-da1d-0ff80dd5eab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-07-04 12:31:02.060671: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-04 12:31:02.060728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-04 12:31:02.062882: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training/evaluation parameters %s Namespace(train_data_file='/kaggle/working/dataset/train.jsonl', output_dir='./saved_models', eval_data_file='/kaggle/working/dataset/test.jsonl', test_data_file='/kaggle/working/dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='', block_size=510, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=16, eval_batch_size=16, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=10, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, start_epoch=0, start_step=0)\n***** Running training *****\n  Num examples = %d 496\n  Num Epochs = %d 10\n  Instantaneous batch size per GPU = %d 16\n  Total train batch size (w. parallel, distributed & accumulation) = %d 16\n  Gradient Accumulation steps = %d 1\n  Total optimization steps = %d 310\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 0 loss 0.69588 acc 0.49194: 100%|██████████| 31/31 [00:21<00:00,  1.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6784\n  %s = %s eval_acc 0.7237\n  %s = %s tp 231\n  %s = %s tn 86\n  %s = %s fp 71\n  %s = %s fn 50\n  %s = %s f1 0.7925\n  %s = %s precision 0.7649\n  %s = %s recall 0.8221\n  %s = %s G 0.6575\n  %s = %s MCC 0.3833\n  ********************\n  Best acc:%s 0.7237\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 1 loss 0.67804 acc 0.59677: 100%|██████████| 31/31 [00:19<00:00,  1.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6604\n  %s = %s eval_acc 0.7032\n  %s = %s tp 239\n  %s = %s tn 69\n  %s = %s fp 88\n  %s = %s fn 42\n  %s = %s f1 0.7862\n  %s = %s precision 0.7309\n  %s = %s recall 0.8505\n  %s = %s G 0.5795\n  %s = %s MCC 0.3197\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 2 loss 0.66653 acc 0.63911: 100%|██████████| 31/31 [00:20<00:00,  1.50it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6575\n  %s = %s eval_acc 0.7511\n  %s = %s tp 227\n  %s = %s tn 102\n  %s = %s fp 55\n  %s = %s fn 54\n  %s = %s f1 0.8064\n  %s = %s precision 0.805\n  %s = %s recall 0.8078\n  %s = %s G 0.7202\n  %s = %s MCC 0.4582\n  ********************\n  Best acc:%s 0.7511\n  ********************\nSaving model checkpoint to %s ./saved_models/checkpoint-best-acc/model.bin\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 3 loss 0.65249 acc 0.65927: 100%|██████████| 31/31 [00:19<00:00,  1.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6288\n  %s = %s eval_acc 0.7009\n  %s = %s tp 251\n  %s = %s tn 56\n  %s = %s fp 101\n  %s = %s fn 30\n  %s = %s f1 0.793\n  %s = %s precision 0.7131\n  %s = %s recall 0.8932\n  %s = %s G 0.5098\n  %s = %s MCC 0.3017\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 4 loss 0.64696 acc 0.65927: 100%|██████████| 31/31 [00:19<00:00,  1.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6299\n  %s = %s eval_acc 0.7123\n  %s = %s tp 243\n  %s = %s tn 69\n  %s = %s fp 88\n  %s = %s fn 38\n  %s = %s f1 0.7941\n  %s = %s precision 0.7341\n  %s = %s recall 0.8648\n  %s = %s G 0.5828\n  %s = %s MCC 0.3396\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 5 loss 0.64516 acc 0.67137: 100%|██████████| 31/31 [00:19<00:00,  1.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6326\n  %s = %s eval_acc 0.742\n  %s = %s tp 234\n  %s = %s tn 91\n  %s = %s fp 66\n  %s = %s fn 47\n  %s = %s f1 0.8055\n  %s = %s precision 0.78\n  %s = %s recall 0.8327\n  %s = %s G 0.6835\n  %s = %s MCC 0.4257\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 6 loss 0.64564 acc 0.66935: 100%|██████████| 31/31 [00:19<00:00,  1.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6339\n  %s = %s eval_acc 0.7511\n  %s = %s tp 227\n  %s = %s tn 102\n  %s = %s fp 55\n  %s = %s fn 54\n  %s = %s f1 0.8064\n  %s = %s precision 0.805\n  %s = %s recall 0.8078\n  %s = %s G 0.7202\n  %s = %s MCC 0.4582\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 7 loss 0.63487 acc 0.67742: 100%|██████████| 31/31 [00:19<00:00,  1.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6321\n  %s = %s eval_acc 0.7489\n  %s = %s tp 226\n  %s = %s tn 102\n  %s = %s fp 55\n  %s = %s fn 55\n  %s = %s f1 0.8043\n  %s = %s precision 0.8043\n  %s = %s recall 0.8043\n  %s = %s G 0.7188\n  %s = %s MCC 0.454\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 8 loss 0.62712 acc 0.68952: 100%|██████████| 31/31 [00:19<00:00,  1.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6254\n  %s = %s eval_acc 0.7511\n  %s = %s tp 235\n  %s = %s tn 94\n  %s = %s fp 63\n  %s = %s fn 46\n  %s = %s f1 0.8117\n  %s = %s precision 0.7886\n  %s = %s recall 0.8363\n  %s = %s G 0.6978\n  %s = %s MCC 0.4473\n***** Running evaluation *****\n  Num examples = %d 438\n  Batch size = %d 16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "epoch 9 loss 0.63474 acc 0.6754: 100%|██████████| 31/31 [00:19<00:00,  1.60it/s] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  %s = %s eval_loss 0.6243\n  %s = %s eval_acc 0.7466\n  %s = %s tp 236\n  %s = %s tn 91\n  %s = %s fp 66\n  %s = %s fn 45\n  %s = %s f1 0.8096\n  %s = %s precision 0.7815\n  %s = %s recall 0.8399\n  %s = %s G 0.6859\n  %s = %s MCC 0.4347\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mU73rlHYW1xw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}